{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "nyax3NuwS-Wg",
        "outputId": "5f22f429-5028-4045-ecde-31b07dfbe09f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"83ddc3b3-0587-4cb5-a406-1bb01318b50e-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"83ddc3b3-0587-4cb5-a406-1bb01318b50e-text\">Updating and upgrading APT</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"83ddc3b3-0587-4cb5-a406-1bb01318b50e-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"83ddc3b3-0587-4cb5-a406-1bb01318b50e-text\");\n",
              "            text.innerText = \"Updated and upgraded APT\";\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"8eb844c1-9949-4e1a-a00b-9c39db4db6b5-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"8eb844c1-9949-4e1a-a00b-9c39db4db6b5-text\">Downloading Google Chrome</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"8eb844c1-9949-4e1a-a00b-9c39db4db6b5-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"8eb844c1-9949-4e1a-a00b-9c39db4db6b5-text\");\n",
              "            text.innerText = \"Downloaded Google Chrome\";\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"d2b3cff7-c59f-40e8-8808-db3e9162973a-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"d2b3cff7-c59f-40e8-8808-db3e9162973a-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"d2b3cff7-c59f-40e8-8808-db3e9162973a-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"d2b3cff7-c59f-40e8-8808-db3e9162973a-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%pip install -q google-colab-selenium\n",
        "import google_colab_selenium as gs\n",
        "driver = gs.Chrome()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSU9_tD2UNhN",
        "outputId": "f388f2a1-5da7-49cf-fe85-524f4a4c2fcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.27.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.27.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "# Install selenium\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q-h7afwVry8"
      },
      "source": [
        "SIMPLE EXAMPLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "3Nj3XVBCKQ3r",
        "outputId": "82b358a3-bd92-4a7d-abf0-bba1f15daccd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"8088ea19-6272-44a1-a862-9c0decf11b9d-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"8088ea19-6272-44a1-a862-9c0decf11b9d-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"8088ea19-6272-44a1-a862-9c0decf11b9d-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"8088ea19-6272-44a1-a862-9c0decf11b9d-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Breaking News: Market Hits Record High\n",
            "Description: The stock market reached an all-time high today.\n",
            "Author: By Jane Doe\n",
            "----------------------------------------\n",
            "Title: Weather Update: Sunny Days Ahead\n",
            "Description: Expect clear skies and warm weather this week.\n",
            "Author: By John Smith\n",
            "----------------------------------------\n",
            "Title: Sports: Local Team Wins Championship\n",
            "Description: The home team clinched the title in a thrilling final.\n",
            "Author: By Alex Brown\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Set up Chrome options for Colab\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "#from selenium.webdriver.common.keys import Keys\n",
        "#from selenium.webdriver.chrome.service import Service\n",
        "import time\n",
        "\n",
        "# Set up Chrome options to use headless mode (for Colab)\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "\n",
        "# Add extra options\n",
        "chrome_options.add_argument(\"--window-size=1920,1080\")  # Set the window size\n",
        "chrome_options.add_argument(\"--disable-infobars\")  # Disable the infobars\n",
        "chrome_options.add_argument(\"--disable-popup-blocking\")  # Disable pop-ups\n",
        "chrome_options.add_argument(\"--ignore-certificate-errors\")  # Ignore certificate errors\n",
        "chrome_options.add_argument(\"--incognito\")  # Use Chrome in incognito mode\n",
        "\n",
        "# Set the path to chromedriver explicitly (installed by apt)\n",
        "chrome_path = \"/usr/bin/chromedriver\"\n",
        "\n",
        "# Initialize the WebDriver with the updated path\n",
        "driver = gs.Chrome(options=chrome_options)\n",
        "\n",
        "# Open the GitHub Issues page\n",
        "url = \"https://sezeruguz.github.io/data/main\"  # URL for GitHub Issues\n",
        "driver.get(url)\n",
        "\n",
        "# Haberleri seçiyoruz\n",
        "news_items = driver.find_elements(By.CLASS_NAME, 'news-item')\n",
        "\n",
        "# Bilgileri çekip ekrana yazdırıyoruz\n",
        "for item in news_items:\n",
        "    title = item.find_element(By.CLASS_NAME, 'title').text\n",
        "    description = item.find_element(By.CLASS_NAME, 'description').text\n",
        "    author = item.find_element(By.CLASS_NAME, 'author').text\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"Description: {description}\")\n",
        "    print(f\"Author: {author}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Tarayıcıyı kapat\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3FFdoljKhFd",
        "outputId": "28b08bdc-1bcc-4f08-a6ed-b78171148b5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Breaking News: Market Hits Record High\n",
            "Description: The stock market reached an all-time high today.\n",
            "Author: By Jane Doe\n",
            "----------------------------------------\n",
            "Title: Weather Update: Sunny Days Ahead\n",
            "Description: Expect clear skies and warm weather this week.\n",
            "Author: By John Smith\n",
            "----------------------------------------\n",
            "Title: Sports: Local Team Wins Championship\n",
            "Description: The home team clinched the title in a thrilling final.\n",
            "Author: By Alex Brown\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Web sayfasının URL'sini belirtin\n",
        "url = 'https://sezeruguz.github.io/data/main'  # Gerçek bir URL ile değiştirin\n",
        "\n",
        "# URL'den HTML içeriğini çekiyoruz\n",
        "response = urllib.request.urlopen(url)\n",
        "html_content = response.read()\n",
        "\n",
        "# HTML'yi BeautifulSoup ile parse ediyoruz\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Haberleri seçiyoruz (örneğin, article sınıfında bulunan haberleri)\n",
        "news_items = soup.find_all('article', class_='news-item')\n",
        "\n",
        "# Bilgileri çekip ekrana yazdırıyoruz\n",
        "for item in news_items:\n",
        "    title = item.find('h2', class_='title').text\n",
        "    description = item.find('p', class_='description').text\n",
        "    author = item.find('span', class_='author').text\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"Description: {description}\")\n",
        "    print(f\"Author: {author}\")\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCWCWZUjU-Hd"
      },
      "source": [
        "ADVANCED EXAMPLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "A4mqD8U_VyRx",
        "outputId": "7094682e-20d6-43b1-fbc5-858fa3836233"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"84a3fb25-86e8-480f-b4e1-0188ddc23e2a-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"84a3fb25-86e8-480f-b4e1-0188ddc23e2a-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"84a3fb25-86e8-480f-b4e1-0188ddc23e2a-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"84a3fb25-86e8-480f-b4e1-0188ddc23e2a-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No more pages or an error occurred.\n",
            "Scraped GitHub Issue Titles and Links:\n",
            "1: ImportError: cannot import name 'ModelFilter' from 'huggingface_hub' - https://github.com/huggingface/huggingface_hub/issues/2710\n",
            "2: need a published docker image with latest huggingface-hub and huggingface-cli - https://github.com/huggingface/huggingface_hub/issues/2708\n",
            "3: [InferenceClient] Provide a way to deal with content-type header when sending raw bytes - https://github.com/huggingface/huggingface_hub/issues/2706\n",
            "4: Enormous repository storage overusage - https://github.com/huggingface/huggingface_hub/issues/2700\n",
            "5: Bad type guess in VS Code with ModelHubMixin inheritance - https://github.com/huggingface/huggingface_hub/issues/2694\n",
            "6: upload_large_folder() issue with uploading to new spaces (create_repo requires space_sdk) - https://github.com/huggingface/huggingface_hub/issues/2693\n",
            "7: downloading large files got stuck at 0% forever when using VPN, but it works when using wget - https://github.com/huggingface/huggingface_hub/issues/2691\n",
            "8: remove list_metrics - https://github.com/huggingface/huggingface_hub/issues/2690\n",
            "9: Error while fetching HF_TOKEN secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'. You are not authenticated with the Hugging Face Hub in this notebook. - https://github.com/huggingface/huggingface_hub/issues/2686\n",
            "10: Stable-FX/JAKI - https://github.com/huggingface/huggingface_hub/issues/2685\n",
            "11: A runtimeerror with repositopry of CLAM - https://github.com/huggingface/huggingface_hub/issues/2681\n",
            "12: test - https://github.com/huggingface/huggingface_hub/issues/2678\n",
            "13: HF_HUB_ENABLE_HF_TRANSFER=1 - last few hundred MBs becomes so so so slow - https://github.com/huggingface/huggingface_hub/issues/2677\n",
            "14: Support server-side filtering for list_repo_tree - https://github.com/huggingface/huggingface_hub/issues/2676\n",
            "15: python 3.12 not working on spaces - https://github.com/huggingface/huggingface_hub/issues/2675\n",
            "16: TypeError: OrthogonalRegularizer.__init__() got an unexpected keyword argument 'num_features' - https://github.com/huggingface/huggingface_hub/issues/2673\n",
            "17: AsyncInferenceClient uses synchronous time.sleep - https://github.com/huggingface/huggingface_hub/issues/2672\n",
            "18: list_inference_endpoints fails with Date parsing error - https://github.com/huggingface/huggingface_hub/issues/2671\n",
            "19: Method to retrieve jwt token + stream logs from a Space - https://github.com/huggingface/huggingface_hub/issues/2667\n",
            "20: HF Upload file doesn't work over 10MB - https://github.com/huggingface/huggingface_hub/issues/2666\n",
            "21: Delamain_chatbot_P12 - https://github.com/huggingface/huggingface_hub/issues/2663\n",
            "22: snapshot_download download speeds are not shown on Kaggle and on some other platforms - https://github.com/huggingface/huggingface_hub/issues/2662\n",
            "23: Single link downloads limited to like 40 megabytes per second can we make snapshot_download to open multi connection and download faster? - https://github.com/huggingface/huggingface_hub/issues/2658\n",
            "24: allow base64 images to be passed to HF api - https://github.com/huggingface/huggingface_hub/issues/2654\n",
            "25: max_tokens's default in docstring of InferenceClient::chat_completion's is 20 but that of TGI is 100 - https://github.com/huggingface/huggingface_hub/issues/2652\n",
            "26: hf token - https://github.com/huggingface/huggingface_hub/issues/2651\n",
            "27: Suggest to use upload_large_folder when using upload_folder with folders containing many files - https://github.com/huggingface/huggingface_hub/issues/2650\n",
            "28: Option to get security status with hf_api - https://github.com/huggingface/huggingface_hub/issues/2649\n",
            "29: InferenceClient: allow passing a pydantic model as response_format - https://github.com/huggingface/huggingface_hub/issues/2646\n",
            "30: OSError: Consistency check failed: file should be of size 18612 but has size 18605 (datasets/tau/scrolls@main/scrolls.py). - https://github.com/huggingface/huggingface_hub/issues/2645\n",
            "31: Inconsistent API format for ModelDataCard - https://github.com/huggingface/huggingface_hub/issues/2642\n",
            "32: [i18n-es] Translating docs to Spanish - https://github.com/huggingface/huggingface_hub/issues/2640\n",
            "33: SecurityStatus does not exist in model_info - https://github.com/huggingface/huggingface_hub/issues/2638\n",
            "34: Spacy packages not installable via (uv) pip - https://github.com/huggingface/huggingface_hub/issues/2637\n",
            "35: huggingface - https://github.com/huggingface/huggingface_hub/issues/2635\n",
            "36: dataset is not working (Salama1429/tarteel-ai-everyayah-Quran) - https://github.com/huggingface/huggingface_hub/issues/2630\n",
            "37: Error getting hf file, getting proxy error - https://github.com/huggingface/huggingface_hub/issues/2625\n",
            "38: ACTIONS_ALLOW_USE_UNSECURE_NODE_VERSION=true - https://github.com/huggingface/huggingface_hub/issues/2624\n",
            "39: Unable to install huggingface-cli - https://github.com/huggingface/huggingface_hub/issues/2619\n",
            "40: integrate firerequests for faster downloads - https://github.com/huggingface/huggingface_hub/issues/2618\n",
            "41: cached_download from Huggin Face import error - https://github.com/huggingface/huggingface_hub/issues/2617\n",
            "42: hii - https://github.com/huggingface/huggingface_hub/issues/2613\n",
            "43: Resolving pain-points for uploading large files and folders - https://github.com/huggingface/huggingface_hub/issues/2612\n",
            "44: NameError: name 'packaging' is not defined - https://github.com/huggingface/huggingface_hub/issues/2609\n",
            "45: It shows error in output - https://github.com/huggingface/huggingface_hub/issues/2608\n",
            "46: snapshot_download edge case exception - https://github.com/huggingface/huggingface_hub/issues/2607\n",
            "47: Passing tool results to the LM - https://github.com/huggingface/huggingface_hub/issues/2606\n",
            "48: Revisions are not automatically created remotely with upload_large_folder - https://github.com/huggingface/huggingface_hub/issues/2602\n",
            "49: make ignore_patterns=\"original\" the default setting for download of model weights. - https://github.com/huggingface/huggingface_hub/issues/2597\n",
            "50: HF_HUB_OFFLINE environment variable breaks Langchain's HuggingFaceEndpoints - https://github.com/huggingface/huggingface_hub/issues/2590\n",
            "51: OSError: Consistency check failed: file should be of size 133466304 but has size 635 - https://github.com/huggingface/huggingface_hub/issues/2589\n",
            "52: huggingface-cli login on Google Colab results in UTF-8 locale error - https://github.com/huggingface/huggingface_hub/issues/2588\n",
            "53: 429 when uploading many files - https://github.com/huggingface/huggingface_hub/issues/2587\n",
            "54: 401 Client Error - Issue with logging with Huggingface CLI/code - https://github.com/huggingface/huggingface_hub/issues/2586\n",
            "55: Error formatting fails to capture request ID and includes raw HTML - https://github.com/huggingface/huggingface_hub/issues/2583\n",
            "56: 504 Server Error: Gateway Time-out - https://github.com/huggingface/huggingface_hub/issues/2581\n",
            "57: Mixed permissions of blobs/locks in a multi-user Hub cache - https://github.com/huggingface/huggingface_hub/issues/2580\n",
            "58: What is the highest Python version currently supported? - https://github.com/huggingface/huggingface_hub/issues/2578\n",
            "59: upload_folder func fails and no proper error message is indicated - https://github.com/huggingface/huggingface_hub/issues/2577\n",
            "60: snapshot_download doesn't download files but also doesn't give any error message - https://github.com/huggingface/huggingface_hub/issues/2576\n",
            "61: Upload folder fails for a parquet file inside folder - https://github.com/huggingface/huggingface_hub/issues/2574\n",
            "62: Metadata Title Validation Error - https://github.com/huggingface/huggingface_hub/issues/2573\n",
            "63: Can we add torch_dtype information to the hf_api.model_info response? - https://github.com/huggingface/huggingface_hub/issues/2571\n",
            "64: defog/sqlcoder-70b-alpha autotokenizer error - https://github.com/huggingface/huggingface_hub/issues/2568\n",
            "65: Raise root error when tring to login and permission denied - https://github.com/huggingface/huggingface_hub/issues/2565\n",
            "66: Model card metadata format is not preserved when loaded + saved again - https://github.com/huggingface/huggingface_hub/issues/2564\n",
            "67: Issue: TypeError: HfApi.upload_large_folder() got an unexpected keyword argument 'path_in_repo' - https://github.com/huggingface/huggingface_hub/issues/2563\n",
            "68: transformers dependency_versions_check.py causes circular import when trying to use transformers.LlamaTokenizerFast - https://github.com/huggingface/huggingface_hub/issues/2562\n",
            "69: oom by huggingface-cli - https://github.com/huggingface/huggingface_hub/issues/2560\n",
            "70: HfHubHTTPError: 500 Server Error: Internal Server Error - https://github.com/huggingface/huggingface_hub/issues/2559\n",
            "71: Some InferenceClient tasks missing parameters argument, inconsistent with task specifications - https://github.com/huggingface/huggingface_hub/issues/2557\n",
            "72: [Feature request] Papers API - https://github.com/huggingface/huggingface_hub/issues/2553\n",
            "73: fs.exists(hub_model_id, refresh=True) does not work correctly after deleting a repository - https://github.com/huggingface/huggingface_hub/issues/2552\n",
            "74: Method HfApi.grant_access only works for models - https://github.com/huggingface/huggingface_hub/issues/2550\n",
            "75: Feature: Re-enable symlinking models downloaded from Hub by default - https://github.com/huggingface/huggingface_hub/issues/2548\n",
            "76: Lock acquisiton fails on download - https://github.com/huggingface/huggingface_hub/issues/2543\n",
            "77: huggingface-cli upload - _wrapped_lfs_upload should retry | http_backoff retry not working with _upload_multi_part/SliceFileObj - https://github.com/huggingface/huggingface_hub/issues/2539\n",
            "78: Falcon-7B-Instruct Model: HfHubHTTPError 422 When Using chat_completion with JSON Response Format - https://github.com/huggingface/huggingface_hub/issues/2538\n",
            "79: Feature: switch visibility with update_repo_settings - https://github.com/huggingface/huggingface_hub/issues/2537\n",
            "80: Add num_following and num_followers to User returned by get_user_overview - https://github.com/huggingface/huggingface_hub/issues/2535\n",
            "81: Why uploading single part file doesn't take with header - https://github.com/huggingface/huggingface_hub/issues/2533\n",
            "82: Clarify documentation on list_models's sort attribute - https://github.com/huggingface/huggingface_hub/issues/2527\n",
            "83: How can I rename folders in given repo? I need to rename folders - https://github.com/huggingface/huggingface_hub/issues/2526\n",
            "84: Keyerror 'safe' in load_dataset - https://github.com/huggingface/huggingface_hub/issues/2524\n",
            "85: KeyError: 'safe' when calling HfFileSystem.ls() - https://github.com/huggingface/huggingface_hub/issues/2523\n",
            "86: [InferenceClient / langchain-HuggingFaceEndpoint] impossible to used Langchain HuggingFaceEndpoint whith trust_env=True - https://github.com/huggingface/huggingface_hub/issues/2522\n",
            "87: AsyncInferenceClient not closing the answer properly when response stream is not entirely consumed - https://github.com/huggingface/huggingface_hub/issues/2521\n",
            "88: OSError: Consistency check failed - https://github.com/huggingface/huggingface_hub/issues/2520\n",
            "89: Error while fetching HF_TOKEN - https://github.com/huggingface/huggingface_hub/issues/2519\n",
            "90: api.upload_large_folder consumes api request limit even when verifying which files were previously uploaded - https://github.com/huggingface/huggingface_hub/issues/2518\n",
            "91: TODO: remove this once https://github.com/huggingface/safetensors/pull/449 is merged. - https://github.com/huggingface/huggingface_hub/issues/2516\n",
            "92: Hugging Face Space: Exception in ASGI application - https://github.com/huggingface/huggingface_hub/issues/2515\n",
            "93: InferenceClient: TypeError: 'NoneType' object is not subscriptable if max_tokens is too big - https://github.com/huggingface/huggingface_hub/issues/2514\n",
            "94: Error not reported correctly by hf_raise_for_status in create_inference_endpoint - https://github.com/huggingface/huggingface_hub/issues/2510\n",
            "95: Misleading doc for instance types and sizes in create_inference_endpoint - https://github.com/huggingface/huggingface_hub/issues/2509\n",
            "96: Fix pagination in list_repo_likers - https://github.com/huggingface/huggingface_hub/issues/2508\n",
            "97: Authorization header is correct, but the token seems invalid - https://github.com/huggingface/huggingface_hub/issues/2507\n",
            "98: Need to add the max-workers argument to the huggingface-cli command - https://github.com/huggingface/huggingface_hub/issues/2499\n",
            "99: [Bug]Permission error when i migrate the model from one user to another with folder access granted - https://github.com/huggingface/huggingface_hub/issues/2495\n",
            "100: client.sentence_similarity() does not use correct route by default - https://github.com/huggingface/huggingface_hub/issues/2494\n",
            "101: AsyncInferenceClient not closing connection properly when answer stream is not entierly consumed - https://github.com/huggingface/huggingface_hub/issues/2493\n",
            "102: How to uplaod folders into repo with most effective way - on error continue resume max speed - https://github.com/huggingface/huggingface_hub/issues/2491\n",
            "103: Passing Secrets to the create_inference_endpoint python client - https://github.com/huggingface/huggingface_hub/issues/2485\n",
            "104: Passing a HF endpoint URL to client.chat_completion() doesn't seem to work anymore - https://github.com/huggingface/huggingface_hub/issues/2484\n",
            "105: Process hung when downloading model - https://github.com/huggingface/huggingface_hub/issues/2483\n",
            "106: Endpoint cannot be created if model repo name is >60 characters - https://github.com/huggingface/huggingface_hub/issues/2481\n",
            "107: How to use the HF Nvidia NIM API with the HF inference client? - https://github.com/huggingface/huggingface_hub/issues/2480\n",
            "108: Model Card: Allow for dicts in datasets and base_model and also update spec - https://github.com/huggingface/huggingface_hub/issues/2479\n",
            "109: ImportError: cannot import name 'ModelFilter' from 'huggingface_hub' - https://github.com/huggingface/huggingface_hub/issues/2478\n",
            "110: endpoint.update() removes environment variables of endpoint - https://github.com/huggingface/huggingface_hub/issues/2472\n",
            "111: To unify text_generation()'s stop_sequences and TextGenerationInput's stop params - https://github.com/huggingface/huggingface_hub/issues/2471\n",
            "112: How can I modify this repo files downloader jupyter notebook script to improve downloading speed? Perhaps multiple downloads at the same time? - https://github.com/huggingface/huggingface_hub/issues/2468\n",
            "113: sentiment - https://github.com/huggingface/huggingface_hub/issues/2467\n",
            "114: Add support for /auth-check - https://github.com/huggingface/huggingface_hub/issues/2466\n",
            "115: OSError: Consistency check failed: file should be of size 28754810880 but has size 26303841200 - https://github.com/huggingface/huggingface_hub/issues/2464\n",
            "116: add support for setting scaleToZeroTimeout with create_inference_endpoint/update_inference_endpoint - https://github.com/huggingface/huggingface_hub/issues/2462\n",
            "117: Problem with asyncc - https://github.com/huggingface/huggingface_hub/issues/2461\n",
            "118: 504 Server Error: Gateway Time-out - https://github.com/huggingface/huggingface_hub/issues/2460\n",
            "119: Gateway Timeout for mistralai/Mixtral-8x7B-Instruct-v0.1 - https://github.com/huggingface/huggingface_hub/issues/2459\n",
            "120: Gated dataset info is leaked - https://github.com/huggingface/huggingface_hub/issues/2457\n",
            "121: AsyncInferenceClient.chat_completion streaming is broken with TGI 2.2.0 - https://github.com/huggingface/huggingface_hub/issues/2455\n",
            "122: Download one GGUF model rather than all kinds of quantized models - https://github.com/huggingface/huggingface_hub/issues/2454\n",
            "123: huggingface-cli upload - Validate README.md before file hashing - https://github.com/huggingface/huggingface_hub/issues/2451\n",
            "124: [Feat] Endpoint configure gating on a repo - https://github.com/huggingface/huggingface_hub/issues/2447\n",
            "125: Support multiple tokens locally - https://github.com/huggingface/huggingface_hub/issues/2446\n",
            "126: HF Hub gets stuck in infinite loop for uploading datasets from Hetzner storage - https://github.com/huggingface/huggingface_hub/issues/2445\n",
            "127: add support for a version command - https://github.com/huggingface/huggingface_hub/issues/2441\n",
            "128: SecurityStatus missing in hf_api.model_info() - https://github.com/huggingface/huggingface_hub/issues/2438\n",
            "129: Is there any equivlent to text_generation special tokens in Hugginface-hub InterfaceClient - https://github.com/huggingface/huggingface_hub/issues/2436\n",
            "130: Redundant assignment of URL in file_download.py - https://github.com/huggingface/huggingface_hub/issues/2435\n",
            "131: [nit] in spaces build logs, hf_transfer is replaced with hf_******** as if it was a token - https://github.com/huggingface/huggingface_hub/issues/2430\n",
            "132: hfapi.list_models() support list the top K trending models for TGI/TEI/Diffusion models - https://github.com/huggingface/huggingface_hub/issues/2427\n",
            "133: New failure when loading dataset-- 500 from hub - https://github.com/huggingface/huggingface_hub/issues/2425\n",
            "134: Support regex of filename in download - https://github.com/huggingface/huggingface_hub/issues/2424\n",
            "135: response_format with regex does not seem to work - https://github.com/huggingface/huggingface_hub/issues/2423\n",
            "136: InferenceClient doesn't use proxies set in environment variables - https://github.com/huggingface/huggingface_hub/issues/2420\n",
            "137: Different behavior when passing exist_ok - https://github.com/huggingface/huggingface_hub/issues/2419\n",
            "138: notebook_login() fails for jupyter notebook - https://github.com/huggingface/huggingface_hub/issues/2417\n",
            "139: glob * as shown in CLI code comments don't work as expected - https://github.com/huggingface/huggingface_hub/issues/2416\n",
            "140: Incompatibility with Empty Enum List in Tool Parameters - https://github.com/huggingface/huggingface_hub/issues/2415\n",
            "141: InferenceClient alignment with base_url as in OpenAI client - https://github.com/huggingface/huggingface_hub/issues/2414\n",
            "142: Commit creation on pr error when using v0.24.0 - https://github.com/huggingface/huggingface_hub/issues/2411\n",
            "143: big file upload complete_multipart network request error - https://github.com/huggingface/huggingface_hub/issues/2409\n",
            "144: Impacts of removing modelId - https://github.com/huggingface/huggingface_hub/issues/2408\n",
            "145: Add support for remote filesystem paths in Hugging Face CLI (--local-dir) - https://github.com/huggingface/huggingface_hub/issues/2407\n",
            "146: Rollback operation for a branch of repositories - https://github.com/huggingface/huggingface_hub/issues/2406\n",
            "147: ImportError: cannot import name 'DatasetFilter' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/init.py) - https://github.com/huggingface/huggingface_hub/issues/2401\n",
            "148: v0.23 download method breaks on filesystems that don't have flock enabled - https://github.com/huggingface/huggingface_hub/issues/2399\n",
            "149: Is there a way to extract a model's download stats (e.g., last 30 days) in times series format? - https://github.com/huggingface/huggingface_hub/issues/2390\n",
            "150: \"most likely due to a circular import\" - https://github.com/huggingface/huggingface_hub/issues/2387\n",
            "151: token=False doesn't work for hf_hub_download - https://github.com/huggingface/huggingface_hub/issues/2385\n",
            "152: Docs: Local Preview URL Discrepancy - https://github.com/huggingface/huggingface_hub/issues/2381\n",
            "153: ModelHubMixin config support throws error - https://github.com/huggingface/huggingface_hub/issues/2379\n",
            "154: ImportError: huggingface-hub>=0.19.3,<1.0 is required for a normal functioning of this module, but found huggingface-hub==0.17.1 - https://github.com/huggingface/huggingface_hub/issues/2377\n",
            "155: snapshot_download should ideally provide a similar API experience to load_dataset (subdirectory download support) - https://github.com/huggingface/huggingface_hub/issues/2376\n",
            "156: huggingface_hub.utils._errors.HfHubHTTPError: 504 Server Error: Gateway Time-out for url - https://github.com/huggingface/huggingface_hub/issues/2375\n",
            "157: huggingface-cli fails to download model because of missing .incomplete file - https://github.com/huggingface/huggingface_hub/issues/2374\n",
            "158: OSError for Consistency check failed and the force_download=True doesn't work. - https://github.com/huggingface/huggingface_hub/issues/2372\n",
            "159: support custom cache path - https://github.com/huggingface/huggingface_hub/issues/2370\n",
            "160: Truly openai drop-in replacement for chat completion - https://github.com/huggingface/huggingface_hub/issues/2369\n",
            "161: Checksum validation with hf_hub_download on model files. - https://github.com/huggingface/huggingface_hub/issues/2364\n",
            "162: huggingface_hub dowload failed - https://github.com/huggingface/huggingface_hub/issues/2362\n",
            "163: Can't download one file of a public huggingface dataset - https://github.com/huggingface/huggingface_hub/issues/2361\n",
            "164: http_backoff does not respect Retry-After headers on 429 requests - https://github.com/huggingface/huggingface_hub/issues/2360\n",
            "165: huggingface_hub download failing when grandparent of the destination folder is not writable - https://github.com/huggingface/huggingface_hub/issues/2359\n",
            "166: 400: Bad Request when accessing a space - https://github.com/huggingface/huggingface_hub/issues/2357\n",
            "167: Always Receiving the \"resume_download is deprecated error\" - https://github.com/huggingface/huggingface_hub/issues/2356\n",
            "168: huggingface-cli download: FileNotFoundError No such file or directory - https://github.com/huggingface/huggingface_hub/issues/2355\n",
            "169: hey can u plz remove the git clone cli - https://github.com/huggingface/huggingface_hub/issues/2353\n",
            "170: CommitOperationAdd cant be imported from huggingface_hub - https://github.com/huggingface/huggingface_hub/issues/2352\n",
            "171: Replace Deprectated Repository with HfApi - https://github.com/huggingface/huggingface_hub/issues/2351\n",
            "172: Handle proxy with AsyncInferenceClient post request - https://github.com/huggingface/huggingface_hub/issues/2349\n",
            "173: Documentation of API Client stale - https://github.com/huggingface/huggingface_hub/issues/2348\n",
            "174: It seem that proxies doesn't pass to get the model info when use snapshot_download - https://github.com/huggingface/huggingface_hub/issues/2343\n",
            "175: null is not defined - https://github.com/huggingface/huggingface_hub/issues/2341\n",
            "176: hardware args in docs for creating inference endpoints are outdated - https://github.com/huggingface/huggingface_hub/issues/2339\n",
            "177: [PyTorchModelHubMixin] argparse.Namespace config does not seem to be pushed - https://github.com/huggingface/huggingface_hub/issues/2334\n",
            "178: snapshot_download downloads files wrong in Google Colab - https://github.com/huggingface/huggingface_hub/issues/2332\n",
            "179: huggingface Error: t.at is not a function - https://github.com/huggingface/huggingface_hub/issues/2327\n",
            "180: updated, not a bug - https://github.com/huggingface/huggingface_hub/issues/2326\n",
            "181: Support getting downloads from all time - https://github.com/huggingface/huggingface_hub/issues/2325\n",
            "182: Connection Error - https://github.com/huggingface/huggingface_hub/issues/2319\n",
            "183: details is always None with text_generation - https://github.com/huggingface/huggingface_hub/issues/2315\n",
            "184: MaxRetryError with login in huggingface_hub[cli] - https://github.com/huggingface/huggingface_hub/issues/2312\n",
            "185: ModuleNotFoundError: No module named 'huggingface_hub' - https://github.com/huggingface/huggingface_hub/issues/2307\n",
            "186: [Feature request] Set environment variable in inference endpoint (custom docker image) - https://github.com/huggingface/huggingface_hub/issues/2301\n",
            "187: Metadata is missed in child classes for models with HubMixinModel - https://github.com/huggingface/huggingface_hub/issues/2300\n",
            "188: better logs for ModelHubMixin unserialized parameters - https://github.com/huggingface/huggingface_hub/issues/2297\n",
            "189: Separate out model and base_url in InferenceClient - https://github.com/huggingface/huggingface_hub/issues/2293\n",
            "190: HF Spaces inference failing with error: huggingface_hub.utils._errors.LocalEntryNotFoundError - https://github.com/huggingface/huggingface_hub/issues/2290\n",
            "191: Datasets with invalid repo_id - https://github.com/huggingface/huggingface_hub/issues/2289\n",
            "192: Symlink snapshot_download files from cache - https://github.com/huggingface/huggingface_hub/issues/2284\n",
            "193: Union typehint in conjunction with coders causes error for PyTorchModelHubMixin - https://github.com/huggingface/huggingface_hub/issues/2283\n",
            "194: Ability to retrieve the protocol response headers in InferenceClient - https://github.com/huggingface/huggingface_hub/issues/2281\n",
            "195: Enabling hf-transfer by default - https://github.com/huggingface/huggingface_hub/issues/2279\n",
            "196: HfAPI().create_inference_endpoint errors and does not follow documentation - https://github.com/huggingface/huggingface_hub/issues/2277\n",
            "197: Failed to download Phi-3-mini-4k-instruct to local dir - https://github.com/huggingface/huggingface_hub/issues/2276\n",
            "198: Any AutoConfig.from_pretrained call results in FutureWarning: resume_download is deprecated and will be removed in version 1.0.0. - https://github.com/huggingface/huggingface_hub/issues/2275\n",
            "199: Invalid input data for ImageToTextOutput: Expected a single instance, but got a list. - https://github.com/huggingface/huggingface_hub/issues/2273\n",
            "200: Timeout when downloading dataset metadata with 8 torchrun workers - https://github.com/huggingface/huggingface_hub/issues/2272\n",
            "201: ValueError: Unable to retrieve user and repo ID from the passed HF ID: https://huggingface.co/MonolithFoundation/Bumblebee-Light - https://github.com/huggingface/huggingface_hub/issues/2268\n",
            "202: Consider using hf_transfer for faster uploads. T - https://github.com/huggingface/huggingface_hub/issues/2267\n",
            "203: Filter by tags with list_datasets() API - https://github.com/huggingface/huggingface_hub/issues/2265\n",
            "204: can't dowload the GGUF, it progress stoped at 1% - https://github.com/huggingface/huggingface_hub/issues/2255\n",
            "205: Harmonization of token Parameter Descriptions and Type Definitions in hf_api.py - https://github.com/huggingface/huggingface_hub/issues/2251\n",
            "206: No package metadata was found for bitsandbytes - https://github.com/huggingface/huggingface_hub/issues/2249\n",
            "207: Content-Range header for multiple part request - https://github.com/huggingface/huggingface_hub/issues/2248\n",
            "208: Document question answering returns empty list without an error - https://github.com/huggingface/huggingface_hub/issues/2247\n",
            "209: Issue occured when using jupyter lab - https://github.com/huggingface/huggingface_hub/issues/2246\n",
            "210: I'm 🙏🏻begging you for an 🙅🏻‍♂️UnfollowButton - https://github.com/huggingface/huggingface_hub/issues/2245\n",
            "211: Change HUGGINGFACE_CO_URL_TEMPLATE with environment variable - https://github.com/huggingface/huggingface_hub/issues/2244\n",
            "212: cli download --local-dir broken - https://github.com/huggingface/huggingface_hub/issues/2238\n",
            "213: [CLI] Command line to delete files on a repo - https://github.com/huggingface/huggingface_hub/issues/2235\n",
            "214: Error using from_pretrained_keras model - https://github.com/huggingface/huggingface_hub/issues/2234\n",
            "215: ImportError: cannot import name 'is_jsonable' from 'huggingface_hub.utils' (/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/__init__.py) - https://github.com/huggingface/huggingface_hub/issues/2231\n",
            "216: Input should be a valid dictionary or instance of SentenceSimilarityInputsCheck: received `Context information is below. - https://github.com/huggingface/huggingface_hub/issues/2227\n",
            "217: chat_completion gives no error for wrong input tupe - https://github.com/huggingface/huggingface_hub/issues/2225\n",
            "218: Give .safetensors metadata in list_models() response - https://github.com/huggingface/huggingface_hub/issues/2224\n",
            "219: set PROXY and Certificate for requests.Sessions - https://github.com/huggingface/huggingface_hub/issues/2222\n",
            "220: Select which files to delete in huggingface-cli delete-cache - https://github.com/huggingface/huggingface_hub/issues/2219\n",
            "221: huggingface-cli scan-cache doesn't capture cached datasets - https://github.com/huggingface/huggingface_hub/issues/2218\n",
            "222: add omegaconf to _JSON_SERIALIZABLE_TYPES - https://github.com/huggingface/huggingface_hub/issues/2212\n",
            "223: gpt2 download keeps failing at consistency checking for 64-8bits.tflite - https://github.com/huggingface/huggingface_hub/issues/2210\n",
            "224: ConnectionResetError: [Errno 54] Connection reset by peer - https://github.com/huggingface/huggingface_hub/issues/2207\n",
            "225: Huggingface_hub import fails with partially initialized module when EAGER_IMPORT=true - https://github.com/huggingface/huggingface_hub/issues/2206\n",
            "226: Hugginface_CLI local_dir_use_symlink doesn't work. - https://github.com/huggingface/huggingface_hub/issues/2203\n",
            "227: error with snapshot_download - https://github.com/huggingface/huggingface_hub/issues/2200\n",
            "228: Error on *.from_pretrained call: Unexpected ('Connection broken: IncompleteRead( 443 - https://github.com/huggingface/huggingface_hub/issues/2199\n",
            "229: Often print: requests.exceptions.HTTPError: 416 Client Error: Range Not Satisfiable for url - https://github.com/huggingface/huggingface_hub/issues/2197\n",
            "230: possible issue with filelock/.locks folder - https://github.com/huggingface/huggingface_hub/issues/2193\n",
            "231: ModelInfo bug - https://github.com/huggingface/huggingface_hub/issues/2186\n",
            "232: add customizable auth token path HF_HUB_TOKEN_PATH - https://github.com/huggingface/huggingface_hub/issues/2181\n",
            "233: HF_HUB_DISABLE_SYMLINKS_WARNING - https://github.com/huggingface/huggingface_hub/issues/2179\n",
            "234: Tensorboard Not Displaying - https://github.com/huggingface/huggingface_hub/issues/2176\n",
            "235: How get the access of modifying past commit info for huggingface repos? - https://github.com/huggingface/huggingface_hub/issues/2187\n",
            "236: 429 error in InferenceClient - https://github.com/huggingface/huggingface_hub/issues/2175\n",
            "237: \"Error: The XML you provided was not well-formed or did not validate against our published schema\" when uploading dataset to Huggingface Hub - https://github.com/huggingface/huggingface_hub/issues/2162\n",
            "238: where to download module named 'haiku' - https://github.com/huggingface/huggingface_hub/issues/2161\n",
            "239: No indication of virus/malware scanning for pytorch bins and safetensors. Only JSON files are scanned - https://github.com/huggingface/huggingface_hub/issues/2160\n",
            "240: Cannot initialize SetFitModel with huggingface_hub == 0.22.0 - https://github.com/huggingface/huggingface_hub/issues/2157\n",
            "241: autofill repo_url field when using ModelHubMixin to generate a model card - https://github.com/huggingface/huggingface_hub/issues/2154\n",
            "242: use HF_ENDPOINT will still be connected to huggingface.co - https://github.com/huggingface/huggingface_hub/issues/2152\n",
            "243: I can't download zoe depth anything's modles like indoor and outdoor,even I download them manualy,the error still there - https://github.com/huggingface/huggingface_hub/issues/2149\n",
            "244: [docs] create a diagram image that explains the hf:// paths - https://github.com/huggingface/huggingface_hub/issues/2148\n",
            "245: older versions breaking down for PytorchModelHubMixin - https://github.com/huggingface/huggingface_hub/issues/2144\n",
            "246: [CLI] Support tag management (create, list, delete) - https://github.com/huggingface/huggingface_hub/issues/2141\n",
            "247: Text generation with some non TGI models hangs - https://github.com/huggingface/huggingface_hub/issues/2135\n",
            "248: [Feature Request] Support COEP/COOP headers on Spaces running via Docker SDK - https://github.com/huggingface/huggingface_hub/issues/2127\n",
            "249: Adding a new task \"SciML/Physical Science/AI4Science: Atomistic Simulations\" - https://github.com/huggingface/huggingface_hub/issues/2123\n",
            "250: Throttle download speed - https://github.com/huggingface/huggingface_hub/issues/2118\n",
            "251: Add a utility in huggingface-cli to migrate the entire cache to a new location - https://github.com/huggingface/huggingface_hub/issues/2117\n",
            "252: Add to documentation all fo the *_CACHE environment variables that can be configured. - https://github.com/huggingface/huggingface_hub/issues/2116\n",
            "253: huggingface-cli fails to upload when HF_ENDPOINT is set - https://github.com/huggingface/huggingface_hub/issues/2114\n",
            "254: MacOS .DS_Store makes CLI's scan-cache think the cache is corrupted - https://github.com/huggingface/huggingface_hub/issues/2110\n",
            "255: Loading local files with Interface Endpoints doesn't work - UniKP - https://github.com/huggingface/huggingface_hub/issues/2106\n",
            "256: Making ModelHubMixin.save_pretrained save self.config on demand for integrated libraries - https://github.com/huggingface/huggingface_hub/issues/2102\n",
            "257: Document more methods of HfFileSystem - https://github.com/huggingface/huggingface_hub/issues/2100\n",
            "258: kwargs not loading when using PytorchModelHubMixin - https://github.com/huggingface/huggingface_hub/issues/2096\n",
            "259: Cache folder gets created even if model name does not exist - https://github.com/huggingface/huggingface_hub/issues/2091\n",
            "260: incorrect cache folder name capitalization - https://github.com/huggingface/huggingface_hub/issues/2090\n",
            "261: huggingface_hub cannot import name 'hf_hub_download' from 'huggingface_hub' - https://github.com/huggingface/huggingface_hub/issues/2089\n",
            "262: [i18n-<CrH-CrH>] Translating docs to <Haitian Creole> - https://github.com/huggingface/huggingface_hub/issues/2087\n",
            "263: Cannot find huggingface_hub's version in importlib_metadata? - https://github.com/huggingface/huggingface_hub/issues/2085\n",
            "264: upload resume - https://github.com/huggingface/huggingface_hub/issues/2083\n",
            "265: cant login to huggingface hub ( ConnectionResetError 10054 ) - https://github.com/huggingface/huggingface_hub/issues/2082\n",
            "266: Disable cache on Inference APIs - https://github.com/huggingface/huggingface_hub/issues/2081\n",
            "267: Does huggingface-cli support convert to safetensor . - https://github.com/huggingface/huggingface_hub/issues/2078\n",
            "268: Support User API endpoints - https://github.com/huggingface/huggingface_hub/issues/2077\n",
            "269: Support sharding in PyTorchHubMixin - https://github.com/huggingface/huggingface_hub/issues/2076\n",
            "270: Pass config in each __init__ arg in PyTorchHubMixin - https://github.com/huggingface/huggingface_hub/issues/2075\n",
            "271: Create default modelcard in PyTorchHubMixin - https://github.com/huggingface/huggingface_hub/issues/2074\n",
            "272: Define all errors in ./src/huggingface_hub/errors.py - https://github.com/huggingface/huggingface_hub/issues/2069\n",
            "273: Add support for /oauth/userinfo - https://github.com/huggingface/huggingface_hub/issues/2068\n",
            "274: Build a Debian build package (.deb) - https://github.com/huggingface/huggingface_hub/issues/2067\n",
            "275: Let's deprecate InferenceClient.conversational - https://github.com/huggingface/huggingface_hub/issues/2066\n",
            "276: Implement save_state_dict and load_state_dict in serialization module - https://github.com/huggingface/huggingface_hub/issues/2065\n",
            "277: Implement chat-based inference in InferenceClient - https://github.com/huggingface/huggingface_hub/issues/2064\n",
            "278: Use inference types dataclasses everywhere - https://github.com/huggingface/huggingface_hub/issues/2063\n",
            "279: Make CI even faster? - https://github.com/huggingface/huggingface_hub/issues/2062\n",
            "280: huggingface_hub.utils._errors.HfHubHTTPError: 503 Server Error: Service Temporarily Unavailable for url: https://huggingface.co/timm/vit_large_patch16_384.augreg_in21k_ft_in1k/resolve/main/model.safetensors - https://github.com/huggingface/huggingface_hub/issues/2060\n",
            "281: list_models(cardData = True) no longer functions - https://github.com/huggingface/huggingface_hub/issues/2057\n",
            "282: How edit cache dir and in bad net download how to redownload with last download point - https://github.com/huggingface/huggingface_hub/issues/2051\n",
            "283: TypeError: TextGenerationStreamResponse.__init__() got an unexpected keyword argument 'index' - https://github.com/huggingface/huggingface_hub/issues/2046\n",
            "284: Getting the hash of the current local \"default\" model/dataset - https://github.com/huggingface/huggingface_hub/issues/2045\n",
            "285: Unable to access Huggingface - https://github.com/huggingface/huggingface_hub/issues/2043\n",
            "286: How to find out the type of files in the repository - https://github.com/huggingface/huggingface_hub/issues/2039\n",
            "287: models cannot be stored on networks storage due to unhandled file lock errors - https://github.com/huggingface/huggingface_hub/issues/2038\n",
            "288: InferenceClient and AsyncInferenceClient breaks when streaming text generation - https://github.com/huggingface/huggingface_hub/issues/2034\n",
            "289: Does huggingface-cli support to qery if model is in safetensor or not - https://github.com/huggingface/huggingface_hub/issues/2032\n",
            "290: huggingface_hub.login() does not accept copy&paste - https://github.com/huggingface/huggingface_hub/issues/2031\n",
            "291: upload_folder() fails to upload large datasets - https://github.com/huggingface/huggingface_hub/issues/2030\n",
            "292: api.get_repo_discussions never ends for some model repositories when using huggingface_hub==0.20.3 - https://github.com/huggingface/huggingface_hub/issues/2029\n",
            "293: list_models() returns no model_index and therefore no model cards - https://github.com/huggingface/huggingface_hub/issues/2025\n",
            "294: Error when calling InferenceClient.conversational - https://github.com/huggingface/huggingface_hub/issues/2023\n",
            "295: Unable to pass no_repeat_ngram_size in text_generation - https://github.com/huggingface/huggingface_hub/issues/2022\n",
            "296: transformer version on the hub - https://github.com/huggingface/huggingface_hub/issues/2021\n",
            "297: Investigate overhead when listing spaces - https://github.com/huggingface/huggingface_hub/issues/2017\n",
            "298: Support audio_to_audio task in InferenceClient - https://github.com/huggingface/huggingface_hub/issues/2016\n",
            "299: The tabular_regression and tabular_classification methods in InferenceClient don't use the default model name - https://github.com/huggingface/huggingface_hub/issues/2015\n",
            "300: ModelFilter uses tag rather than pipeline_tag - https://github.com/huggingface/huggingface_hub/issues/2009\n",
            "301: Cannot have -- or .. in repo_id - https://github.com/huggingface/huggingface_hub/issues/2008\n",
            "302: Downloads of snapshot_download are not counted in the model card - https://github.com/huggingface/huggingface_hub/issues/2007\n",
            "303: TextGenerationStreamResponse missing a key present in the stream response - https://github.com/huggingface/huggingface_hub/issues/2005\n",
            "304: Issue with SentenceTransformer Model Validation for Local Paths - https://github.com/huggingface/huggingface_hub/issues/2004\n",
            "305: Uploading a Model (Python push_to_hub) - https://github.com/huggingface/huggingface_hub/issues/2002\n",
            "306: Is it possible to create a pull request from an existing branch? - https://github.com/huggingface/huggingface_hub/issues/1998\n",
            "307: huggingface-cli delete-cache --disable-tui improvements - https://github.com/huggingface/huggingface_hub/issues/1997\n",
            "308: TQDM: AttibuteError del tqdm_class._lock on async / threaded usage - https://github.com/huggingface/huggingface_hub/issues/1994\n",
            "309: Use safetensors by default for PyTorchModelHubMixin class - https://github.com/huggingface/huggingface_hub/issues/1989\n",
            "310: Show model/repo name when downloading - https://github.com/huggingface/huggingface_hub/issues/1984\n",
            "311: Inconsistent warning raising - https://github.com/huggingface/huggingface_hub/issues/1978\n",
            "312: test_from_raw_initialization seems to be missing daylight savings? - https://github.com/huggingface/huggingface_hub/issues/1976\n",
            "313: Many tests fail when on tight VPN - https://github.com/huggingface/huggingface_hub/issues/1975\n",
            "314: Testing logs hung on logging into Hugging Face - https://github.com/huggingface/huggingface_hub/issues/1973\n",
            "315: Request: setup.py extra for hf_transfer - https://github.com/huggingface/huggingface_hub/issues/1964\n",
            "316: Docs request: CLI default of --repo-type - https://github.com/huggingface/huggingface_hub/issues/1963\n",
            "317: Listing and Downloading models file recursively - https://github.com/huggingface/huggingface_hub/issues/1969\n",
            "318: from_pretrained_fastai does not download files - https://github.com/huggingface/huggingface_hub/issues/1961\n",
            "319: Always broken when nearly finish. - https://github.com/huggingface/huggingface_hub/issues/1959\n",
            "320: Missing = None in GitRefs for pull_requests. - https://github.com/huggingface/huggingface_hub/issues/1956\n",
            "321: userdata.get('HF_TOKEN') hangs forever on Google Colab - https://github.com/huggingface/huggingface_hub/issues/1952\n",
            "322: BadRequestError due to python/tuple in YAML - https://github.com/huggingface/huggingface_hub/issues/1950\n",
            "323: Parameter revision is not taken into account by HfFileSystem - https://github.com/huggingface/huggingface_hub/issues/1947\n",
            "324: Consistency check failed: file should be of size 440449768 but has size 4122356 (model.safetensors). - https://github.com/huggingface/huggingface_hub/issues/1946\n",
            "325: Cannot get a repo info which just created in a same thread. - https://github.com/huggingface/huggingface_hub/issues/1945\n",
            "326: In a local JupyterLab that is not a Google Colab environment, _get_token_from_google_colab freezes and stops responding - https://github.com/huggingface/huggingface_hub/issues/1944\n",
            "327: The best_of parameter in a non-TGI server - https://github.com/huggingface/huggingface_hub/issues/1943\n",
            "328: Arbled characters when using snapshot_download and hf_hub_download to download files with cn/jp/kr names - https://github.com/huggingface/huggingface_hub/issues/1942\n",
            "329: Key error [0] - https://github.com/huggingface/huggingface_hub/issues/1941\n",
            "330: Add support for truncate and normalize to InferenceClient.feature_extraction() - https://github.com/huggingface/huggingface_hub/issues/1939\n",
            "331: Maximum individual file size is 50.0GB error when my file is less than 50.0GB - https://github.com/huggingface/huggingface_hub/issues/1933\n",
            "332: Connection error with 0.20.1 - https://github.com/huggingface/huggingface_hub/issues/1932\n",
            "333: Logout raises warning in Colab - https://github.com/huggingface/huggingface_hub/issues/1929\n",
            "334: Fail early if modelcard metadata is not correct on create_commit - https://github.com/huggingface/huggingface_hub/issues/1927\n",
            "335: Make note that token from hugging face will not echo - https://github.com/huggingface/huggingface_hub/issues/1924\n",
            "336: Oversized cache folder, multiple downloads of the same checkpoints. - https://github.com/huggingface/huggingface_hub/issues/1922\n",
            "337: Using DISABLE_TELEMETRY is deprecated - https://github.com/huggingface/huggingface_hub/issues/1917\n",
            "338: [i18n-CN] Translating docs to Simplified Chinese - https://github.com/huggingface/huggingface_hub/issues/1915\n",
            "339: Use dataclasses for all objects returned by HfApi - https://github.com/huggingface/huggingface_hub/issues/1911\n",
            "340: hf_hub_download() not checking the local_dir and just return pointer_path - https://github.com/huggingface/huggingface_hub/issues/1909\n",
            "341: Upload speed not displayed - https://github.com/huggingface/huggingface_hub/issues/1908\n",
            "342: How to fix \"VBox(children=(HTML(value='<center> <img...\" error? When trying login() - https://github.com/huggingface/huggingface_hub/issues/1907\n",
            "343: snapshot_download timeout will not resume - https://github.com/huggingface/huggingface_hub/issues/1903\n",
            "344: Improve whoami method - https://github.com/huggingface/huggingface_hub/issues/1902\n",
            "345: [i18n-ta] Translating docs to Tamil - https://github.com/huggingface/huggingface_hub/issues/1901\n",
            "346: Support listening to Space updates - https://github.com/huggingface/huggingface_hub/issues/1898\n",
            "347: [i18n-FR] Translating docs to French - https://github.com/huggingface/huggingface_hub/issues/1897\n",
            "348: Upload anonymously to public repository - https://github.com/huggingface/huggingface_hub/issues/1894\n",
            "349: super slow from_pretrained - https://github.com/huggingface/huggingface_hub/issues/1893\n",
            "350: Enable multi_commit in upload_folder to work for branches/revisions - https://github.com/huggingface/huggingface_hub/issues/1892\n",
            "351: [Feature Request] An operation for collapse large number of commits in hf repo - https://github.com/huggingface/huggingface_hub/issues/1891\n",
            "352: problem with spaces when working with a custom model file - https://github.com/huggingface/huggingface_hub/issues/1883\n",
            "353: PermissionError while using file_download.py with 2 volumes - https://github.com/huggingface/huggingface_hub/issues/1882\n",
            "354: Error on upload_file function comment - https://github.com/huggingface/huggingface_hub/issues/1881\n",
            "355: Error on upload_file function comment - https://github.com/huggingface/huggingface_hub/issues/1880\n",
            "356: Class Repository cannot be subclassed after being decorated with @_deprecate_method() - https://github.com/huggingface/huggingface_hub/issues/1878\n",
            "357: Add the normalize and truncate parameters to feature_extraction to be compatible with Text Embeddings Inference (TEI) - https://github.com/huggingface/huggingface_hub/issues/1877\n",
            "358: TypeError: huggingface_hub.hf_file_system.HfFileSystem.find() got multiple values for keyword argument 'maxdepth' - https://github.com/huggingface/huggingface_hub/issues/1872\n",
            "359: Make commands fail to execute - https://github.com/huggingface/huggingface_hub/issues/1863\n",
            "360: empty string credential.helper causes IndexError when running huggingface-cli login - https://github.com/huggingface/huggingface_hub/issues/1858\n",
            "361: Uncaught KeyError: 'lastCommit' - https://github.com/huggingface/huggingface_hub/issues/1853\n",
            "362: Issue detecting graphviz on macos - https://github.com/huggingface/huggingface_hub/issues/1846\n",
            "363: [CLI] hugging-face init command to help bootstrap new dataset, model, spaces - https://github.com/huggingface/huggingface_hub/issues/1844\n",
            "364: gated attribute of DatasetInfo is returned as string instead of bool - https://github.com/huggingface/huggingface_hub/issues/1843\n",
            "365: Unable to create branches with the --revision CLI flag - https://github.com/huggingface/huggingface_hub/issues/1841\n",
            "366: Unable to run huggingface-cli - https://github.com/huggingface/huggingface_hub/issues/1840\n",
            "367: pydantic deprecated validator, migrate to field_validator - https://github.com/huggingface/huggingface_hub/issues/1836\n",
            "368: API to list Collections - https://github.com/huggingface/huggingface_hub/issues/1835\n",
            "369: Missing test for accept header audio - https://github.com/huggingface/huggingface_hub/issues/1834\n",
            "370: Error in are_symlinks_supported causing hf_hub_download to stall indefinitely on Windows - https://github.com/huggingface/huggingface_hub/issues/1833\n",
            "371: Safetensors metadata remote reader - https://github.com/huggingface/huggingface_hub/issues/1832\n",
            "372: using hf_transfer should download shards in parallel not sequentially - https://github.com/huggingface/huggingface_hub/issues/1831\n",
            "373: conflicting_files of DiscussionWithDetails is returned as a bool - https://github.com/huggingface/huggingface_hub/issues/1830\n",
            "374: Respect .gitignore file in upload_folder - https://github.com/huggingface/huggingface_hub/issues/1826\n",
            "375: [i18n-UR] Translating docs to Urdu - https://github.com/huggingface/huggingface_hub/issues/1820\n",
            "376: Add support for /health and /info endpoints for TGI - https://github.com/huggingface/huggingface_hub/issues/1819\n",
            "377: Concurrent model weight download - https://github.com/huggingface/huggingface_hub/issues/1818\n",
            "378: TypeError: string indices must be integers when iterating over models from list_models - https://github.com/huggingface/huggingface_hub/issues/1817\n",
            "379: Behavior of list_models/datasets/spaces - https://github.com/huggingface/huggingface_hub/issues/1814\n",
            "380: HTTP request to https://huggingface.co/api/repos/move fails unexpectedly - https://github.com/huggingface/huggingface_hub/issues/1813\n",
            "381: Rename CLI to hf or at least put an alias on it - https://github.com/huggingface/huggingface_hub/issues/1812\n",
            "382: Running an inference endpoint for a model locally or remotely from the command line - https://github.com/huggingface/huggingface_hub/issues/1811\n",
            "383: KeyError: 'gcTimeout' on get_space_runtime call - https://github.com/huggingface/huggingface_hub/issues/1810\n",
            "384: Implement Webhooks API - https://github.com/huggingface/huggingface_hub/issues/1808\n",
            "385: SpaceInfo does not load tags from Spaces - https://github.com/huggingface/huggingface_hub/issues/1806\n",
            "386: Entire operation get cancelled when 1 file fails when using api.upload_folder - how to make it iterative - https://github.com/huggingface/huggingface_hub/issues/1801\n",
            "387: Improve error message to write to the Hub using DuckDB when repo doesn't exist - https://github.com/huggingface/huggingface_hub/issues/1800\n",
            "388: Make downloads of models independent of X-Repo-Commit http header to allow download via proxy repositories - https://github.com/huggingface/huggingface_hub/issues/1796\n",
            "389: Model \"xxx\" was not found on hub! - https://github.com/huggingface/huggingface_hub/issues/1794\n",
            "390: Running InferenceClient in docker for TGI web interface - https://github.com/huggingface/huggingface_hub/issues/1790\n",
            "391: Broken stream=True in text_generation for TGI endpoint - https://github.com/huggingface/huggingface_hub/issues/1787\n",
            "392: Proxies parameters of hf_hub_download and snapshot_download cannot proxy LFS's HTTP file download requests. - https://github.com/huggingface/huggingface_hub/issues/1780\n",
            "393: Write a CLI-specific documentation page - https://github.com/huggingface/huggingface_hub/issues/1778\n",
            "394: Upcoming releases - https://github.com/huggingface/huggingface_hub/issues/1777\n",
            "395: [i18n-HI] Translating docs to Hindi - https://github.com/huggingface/huggingface_hub/issues/1771\n",
            "396: Raise RepositoryNotFoundError if repository is not found when adding a repo to a collection - https://github.com/huggingface/huggingface_hub/issues/1769\n",
            "397: Create default modelcard when pushing with ModelHubMixin (and therefore PytorchModelHubMixin as well - https://github.com/huggingface/huggingface_hub/issues/1768\n",
            "398: Request: discerning what the default model is when using InferenceClient without a model - https://github.com/huggingface/huggingface_hub/issues/1767\n",
            "399: Endpoint cannot support http:// protocol with proxy - https://github.com/huggingface/huggingface_hub/issues/1764\n",
            "400: Support src_lang/tgt_lang in InferenceClient.translation() - https://github.com/huggingface/huggingface_hub/issues/1763\n",
            "401: Hitting usage limits, what are rate limits for Pro? - https://github.com/huggingface/huggingface_hub/issues/1762\n",
            "402: Repo set up: autolinks for Hugging Face - https://github.com/huggingface/huggingface_hub/issues/1760\n",
            "403: Request: model_info opt-in inclusion of config.json under ModelInfo.config - https://github.com/huggingface/huggingface_hub/issues/1759\n",
            "404: CLI command to move cache folder - https://github.com/huggingface/huggingface_hub/issues/1757\n",
            "405: Lightweight (and faster) scan-cache to list downloaded files without metadata - https://github.com/huggingface/huggingface_hub/issues/1756\n",
            "406: Vague error of 422 Client Error: Unprocessable Entity - https://github.com/huggingface/huggingface_hub/issues/1753\n",
            "407: Facing issue no module named Newspaper - https://github.com/huggingface/huggingface_hub/issues/1752\n",
            "408: Request: InferenceClient being able to query model metadata - https://github.com/huggingface/huggingface_hub/issues/1751\n",
            "409: Support passing config as dataclass in ModelHubMixin - https://github.com/huggingface/huggingface_hub/issues/1750\n",
            "410: Adding/approving model repo access via API - https://github.com/huggingface/huggingface_hub/issues/1749\n",
            "411: How can I search a ner model with date and money label in the HuggingFace website? Should I enter the homepage of every ner model and look what label it support? - https://github.com/huggingface/huggingface_hub/issues/1748\n",
            "412: Improve download display for end users - https://github.com/huggingface/huggingface_hub/issues/1747\n",
            "413: InferenceClient.feature_extraction allowing for List[str] - https://github.com/huggingface/huggingface_hub/issues/1745\n",
            "414: from_pretrained_keras - add model path - https://github.com/huggingface/huggingface_hub/issues/1744\n",
            "415: Request: leaderboard for models in InferenceClient.list_deployed_models - https://github.com/huggingface/huggingface_hub/issues/1741\n",
            "416: Retry in hf_hub_download on ConnectionError and SSLError - https://github.com/huggingface/huggingface_hub/issues/1739\n",
            "417: Revampt workflow when downloading to local dir - https://github.com/huggingface/huggingface_hub/issues/1738\n",
            "418: Docs request: what is loaded/loadable? - https://github.com/huggingface/huggingface_hub/issues/1734\n",
            "419: HfFileSystem's transaction is working counterintuitively - https://github.com/huggingface/huggingface_hub/issues/1733\n",
            "420: Docs request: InferenceClient.post - https://github.com/huggingface/huggingface_hub/issues/1730\n",
            "421: Thoughts: streaming for conversational - https://github.com/huggingface/huggingface_hub/issues/1729\n",
            "422: Request: bumping inference extra to pydantic v2 - https://github.com/huggingface/huggingface_hub/issues/1726\n",
            "423: UnboundLocalError When Using Commit Context Manager - https://github.com/huggingface/huggingface_hub/issues/1721\n",
            "424: requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out. - https://github.com/huggingface/huggingface_hub/issues/1718\n",
            "425: [HFFS] Hanlde \"refs/convert/parquet\" and \"refs/pr/(\\d)+\" (PRs) revision correctly - https://github.com/huggingface/huggingface_hub/issues/1710\n",
            "426: Implement a FlaxModelHubMixin - https://github.com/huggingface/huggingface_hub/issues/1709\n",
            "427: load dataset from refs/convert/parquet instead of main - https://github.com/huggingface/huggingface_hub/issues/1707\n",
            "428: Git LFS fails to fetch files from git@hf.co with SSH auth - https://github.com/huggingface/huggingface_hub/issues/1706\n",
            "429: Return the liked users for for a huggingface model - https://github.com/huggingface/huggingface_hub/issues/1705\n",
            "430: hf_hub_download is writing to paths outside of $HF_HOME - https://github.com/huggingface/huggingface_hub/issues/1704\n",
            "431: Adding the ability to configure etag timeout as env property - https://github.com/huggingface/huggingface_hub/issues/1703\n",
            "432: [Community event] Translate documentation to your own langage - https://github.com/huggingface/huggingface_hub/issues/1700\n",
            "433: How to change cache dir? - https://github.com/huggingface/huggingface_hub/issues/1698\n",
            "434: snapshot_download does not support argument verify=False - https://github.com/huggingface/huggingface_hub/issues/1691\n",
            "435: check_disk_space fails when the target directory does not exist - https://github.com/huggingface/huggingface_hub/issues/1690\n",
            "436: RuntimeError: Error while uploading 'pytorch_model-00001-of-00003.bin' to the Hub. - https://github.com/huggingface/huggingface_hub/issues/1689\n",
            "437: Add Collection API (create, list, add to collection, delete?) - https://github.com/huggingface/huggingface_hub/issues/1682\n",
            "438: Deprecate ModelFilter/DatasetFilter and remove GeneralTags/ModelTags/DatasetTags - https://github.com/huggingface/huggingface_hub/issues/1676\n",
            "439: progress bar trims important part of filename: Downloading (…)of-00081.safetensors - https://github.com/huggingface/huggingface_hub/issues/1674\n",
            "440: [Bug] Filtering models by tags using ModelFilter doesn't work - https://github.com/huggingface/huggingface_hub/issues/1668\n",
            "441: SSLError on huggingface API - https://github.com/huggingface/huggingface_hub/issues/1665\n",
            "442: Harmonize return type on commit - https://github.com/huggingface/huggingface_hub/issues/1663\n",
            "443: Request: typing support (for mypy) - https://github.com/huggingface/huggingface_hub/issues/1662\n",
            "444: Fail to list repo commits - https://github.com/huggingface/huggingface_hub/issues/1661\n",
            "445: [CLI] Support --create-branch option in huggingface-cli upload - https://github.com/huggingface/huggingface_hub/issues/1657\n",
            "446: [CLI] Support repo management (create, list, delete) - https://github.com/huggingface/huggingface_hub/issues/1656\n",
            "447: [CLI] Support branch management (create, list, delete) - https://github.com/huggingface/huggingface_hub/issues/1655\n",
            "448: Templates: Annotated Policy Model Card template - https://github.com/huggingface/huggingface_hub/issues/1654\n",
            "449: Warn when 403 unauthorized error + token is read-only - https://github.com/huggingface/huggingface_hub/issues/1653\n",
            "450: Document more prominently the use of hf_transfer - https://github.com/huggingface/huggingface_hub/issues/1652\n",
            "451: InferenceClient: handle error response in when streaming tokens from TGI - https://github.com/huggingface/huggingface_hub/issues/1651\n",
            "452: Upload folder with multi_commits=True fails on large folder - https://github.com/huggingface/huggingface_hub/issues/1650\n",
            "453: InferenceClient.feature_extraction makes wrong request to the inference endpoint - https://github.com/huggingface/huggingface_hub/issues/1647\n",
            "454: 🌐 [i18n-DE] Translating docs to German - https://github.com/huggingface/huggingface_hub/issues/1645\n",
            "455: We couldn't connect to 'https://huggingface.co/' to load this model and it looks like distilbert-base-uncased is not the path to a directory conaining a config.json file. Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'. - https://github.com/huggingface/huggingface_hub/issues/1643\n",
            "456: Support for super-squash commit history. - https://github.com/huggingface/huggingface_hub/issues/1636\n",
            "457: ~/.huggingface/token is loaded with EOL - https://github.com/huggingface/huggingface_hub/issues/1634\n",
            "458: huggingface-cli login: ConnectionError 104 - https://github.com/huggingface/huggingface_hub/issues/1633\n",
            "459: Building takes an insanely long time - https://github.com/huggingface/huggingface_hub/issues/1632\n",
            "460: 🌐[i18n-KO] Translating docs to Korean - https://github.com/huggingface/huggingface_hub/issues/1626\n",
            "461: Failed to push data to a dataset repository - https://github.com/huggingface/huggingface_hub/issues/1623\n",
            "462: Outstanding issues with model evaluator - https://github.com/huggingface/huggingface_hub/issues/1619\n",
            "463: vague invalid credentials errror with api key that works locally but not on cloud server - https://github.com/huggingface/huggingface_hub/issues/1616\n",
            "464: Loading model fails - https://github.com/huggingface/huggingface_hub/issues/1610\n",
            "465: Wrong error to handle Paused or Scaled to Zero endpoints. - https://github.com/huggingface/huggingface_hub/issues/1605\n",
            "466: load_dataset (model) behind proxy - https://github.com/huggingface/huggingface_hub/issues/1604\n",
            "467: I want to fork this Hugging Face hub GitHub repository to do a translation. Is this feasible? - https://github.com/huggingface/huggingface_hub/issues/1601\n",
            "468: Can no longer connect to HuggingFaceHub. \"Distant resource does not seem to be on huggingface.co \" - https://github.com/huggingface/huggingface_hub/issues/1600\n",
            "469: Hi, we need java/scala hub api client - https://github.com/huggingface/huggingface_hub/issues/1599\n",
            "470: Impossible to download data with snapshot_download - https://github.com/huggingface/huggingface_hub/issues/1598\n",
            "471: Show number of parameters on model card - https://github.com/huggingface/huggingface_hub/issues/1596\n",
            "472: Enable/disable specific progress bars - https://github.com/huggingface/huggingface_hub/issues/1595\n",
            "473: Can't load model from hub - https://github.com/huggingface/huggingface_hub/issues/1593\n",
            "474: Hosted inference inactive on Llama 2 models - https://github.com/huggingface/huggingface_hub/issues/1589\n",
            "475: Trigger factory reboot from huggingface_hub - https://github.com/huggingface/huggingface_hub/issues/1585\n",
            "476: \"batch response: Too many password attempts\".can't push a lot of files. - https://github.com/huggingface/huggingface_hub/issues/1584\n",
            "477: RuntimeError: Error while uploading 'data/train-00040-of-00157-15109dabc9b3967a.parquet' to the Hub - https://github.com/huggingface/huggingface_hub/issues/1587\n",
            "478: API client for datasets-server - https://github.com/huggingface/huggingface_hub/issues/1583\n",
            "479: AttributeError: type object 'tqdm' has no attribute '_lock' - https://github.com/huggingface/huggingface_hub/issues/1603\n",
            "480: AsyncInferenceClient ignores extra parameters in text2image call - https://github.com/huggingface/huggingface_hub/issues/1581\n",
            "481: License information can't be accessed from gated repos - https://github.com/huggingface/huggingface_hub/issues/1579\n",
            "482: Upload only files that have changed since last upload in upload_folder - https://github.com/huggingface/huggingface_hub/issues/1576\n",
            "483: Commit not created when using HfApi.create_commit in ThreadPoolExecutor - https://github.com/huggingface/huggingface_hub/issues/1574\n",
            "484: Add hf_hub_download / snapshot_download as aliases in HfApi - https://github.com/huggingface/huggingface_hub/issues/1573\n",
            "485: Provide user manual download for \"hf_hub_download \" when \"hf_hub_download\" does not work - https://github.com/huggingface/huggingface_hub/issues/1572\n",
            "486: some requests options like proxies and verify does not pass to model_info - https://github.com/huggingface/huggingface_hub/issues/1570\n",
            "487: huggingface_hub.notebook_login fails on AWS Sagemaker Studio - https://github.com/huggingface/huggingface_hub/issues/1569\n",
            "488: Ability to add ReadMe tags from create_repo - https://github.com/huggingface/huggingface_hub/issues/1568\n",
            "489: Colab notebook_login goes to hf.co login page in Safari - https://github.com/huggingface/huggingface_hub/issues/1566\n",
            "490: scan-cache experiences extreme slowdowns with large models - https://github.com/huggingface/huggingface_hub/issues/1564\n",
            "491: huggingface-cli login not working - https://github.com/huggingface/huggingface_hub/issues/1563\n",
            "492: instead of hf hub api to get model ids I get: <generator object HfApi.list_models ..> - https://github.com/huggingface/huggingface_hub/issues/1562\n",
            "493: error: failed to push some refs to 'https://user..' - https://github.com/huggingface/huggingface_hub/issues/1560\n",
            "494: Add get_model_status to get a model status on InferenceAPI - https://github.com/huggingface/huggingface_hub/issues/1558\n",
            "495: Add list_deployed_models to list models deployed on InferenceAPI - https://github.com/huggingface/huggingface_hub/issues/1557\n",
            "496: Add support for secrets's description - https://github.com/huggingface/huggingface_hub/issues/1556\n",
            "497: Should low verbosity suppress progress bars? - https://github.com/huggingface/huggingface_hub/issues/1555\n",
            "498: Add endpoint to retrieve space logs - https://github.com/huggingface/huggingface_hub/issues/1554\n",
            "499: Add support for Space Persistent Storage - https://github.com/huggingface/huggingface_hub/issues/1553\n",
            "500: Add support for space variables - https://github.com/huggingface/huggingface_hub/issues/1552\n",
            "501: Check disk_usage before download - https://github.com/huggingface/huggingface_hub/issues/1551\n",
            "502: [hffs] Alias for the refs/convert/parquet branch - https://github.com/huggingface/huggingface_hub/issues/1550\n",
            "503: StableDiffusionImg2ImgPipeline OSError: Consistency check failed - https://github.com/huggingface/huggingface_hub/issues/1549\n",
            "504: ValueError on Windows when loading a repo card with empty YAML - https://github.com/huggingface/huggingface_hub/issues/1546\n",
            "505: CLI interface for uploading files - https://github.com/huggingface/huggingface_hub/issues/1543\n",
            "506: Allow snapshot_download to retry on requests.exceptions.ConnectionError - https://github.com/huggingface/huggingface_hub/issues/1542\n",
            "507: Implement API for Inference Endpoint - https://github.com/huggingface/huggingface_hub/issues/1541\n",
            "508: Implement remaining tasks in InferenceClient - https://github.com/huggingface/huggingface_hub/issues/1539\n",
            "509: Facebook models missing from hub - https://github.com/huggingface/huggingface_hub/issues/1537\n",
            "510: broken link to api-inference-community - https://github.com/huggingface/huggingface_hub/issues/1536\n",
            "511: 🚀 Manage access requests approval / rejection via API - https://github.com/huggingface/huggingface_hub/issues/1535\n",
            "512: Update Streamlit Spaces support to version 1.24.0 - https://github.com/huggingface/huggingface_hub/issues/1532\n",
            "513: model safetensors - https://github.com/huggingface/huggingface_hub/issues/1531\n",
            "514: Evaluation on the Hub does not see all public datasets - https://github.com/huggingface/huggingface_hub/issues/1527\n",
            "515: hf_hub_download doesn't retry on ReadTimeout - https://github.com/huggingface/huggingface_hub/issues/1526\n",
            "516: Support COEP/COOP headers on Spaces so that WebML spaces can use multiple threads - https://github.com/huggingface/huggingface_hub/issues/1525\n",
            "517: Inconsistency error occurred when downloading the model from the Hugging Face space - https://github.com/huggingface/huggingface_hub/issues/1521\n",
            "518: Request for Detailed Download Statistics in Hub API Endpoints - https://github.com/huggingface/huggingface_hub/issues/1520\n",
            "519: Bz2 library import error on macos, python3.7 - https://github.com/huggingface/huggingface_hub/issues/1519\n",
            "520: huggingface-cli login，ConnectionResetError: [WinError 10054] - https://github.com/huggingface/huggingface_hub/issues/1517\n",
            "521: Hundreds of experiments started throwing HfHubHTTPError: 504 Server Error: Gateway Time-out for url Error - https://github.com/huggingface/huggingface_hub/issues/1516\n",
            "522: RevisionNotFoundError when fs.ls() in \"refs/convert/parquet\" for big datasets - https://github.com/huggingface/huggingface_hub/issues/1515\n",
            "523: zsh: command not found: huggingface-cli - https://github.com/huggingface/huggingface_hub/issues/1512\n",
            "524: Can't copy files with CommitOperationCopy when there are too many of them - https://github.com/huggingface/huggingface_hub/issues/1503\n",
            "525: Error to send model for evaluation “was not found on the hub!” - https://github.com/huggingface/huggingface_hub/issues/1500\n",
            "526: Streamlit based spaces are not being loaded - https://github.com/huggingface/huggingface_hub/issues/1499\n",
            "527: OSError: Consistency check failed - https://github.com/huggingface/huggingface_hub/issues/1498\n",
            "528: Docker + Streamlit bug when using file upload - https://github.com/huggingface/huggingface_hub/issues/1496\n",
            "529: Some attributes in SpaceInfo returned by list_spaces are incorrect - https://github.com/huggingface/huggingface_hub/issues/1493\n",
            "530: GPT2Tokenizer from pretrained download issue - https://github.com/huggingface/huggingface_hub/issues/1490\n",
            "531: InferenceClient: next steps - https://github.com/huggingface/huggingface_hub/issues/1488\n",
            "532: Endpoint interface for inpainting models that require two images. - https://github.com/huggingface/huggingface_hub/issues/1486\n",
            "533: Add firstCreated entry to HfApi - https://github.com/huggingface/huggingface_hub/issues/1481\n",
            "534: Multiple attempts to download the model resulted in errors - https://github.com/huggingface/huggingface_hub/issues/1480\n",
            "535: space_hardware is not respected in huggingface_hub.create_repo - https://github.com/huggingface/huggingface_hub/issues/1477\n",
            "536: Timeout when running hugging face LLMs with Langchain (Streamlit) - https://github.com/huggingface/huggingface_hub/issues/1473\n",
            "537: Pushing a 1.5TB dataset keeps throwing RPC error - https://github.com/huggingface/huggingface_hub/issues/1472\n",
            "538: Can not run huggingface-cli login - https://github.com/huggingface/huggingface_hub/issues/1471\n",
            "539: Download files by SHA256 - https://github.com/huggingface/huggingface_hub/issues/1470\n",
            "540: 404 Client Error using snapshot_download - https://github.com/huggingface/huggingface_hub/issues/1466\n",
            "541: Discussion.url is being wrong - https://github.com/huggingface/huggingface_hub/issues/1463\n",
            "542: HTTP error 404 during installation - https://github.com/huggingface/huggingface_hub/issues/1459\n",
            "543: Handle hardware, storage, secrets and variables when creating or duplicating repos - https://github.com/huggingface/huggingface_hub/issues/1457\n",
            "544: InferenceAPI: add configurable timeout - https://github.com/huggingface/huggingface_hub/issues/1455\n",
            "545: InferenceAPI: sleep and loop if wait_for_model=True - https://github.com/huggingface/huggingface_hub/issues/1454\n",
            "546: HfApi.dataset_info is case insensitive - https://github.com/huggingface/huggingface_hub/issues/1453\n",
            "547: [HfFileSystem] Reuse caching when downloading a file - https://github.com/huggingface/huggingface_hub/issues/1452\n",
            "548: Document metadate_update in model cards guide - https://github.com/huggingface/huggingface_hub/issues/1448\n",
            "549: Add expand to /tree endpoint - https://github.com/huggingface/huggingface_hub/issues/1447\n",
            "550: Model Compression - https://github.com/huggingface/huggingface_hub/issues/1446\n",
            "551: Downloaded File Contents from Repository Do Not Match SHA-256 and File Size - https://github.com/huggingface/huggingface_hub/issues/1445\n",
            "552: \"import transformers\"出现问题 - https://github.com/huggingface/huggingface_hub/issues/1434\n",
            "553: File name too long when downloading models - https://github.com/huggingface/huggingface_hub/issues/1426\n",
            "554: Encoding - https://github.com/huggingface/huggingface_hub/issues/1425\n",
            "555: Support image-to-image task in InferenceAPI - https://github.com/huggingface/huggingface_hub/issues/1424\n",
            "556: Unable to acquire file lock in some situations - https://github.com/huggingface/huggingface_hub/issues/1423\n",
            "557: Thread safe when upload files in multiple threads or processes - https://github.com/huggingface/huggingface_hub/issues/1422\n",
            "558: Inconsistency between list_models and list_spaces - https://github.com/huggingface/huggingface_hub/issues/1419\n",
            "559: OSError: Distant resource does not have an ETag, we won't be able to reliably ensure reproducibility while importing RobertaTokenizer.from_pretrained(model_name) - https://github.com/huggingface/huggingface_hub/issues/1417\n",
            "560: Log at warning level when waiting for commands to finish - https://github.com/huggingface/huggingface_hub/issues/1412\n",
            "561: Do not create empty commit - https://github.com/huggingface/huggingface_hub/issues/1411\n",
            "562: Huggingface space python version - https://github.com/huggingface/huggingface_hub/issues/1409\n",
            "563: keeps saying \"remote: Your push was rejected because it contains binary files.\" after I deleted that file - https://github.com/huggingface/huggingface_hub/issues/1404\n",
            "564: Ignore .git folder in upload_folder - https://github.com/huggingface/huggingface_hub/issues/1402\n",
            "565: Internal Server Error when pushing local folder to repo branch - https://github.com/huggingface/huggingface_hub/issues/1401\n",
            "566: Error using cache_dir in from_pretrained since update - https://github.com/huggingface/huggingface_hub/issues/1398\n",
            "567: Repository wrapper seems to be slower than git clone - https://github.com/huggingface/huggingface_hub/issues/1397\n",
            "568: resumable download and file integration check - https://github.com/huggingface/huggingface_hub/issues/1396\n",
            "569: git+lfs push_to_hub get's stuck forever - https://github.com/huggingface/huggingface_hub/issues/1393\n",
            "570: Harmonize attributes returned by model_info, dataset_info and space_info - https://github.com/huggingface/huggingface_hub/issues/1389\n",
            "571: FileNotFoundError when reading file symlink from cache dir. - https://github.com/huggingface/huggingface_hub/issues/1388\n",
            "572: Integrate 'autotrain-advanced' as module inside huggingface_hub - https://github.com/huggingface/huggingface_hub/issues/1387\n",
            "573: failed to download model snapshot in tmp_folder on Windows - https://github.com/huggingface/huggingface_hub/issues/1385\n",
            "574: notebook_login() does not update UI on Databricks - https://github.com/huggingface/huggingface_hub/issues/1384\n",
            "575: Add force_download parameter in snapshot_download - https://github.com/huggingface/huggingface_hub/issues/1383\n",
            "576: When uploading an image to a HF Dataset, don't clone locally - https://github.com/huggingface/huggingface_hub/issues/1379\n",
            "577: Support for Temporal Point Processes / continuous-time sequence models - https://github.com/huggingface/huggingface_hub/issues/1378\n",
            "578: Cannot load private models when being logged in - https://github.com/huggingface/huggingface_hub/issues/1374\n",
            "579: Uploading an image to a HF Dataset sometimes crashes Gradio - https://github.com/huggingface/huggingface_hub/issues/1373\n",
            "580: Wrong case of repo_id when using upload_folder or upload_file causes unpredictable error displayed - https://github.com/huggingface/huggingface_hub/issues/1371\n",
            "581: [RFC] Consider switching to httpx instead of requests - https://github.com/huggingface/huggingface_hub/issues/1368\n",
            "582: Updating Markdown '#' in MC template - https://github.com/huggingface/huggingface_hub/issues/1366\n",
            "583: Modelcard pretty HTML - https://github.com/huggingface/huggingface_hub/issues/1365\n",
            "584: Private token does not work - https://github.com/huggingface/huggingface_hub/issues/1363\n",
            "585: Make ModelCard's more robust to malformed model-index fields - https://github.com/huggingface/huggingface_hub/issues/1361\n",
            "586: upload_folder (more broadly create_commit) should not reupload LFS files that are already uploaded - https://github.com/huggingface/huggingface_hub/issues/1355\n",
            "587: [RFC] High-level and opinionated huggingface-cli push command - https://github.com/huggingface/huggingface_hub/issues/1352\n",
            "588: bart_lfqa: content has to be taken into account and a message is to be uttered if no reply is being found instead of going freestyle. - https://github.com/huggingface/huggingface_hub/issues/1350\n",
            "589: Add duplicate_space endpoint - https://github.com/huggingface/huggingface_hub/issues/1349\n",
            "590: list_datasets() different result from HF Datasets - https://github.com/huggingface/huggingface_hub/issues/1351\n",
            "591: Add a SpaceCard class or make RepoCard as top-level + few tweak - https://github.com/huggingface/huggingface_hub/issues/1348\n",
            "592: Incorrect validation error - https://github.com/huggingface/huggingface_hub/issues/1343\n",
            "593: Smarter cache for snapshot_download - https://github.com/huggingface/huggingface_hub/issues/1342\n",
            "594: Updating MC template - https://github.com/huggingface/huggingface_hub/issues/1340\n",
            "595: RepoUrl should be able to parse canonical datasets - https://github.com/huggingface/huggingface_hub/issues/1336\n",
            "596: Promote more HTTP method instead of Repository in guides - https://github.com/huggingface/huggingface_hub/issues/1334\n",
            "597: Add a guide of Pytorch Mixins - https://github.com/huggingface/huggingface_hub/issues/1333\n",
            "598: Programmatically pause a Space - https://github.com/huggingface/huggingface_hub/issues/1330\n",
            "599: Investigate notebook_login on Kaggle notebooks - https://github.com/huggingface/huggingface_hub/issues/1327\n",
            "600: Contrib CI for timm is broken - https://github.com/huggingface/huggingface_hub/issues/1324\n",
            "601: Public / private toggle on models is broken - https://github.com/huggingface/huggingface_hub/issues/1322\n",
            "602: missing 1 required positional argument: 'response' in huggingface_hub.utils.EntryNotFoundError - https://github.com/huggingface/huggingface_hub/issues/1319\n",
            "603: Unexpected characters in progress bar when downloading with hf_hub_download - https://github.com/huggingface/huggingface_hub/issues/1318\n",
            "604: Unable to do upload_folder - https://github.com/huggingface/huggingface_hub/issues/1314\n",
            "605: TypeError: _from_pretrained() got multiple values for argument 'revision' when using PyTorchModelHubMixin - https://github.com/huggingface/huggingface_hub/issues/1313\n",
            "606: 403 Client Error when pushing dataset to hub - https://github.com/huggingface/huggingface_hub/issues/1312\n",
            "607: Error when uploading any lfs file to model repo - https://github.com/huggingface/huggingface_hub/issues/1311\n",
            "608: Caching error when using from_pretrained - https://github.com/huggingface/huggingface_hub/issues/1305\n",
            "609: Feature request: Allow upload_file to upload multiple files - https://github.com/huggingface/huggingface_hub/issues/1301\n",
            "610: HfApi.upload_file consumes huge amount of memory when hashing - https://github.com/huggingface/huggingface_hub/issues/1294\n",
            "611: huggingface-cli login should indicate if I am already logged in - https://github.com/huggingface/huggingface_hub/issues/1290\n",
            "612: Is it possible to drop everything inside the main branch before creating a new branch? - https://github.com/huggingface/huggingface_hub/issues/1288\n",
            "613: Import error because no actual libraries, as far as I can tell. - https://github.com/huggingface/huggingface_hub/issues/1287\n",
            "614: Repository does not work on HF spaces - https://github.com/huggingface/huggingface_hub/issues/1284\n",
            "615: Unable to install huggingface-cli - https://github.com/huggingface/huggingface_hub/issues/1283\n",
            "616: snapshot_download does not show the name of the files downloaded in parallel - https://github.com/huggingface/huggingface_hub/issues/1281\n",
            "617: KeyError: 'multilinguality' when calling DatasetSearchArguments() - https://github.com/huggingface/huggingface_hub/issues/1280\n",
            "618: [Windows] Repository push LFS fails at cleanup - https://github.com/huggingface/huggingface_hub/issues/1278\n",
            "619: hf_hub_download call does not increase the download counter - https://github.com/huggingface/huggingface_hub/issues/1275\n",
            "620: Internal Server Error with Hub - https://github.com/huggingface/huggingface_hub/issues/1274\n",
            "621: Dealing with requests 2.28+ and SSL - https://github.com/huggingface/huggingface_hub/issues/1271\n",
            "622: Dry-run option in upload_folder - https://github.com/huggingface/huggingface_hub/issues/1262\n",
            "623: List available branches for a given model/dataset - https://github.com/huggingface/huggingface_hub/issues/1260\n",
            "624: models downloaded by diffusers 0.10 do not appear in scan-cache - https://github.com/huggingface/huggingface_hub/issues/1259\n",
            "625: Endpoint to dynamically like a repo - https://github.com/huggingface/huggingface_hub/issues/1255\n",
            "626: get_space_runtime should use space_info internally - https://github.com/huggingface/huggingface_hub/issues/1252\n",
            "627: Allow path and path_or_fileobj to be pathlib.Path - https://github.com/huggingface/huggingface_hub/issues/1246\n",
            "628: http-based upload function should have a progress bar and show upload speed - https://github.com/huggingface/huggingface_hub/issues/1244\n",
            "629: space can't reach getElementById inside the iframe - https://github.com/huggingface/huggingface_hub/issues/1242\n",
            "630: Opt-in for downloading without symlinks - https://github.com/huggingface/huggingface_hub/issues/1240\n",
            "631: push_to_hub_keras error - https://github.com/huggingface/huggingface_hub/issues/1239\n",
            "632: Add Secrets via API on hf_api - https://github.com/huggingface/huggingface_hub/issues/1234\n",
            "633: Store user token in huggingface home folder - https://github.com/huggingface/huggingface_hub/issues/1232\n",
            "634: Ability to version with training script to model repository - https://github.com/huggingface/huggingface_hub/issues/1231\n",
            "635: ConnectionError when calling list_repo_files or hf_hub_download - https://github.com/huggingface/huggingface_hub/issues/1230\n",
            "636: Streamline auth tokens used for file downloads - https://github.com/huggingface/huggingface_hub/issues/1229\n",
            "637: Convert existing models with cache duplication to symlinks - https://github.com/huggingface/huggingface_hub/issues/1222\n",
            "638: Potential harmonization possible. - https://github.com/huggingface/huggingface_hub/issues/1217\n",
            "639: hf_hub_download always writes to disk when revision is not a complete commit hash - https://github.com/huggingface/huggingface_hub/issues/1216\n",
            "640: from_pretrained download doesn't respect umask - https://github.com/huggingface/huggingface_hub/issues/1215\n",
            "641: Easy check repository and branch - https://github.com/huggingface/huggingface_hub/issues/1213\n",
            "642: Getting the pickle analysis data from within Python - https://github.com/huggingface/huggingface_hub/issues/1212\n",
            "643: 500 Server Error when calling ModelSearchArguments() - https://github.com/huggingface/huggingface_hub/issues/1211\n",
            "644: metadata_update() duplicates metrics when updating a single field from None - https://github.com/huggingface/huggingface_hub/issues/1210\n",
            "645: AttributeError: 'NoneType' object has no attribute 'split' - https://github.com/huggingface/huggingface_hub/issues/1209\n",
            "646: Dataset metadata lost when loading evaluations with ModelCard.load() - https://github.com/huggingface/huggingface_hub/issues/1208\n",
            "647: [advice] use github to replace gitlab in pre-commit-config - https://github.com/huggingface/huggingface_hub/issues/1206\n",
            "648: Improve error message when push to hub fails because it is not a git repo - https://github.com/huggingface/huggingface_hub/issues/1204\n",
            "649: No module found huggingface_hub.snapshot_download in 0.11.0 - https://github.com/huggingface/huggingface_hub/issues/1199\n",
            "650: Programmatically accept gated repo - https://github.com/huggingface/huggingface_hub/issues/1198\n",
            "651: Shell completions - https://github.com/huggingface/huggingface_hub/issues/1197\n",
            "652: Expose a modified requests module to wrap build_hf_headers and hf_raise_for_status - https://github.com/huggingface/huggingface_hub/issues/1194\n",
            "653: Create a contrib/ folder with example scripts from downstream libraries - https://github.com/huggingface/huggingface_hub/issues/1190\n",
            "654: metadata_update() function inserts verified=True in self-reported evaluations - https://github.com/huggingface/huggingface_hub/issues/1185\n",
            "655: Creating PR via create_commit on a repo for which we don't have permissions seems broken - https://github.com/huggingface/huggingface_hub/issues/1182\n",
            "656: Add a helper to print setup/machine details - https://github.com/huggingface/huggingface_hub/issues/1173\n",
            "657: Import constants module instead of importing constant values directly - https://github.com/huggingface/huggingface_hub/issues/1172\n",
            "658: Add create_branch and delete_branch http-endpoints - https://github.com/huggingface/huggingface_hub/issues/1165\n",
            "659: Send delete operations before add operations in create_commit - https://github.com/huggingface/huggingface_hub/issues/1162\n",
            "660: Filename must be url encoded in hf_hub_url (and maybe elsewhere) - https://github.com/huggingface/huggingface_hub/issues/1161\n",
            "661: [notebook_login] Error: Module @jupyter-widgets/controls, version ^1.5.0 is not registered, however, 2.0.0 is - https://github.com/huggingface/huggingface_hub/issues/1160\n",
            "662: Ability to get file size for RepoFile - https://github.com/huggingface/huggingface_hub/issues/1158\n",
            "663: Allow create PRs against non-main branch from create_commit method - https://github.com/huggingface/huggingface_hub/issues/1156\n",
            "664: Some RepoCard tests incorrectly format content string, causing CardData to be ignored - https://github.com/huggingface/huggingface_hub/issues/1154\n",
            "665: Harmonize parameters in list_models, list_datasets and list_spaces - https://github.com/huggingface/huggingface_hub/issues/1153\n",
            "666: Support delete folder on the commit endpoint - https://github.com/huggingface/huggingface_hub/issues/1151\n",
            "667: At least some boolean arguments in query functions are not working correctly - https://github.com/huggingface/huggingface_hub/issues/1146\n",
            "668: Make it easier to modify the model card content - https://github.com/huggingface/huggingface_hub/issues/1144\n",
            "669: Permission on local cached files should be configurable - https://github.com/huggingface/huggingface_hub/issues/1141\n",
            "670: Add support for renaming files in commit endpoint - https://github.com/huggingface/huggingface_hub/issues/1140\n",
            "671: HfApi method to fetch \"liked\" models by current user - https://github.com/huggingface/huggingface_hub/issues/1137\n",
            "672: hf_hub_download fails with FileExistsError when run in parallel for the same file - https://github.com/huggingface/huggingface_hub/issues/1133\n",
            "673: language vs languages in Model and Dataset filters - https://github.com/huggingface/huggingface_hub/issues/1130\n",
            "674: Add scvi-tools as a Hugging Face library - https://github.com/huggingface/huggingface_hub/issues/1129\n",
            "675: Support incremental uploads - https://github.com/huggingface/huggingface_hub/issues/1430\n",
            "676: Hello! - https://github.com/huggingface/huggingface_hub/issues/1127\n",
            "677: Model card specific features of Keras mixin don't work with JAX model converted to TF - https://github.com/huggingface/huggingface_hub/issues/1126\n",
            "678: Better error message on 401 - https://github.com/huggingface/huggingface_hub/issues/1124\n",
            "679: Add support for async API calls - https://github.com/huggingface/huggingface_hub/issues/1123\n",
            "680: Documentation: document that we can download from a separate branch using revision=... - https://github.com/huggingface/huggingface_hub/issues/1120\n",
            "681: error 403 Client Error: Forbidden for url when downloading a model - https://github.com/huggingface/huggingface_hub/issues/1119\n",
            "682: Improve Error message on BadRequest - https://github.com/huggingface/huggingface_hub/issues/1114\n",
            "683: Overridable download progress display - https://github.com/huggingface/huggingface_hub/issues/1110\n",
            "684: Flaky error \"File already exist\" during symlink creation - https://github.com/huggingface/huggingface_hub/issues/1108\n",
            "685: CLI interface for downloading files - https://github.com/huggingface/huggingface_hub/issues/1105\n",
            "686: Add a token attribute to HfApi - https://github.com/huggingface/huggingface_hub/issues/1104\n",
            "687: Network connection error prevent use of a cached download - https://github.com/huggingface/huggingface_hub/issues/1099\n",
            "688: Add a delete_tag method - https://github.com/huggingface/huggingface_hub/issues/1095\n",
            "689: Remove use_auth_token argument in favor of token everywhere - https://github.com/huggingface/huggingface_hub/issues/1094\n",
            "690: Delete remaining arguments/methods that have been deprecated and should be removed in v0.9/0.10/0.11 - https://github.com/huggingface/huggingface_hub/issues/1090\n",
            "691: Warn users about future pagination on list_models / list_datasets / list_spaces - https://github.com/huggingface/huggingface_hub/issues/1087\n",
            "692: Enable upload_folder to upload content in chunks - https://github.com/huggingface/huggingface_hub/issues/1085\n",
            "693: Add copy operation to commit API - https://github.com/huggingface/huggingface_hub/issues/1083\n",
            "694: Add endpoint param to hf_hub_url - https://github.com/huggingface/huggingface_hub/issues/1082\n",
            "695: ModelCard loading fails with unexpected error - https://github.com/huggingface/huggingface_hub/issues/1080\n",
            "696: after install huggingface_hub with pip, the huggingface_cli command not found - https://github.com/huggingface/huggingface_hub/issues/1079\n",
            "697: upload_file raises internal server error (500) - https://github.com/huggingface/huggingface_hub/issues/1078\n",
            "698: Run CI on a Windows machine as well - https://github.com/huggingface/huggingface_hub/issues/1074\n",
            "699: notebook_login errors out after passing token in main branch - https://github.com/huggingface/huggingface_hub/issues/1072\n",
            "700: Allow to retrieve discussions/PRs using list_models - https://github.com/huggingface/huggingface_hub/issues/1071\n",
            "701: Add options to the \"delete-cache\" command - https://github.com/huggingface/huggingface_hub/issues/1065\n",
            "702: Developer mode requirement on Windows - https://github.com/huggingface/huggingface_hub/issues/1062\n",
            "703: Get rid of as much subprocess as possible, and get rid of passing args via CLI where possible - https://github.com/huggingface/huggingface_hub/issues/1056\n",
            "704: Deprecate private and repo_type in Repository class - https://github.com/huggingface/huggingface_hub/issues/1055\n",
            "705: Git: find a \"better\" way to handle tokens than git credential store - https://github.com/huggingface/huggingface_hub/issues/1051\n",
            "706: Bug: scan cache fails if a cached snapshot is empty - https://github.com/huggingface/huggingface_hub/issues/1050\n",
            "707: Add \"size\" parameter for lfsFiles for commit API - https://github.com/huggingface/huggingface_hub/issues/1047\n",
            "708: Automatically create HF PRs - https://github.com/huggingface/huggingface_hub/issues/1044\n",
            "709: notebook_login showing unavoidable warning on Google Colab - https://github.com/huggingface/huggingface_hub/issues/1043\n",
            "710: Concurrent downloads when using snapshot_download - https://github.com/huggingface/huggingface_hub/issues/1042\n",
            "711: Update create_commit output type to return something even when no PR is created - https://github.com/huggingface/huggingface_hub/issues/1037\n",
            "712: Handle autocomplete in IDE - https://github.com/huggingface/huggingface_hub/issues/1036\n",
            "713: 🚩 Handle .no_exist files in try_to_load_from_cache - https://github.com/huggingface/huggingface_hub/issues/1033\n",
            "714: Add template kwargs for dataset card - https://github.com/huggingface/huggingface_hub/issues/1030\n",
            "715: User-facing APIs for _request_for_retry and _normalize_etag - https://github.com/huggingface/huggingface_hub/issues/1028\n",
            "716: 503 Error when pushing repeatedly - https://github.com/huggingface/huggingface_hub/issues/1027\n",
            "717: 🚩 Scan cache tool: ability to free up space - https://github.com/huggingface/huggingface_hub/issues/1025\n",
            "718: 🚩 Scan-cache tool: select columns, sort by size, csv-style output - https://github.com/huggingface/huggingface_hub/issues/1024\n",
            "719: Allow dry_run for snapshot_download - https://github.com/huggingface/huggingface_hub/issues/1023\n",
            "720: Connection Error under Proxy - https://github.com/huggingface/huggingface_hub/issues/1020\n",
            "721: Add User-Agent header to all requests to the hub - https://github.com/huggingface/huggingface_hub/issues/1018\n",
            "722: Activity view shows likes for private datasets - https://github.com/huggingface/huggingface_hub/issues/1017\n",
            "723: Replace empty subfolder with None in hf_hub_download - https://github.com/huggingface/huggingface_hub/issues/1016\n",
            "724: Add a git tag to a repository - https://github.com/huggingface/huggingface_hub/issues/1014\n",
            "725: Automatic pruning of previously downloaded files superceded by updated files - https://github.com/huggingface/huggingface_hub/issues/1013\n",
            "726: Add descriptive error message on use_auth_token for gated models - https://github.com/huggingface/huggingface_hub/issues/1012\n",
            "727: Documentation about \".exist\" folder in the cache - https://github.com/huggingface/huggingface_hub/issues/1011\n",
            "728: metadata_update fails when README.md has no text content (but it has metadata) - https://github.com/huggingface/huggingface_hub/issues/1010\n",
            "729: metadata_update fails when no README.md exists - https://github.com/huggingface/huggingface_hub/issues/1009\n",
            "730: Should throw an error if repo_id is not valid - https://github.com/huggingface/huggingface_hub/issues/1008\n",
            "731: Not specifiying \"repo_type\" throws RepositoryNotFound error. - https://github.com/huggingface/huggingface_hub/issues/1007\n",
            "732: Add Progress bar to snapshot_download to see the progress for the whole \"download\" - https://github.com/huggingface/huggingface_hub/issues/1004\n",
            "733: AttributeError: 'ModelInfo' object has no attribute 'securityStatus' - https://github.com/huggingface/huggingface_hub/issues/1002\n",
            "734: [Spaces] Can't install from private git - https://github.com/huggingface/huggingface_hub/issues/997\n",
            "735: HTTP-based push_to_hub for fastai integration - https://github.com/huggingface/huggingface_hub/issues/996\n",
            "736: Harmonize warnings/errors/documentation related to file size limit - https://github.com/huggingface/huggingface_hub/issues/995\n",
            "737: Better error messages when pushing/pulling to/from the hub - https://github.com/huggingface/huggingface_hub/issues/989\n",
            "738: Improve create_commit, upload_file, upload_folder and delete_file docstrings with examples - https://github.com/huggingface/huggingface_hub/issues/984\n",
            "739: Ignore file patterns when uploading a folder - https://github.com/huggingface/huggingface_hub/issues/982\n",
            "740: Handle redirections in hf_hub_download for a renamed repo - https://github.com/huggingface/huggingface_hub/issues/981\n",
            "741: Use a finer exception when local_files_only=True and a file is missing in cache - https://github.com/huggingface/huggingface_hub/issues/979\n",
            "742: Add ability to turn on and off progress bars - https://github.com/huggingface/huggingface_hub/issues/978\n",
            "743: Enable code coverage properly in Github Actions - https://github.com/huggingface/huggingface_hub/issues/977\n",
            "744: Installing from main does not work - https://github.com/huggingface/huggingface_hub/issues/973\n",
            "745: Add a utility to list cached things - https://github.com/huggingface/huggingface_hub/issues/972\n",
            "746: Make path_in_repo optional in upload_folder - https://github.com/huggingface/huggingface_hub/issues/971\n",
            "747: Speed-up tests using pytest-xdist - https://github.com/huggingface/huggingface_hub/issues/970\n",
            "748: Exception not caught in tests (keras custom layers) - https://github.com/huggingface/huggingface_hub/issues/969\n",
            "749: ArchLinux cant push models ! atleast on keras - https://github.com/huggingface/huggingface_hub/issues/968\n",
            "750: Add flake8-bugbear to the code quality checks - https://github.com/huggingface/huggingface_hub/issues/966\n",
            "751: Add git tags visually in the Files and versions tab - https://github.com/huggingface/huggingface_hub/issues/960\n",
            "752: Support any git method - https://github.com/huggingface/huggingface_hub/issues/955\n",
            "753: Support git push --force - https://github.com/huggingface/huggingface_hub/issues/954\n",
            "754: Support git commit --amend - https://github.com/huggingface/huggingface_hub/issues/953\n",
            "755: Uploading an empty file with create_commit fails if the file is empty - https://github.com/huggingface/huggingface_hub/issues/946\n",
            "756: Path for saving authentication token should be configureable. - https://github.com/huggingface/huggingface_hub/issues/942\n",
            "757: Support non-blocking push with non-git methods - https://github.com/huggingface/huggingface_hub/issues/939\n",
            "758: High level feedback on docs - https://github.com/huggingface/huggingface_hub/issues/938\n",
            "759: error: failed to push some refs to 'https://huggingface.co/username/modelname' - https://github.com/huggingface/huggingface_hub/issues/937\n",
            "760: Replace constrained strings (model/task names...) with enums. - https://github.com/huggingface/huggingface_hub/issues/935\n",
            "761: Silero Models License Infringement - https://github.com/huggingface/huggingface_hub/issues/933\n",
            "762: API Implementation Error: (intermediate value).filter is not a function - https://github.com/huggingface/huggingface_hub/issues/932\n",
            "763: hf_hub_download and cached_download should read the token by default - https://github.com/huggingface/huggingface_hub/issues/926\n",
            "764: Missing images in the Documentation - https://github.com/huggingface/huggingface_hub/issues/929\n",
            "765: Repository class should create a repository if there isn't in the local directory - https://github.com/huggingface/huggingface_hub/issues/923\n",
            "766: 413 Client Error: Payload Too Large when using upload_folder on a lot of files - https://github.com/huggingface/huggingface_hub/issues/918\n",
            "767: hf_hub_url not giving appropriate URL - https://github.com/huggingface/huggingface_hub/issues/915\n",
            "768: Hyperparameters should be parsed recursively for Keras mixin - https://github.com/huggingface/huggingface_hub/issues/911\n",
            "769: hub-ci returning error 500 when trying to create a Space - https://github.com/huggingface/huggingface_hub/issues/907\n",
            "770: Hub API calls: print X-Request-Id value in case of server error - https://github.com/huggingface/huggingface_hub/issues/906\n",
            "771: Repository.clone_from shouldn't create a repo on the hub - https://github.com/huggingface/huggingface_hub/issues/905\n",
            "772: Release v0.8 - https://github.com/huggingface/huggingface_hub/issues/899\n",
            "773: create_repo does not work unless you set private argument - https://github.com/huggingface/huggingface_hub/issues/897\n",
            "774: missing \"view the api\" on new spaces - https://github.com/huggingface/huggingface_hub/issues/896\n",
            "775: Difference between load_from_disk and load_dataset (help wanted) - https://github.com/huggingface/huggingface_hub/issues/894\n",
            "776: Create ability to push tf.Module inside Keras Mixin - https://github.com/huggingface/huggingface_hub/issues/887\n",
            "777: Add list_spaces - https://github.com/huggingface/huggingface_hub/issues/881\n",
            "778: RFC rename snapshot_download.py - https://github.com/huggingface/huggingface_hub/issues/875\n",
            "779: wrong link for the paper of facebook/blenderbot-400M-distill - https://github.com/huggingface/huggingface_hub/issues/873\n",
            "780: Remove history from Keras model card - https://github.com/huggingface/huggingface_hub/issues/869\n",
            "781: Unable to download a particular public model due to Forbidden connection error - https://github.com/huggingface/huggingface_hub/issues/865\n",
            "782: Allow boolean values for force_filename in hf_hub_download - https://github.com/huggingface/huggingface_hub/issues/863\n",
            "783: Private GitLab - https://github.com/huggingface/huggingface_hub/issues/862\n",
            "784: Add filename as args to fastai method from_pretrained_fastai - https://github.com/huggingface/huggingface_hub/issues/859\n",
            "785: Consider implementing Rich Repr Protocol for classes in hf_api - https://github.com/huggingface/huggingface_hub/issues/857\n",
            "786: Use tempfile consistently across all tests - https://github.com/huggingface/huggingface_hub/issues/853\n",
            "787: CLI option to manage model and dataset visibility - https://github.com/huggingface/huggingface_hub/issues/851\n",
            "788: Changes suggested to fastai integration - https://github.com/huggingface/huggingface_hub/issues/848\n",
            "789: Deprecation of pushing arguments for cleaner DX in Keras mixin - https://github.com/huggingface/huggingface_hub/issues/846\n",
            "790: Guide HF transformer users to use the corresponding hub functions - https://github.com/huggingface/huggingface_hub/issues/845\n",
            "791: History and Config in Keras repositories - https://github.com/huggingface/huggingface_hub/issues/843\n",
            "792: [Feature Request] Add \"git clone XXXXX\" snippet for Spaces - https://github.com/huggingface/huggingface_hub/issues/842\n",
            "793: Add a quick start guide - https://github.com/huggingface/huggingface_hub/issues/841\n",
            "794: Uniformize token retrieval/validation - https://github.com/huggingface/huggingface_hub/issues/837\n",
            "795: Rename allow_regex and ignore_regex in snapshot_download to be consistent with fnmatch - https://github.com/huggingface/huggingface_hub/issues/836\n",
            "796: Expose a function to update the meta data in the readme - https://github.com/huggingface/huggingface_hub/issues/835\n",
            "797: Internal test for security fails - https://github.com/huggingface/huggingface_hub/issues/831\n",
            "798: PyTorchModelHubMixin doesn't work when pushing to the hub several times - https://github.com/huggingface/huggingface_hub/issues/829\n",
            "799: Removal of /api/{login,logout} methods - https://github.com/huggingface/huggingface_hub/issues/827\n",
            "800: /api/repos/ls has been removed - https://github.com/huggingface/huggingface_hub/issues/826\n",
            "801: push_to_hub_keras rejected because it contains binary files - https://github.com/huggingface/huggingface_hub/issues/825\n",
            "802: RFC Creating a coherent user and developer experience regarding integrations - https://github.com/huggingface/huggingface_hub/issues/824\n",
            "803: HfApi.create_repo fails in 0.5 if the org name is passed in repo_id and organization arguments - https://github.com/huggingface/huggingface_hub/issues/821\n",
            "804: Docs review - https://github.com/huggingface/huggingface_hub/issues/818\n",
            "805: Relax version constraint on requests - https://github.com/huggingface/huggingface_hub/issues/816\n",
            "806: CI replace doc build comment with a GH action - https://github.com/huggingface/huggingface_hub/issues/814\n",
            "807: [API Reference docs] Lack of images - https://github.com/huggingface/huggingface_hub/issues/812\n",
            "808: [API Reference docs] The build_documentation Action does not run successfully - https://github.com/huggingface/huggingface_hub/issues/811\n",
            "809: HfApi.endpoint should be configurable in Repository - https://github.com/huggingface/huggingface_hub/issues/808\n",
            "810: push_to_hub_keras API dose not support sequential GANs models. - https://github.com/huggingface/huggingface_hub/issues/805\n",
            "811: Allow specifying multiple tags with the Keras mixin - https://github.com/huggingface/huggingface_hub/issues/800\n",
            "812: Is it possible to make HF_ENDPOINT endpoint overridable in environment variable ? - https://github.com/huggingface/huggingface_hub/issues/798\n",
            "813: PyYAML parses no as False - https://github.com/huggingface/huggingface_hub/issues/797\n",
            "814: RFC tokens should be persisted only with explicit request of the user - https://github.com/huggingface/huggingface_hub/issues/796\n",
            "815: Reviewer assing task fails on PRs from forks - https://github.com/huggingface/huggingface_hub/issues/791\n",
            "816: Support python=3.10 - https://github.com/huggingface/huggingface_hub/issues/786\n",
            "817: CI fail on two tests - https://github.com/huggingface/huggingface_hub/issues/785\n",
            "818: remote: tput: No value for $TERM and no -T specified - https://github.com/huggingface/huggingface_hub/issues/784\n",
            "819: Document integration requirements - https://github.com/huggingface/huggingface_hub/issues/777\n",
            "820: /api/repos/move now completes in a synchronous way - https://github.com/huggingface/huggingface_hub/issues/775\n",
            "821: How to correctly upload 160 RL models? - https://github.com/huggingface/huggingface_hub/issues/769\n",
            "822: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/762\n",
            "823: API reference using doc-builder - https://github.com/huggingface/huggingface_hub/issues/759\n",
            "824: Token cannot be pasted in JupyterNotebook in VScode - https://github.com/huggingface/huggingface_hub/issues/752\n",
            "825: For HTTP request errors, add response to exception message - https://github.com/huggingface/huggingface_hub/issues/751\n",
            "826: Tests fail when run in parallel - https://github.com/huggingface/huggingface_hub/issues/746\n",
            "827: RFC Split the repo based on the projects in the repo - https://github.com/huggingface/huggingface_hub/issues/744\n",
            "828: @retry_endpoint doesn't worth with pytest.mark.parametrize - https://github.com/huggingface/huggingface_hub/issues/740\n",
            "829: Allow snapshot_download to download dataset and space repos as well - https://github.com/huggingface/huggingface_hub/issues/739\n",
            "830: API to download tar.gz of model from HuggingFace Hub - https://github.com/huggingface/huggingface_hub/issues/737\n",
            "831: Deprecate passing positional args to most of the public API - https://github.com/huggingface/huggingface_hub/issues/732\n",
            "832: Make the fastai integration test more robust by comparing fastai Learners to be the same - https://github.com/huggingface/huggingface_hub/issues/771\n",
            "833: [🐞 BUG?] Empty README.md with YAML header gets overwritten for datasets - https://github.com/huggingface/huggingface_hub/issues/725\n",
            "834: Adding a new \"model tag category\": Environments for Reinforcement Learning task - https://github.com/huggingface/huggingface_hub/issues/724\n",
            "835: Inference API documentation detailed parameters missing \"do_sample\" - https://github.com/huggingface/huggingface_hub/issues/722\n",
            "836: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/721\n",
            "837: Update permissions for a token automatically - https://github.com/huggingface/huggingface_hub/issues/715\n",
            "838: [🐞Bug?] Canonical datasets on Hub link to github file instead of dir - https://github.com/huggingface/huggingface_hub/issues/713\n",
            "839: Grouping models or dataset - https://github.com/huggingface/huggingface_hub/issues/710\n",
            "840: TensorBoard logs with Timestamps - https://github.com/huggingface/huggingface_hub/issues/708\n",
            "841: Support multi-file and/or folder uploads via the Hugging Face Hub UI - https://github.com/huggingface/huggingface_hub/issues/707\n",
            "842: Deprecate organization parameter in create/delete/update_repo_visibility - https://github.com/huggingface/huggingface_hub/issues/706\n",
            "843: TensorBoard log overwrite disabled - https://github.com/huggingface/huggingface_hub/issues/705\n",
            "844: Download speed Repository - https://github.com/huggingface/huggingface_hub/issues/700\n",
            "845: hugging_face_cli login not working in WIndow - https://github.com/huggingface/huggingface_hub/issues/696\n",
            "846: Error in creating repository - https://github.com/huggingface/huggingface_hub/issues/695\n",
            "847: rename_repo function for huggingface_hub library - https://github.com/huggingface/huggingface_hub/issues/694\n",
            "848: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/690\n",
            "849: LFS: Authorization error when uploading many/large files. - https://github.com/huggingface/huggingface_hub/issues/689\n",
            "850: Git add with auto_lfs_track should track binary files - https://github.com/huggingface/huggingface_hub/issues/687\n",
            "851: Support canonical dataset endpoints in HfApi.upload_file - https://github.com/huggingface/huggingface_hub/issues/686\n",
            "852: Time-ago issue on 'Files and versions' / repo view - https://github.com/huggingface/huggingface_hub/issues/685\n",
            "853: Document server-side locks for HfApi.upload_file - https://github.com/huggingface/huggingface_hub/issues/683\n",
            "854: Model filter does not seem to work correctly with model_args.author.NewT5 - https://github.com/huggingface/huggingface_hub/issues/675\n",
            "855: Repository does not work with Spaces - https://github.com/huggingface/huggingface_hub/issues/672\n",
            "856: api.set_access_token seems to fail silently - https://github.com/huggingface/huggingface_hub/issues/661\n",
            "857: Logging with organization token is successful and leads to side effects - https://github.com/huggingface/huggingface_hub/issues/658\n",
            "858: Automatically obtain organization in keras push_to_hub - https://github.com/huggingface/huggingface_hub/issues/652\n",
            "859: Invoke the CLI on a private hub endpoint - https://github.com/huggingface/huggingface_hub/issues/650\n",
            "860: Render Jupyter Notebooks as web pages - https://github.com/huggingface/huggingface_hub/issues/644\n",
            "861: DOC Clarification on non-text examples - https://github.com/huggingface/huggingface_hub/issues/643\n",
            "862: Allow handle_impossible_answer=True in Inference API - https://github.com/huggingface/huggingface_hub/issues/640\n",
            "863: List all datasets or models using token - https://github.com/huggingface/huggingface_hub/issues/634\n",
            "864: Model tags are not correctly loaded - https://github.com/huggingface/huggingface_hub/issues/629\n",
            "865: Model cards for Keras models - https://github.com/huggingface/huggingface_hub/issues/628\n",
            "866: Automatically upload tensorboard traces if available with keras push to hub - https://github.com/huggingface/huggingface_hub/issues/627\n",
            "867: unknown error in TTS Model Loading - https://github.com/huggingface/huggingface_hub/issues/624\n",
            "868: Text2text task link redirects to text2text - https://github.com/huggingface/huggingface_hub/issues/612\n",
            "869: Update widget links - https://github.com/huggingface/huggingface_hub/issues/604\n",
            "870: hf_hub_download does not pre-cache files for Model.from_pretrained - https://github.com/huggingface/huggingface_hub/issues/601\n",
            "871: Paginate Organizations page - https://github.com/huggingface/huggingface_hub/issues/600\n",
            "872: Use all parameters of the keras.models.save_model - https://github.com/huggingface/huggingface_hub/issues/598\n",
            "873: Enable download stats for Keras models - https://github.com/huggingface/huggingface_hub/issues/597\n",
            "874: Some custom objects are not being serialized with push_to_hub_keras - https://github.com/huggingface/huggingface_hub/issues/595\n",
            "875: Saving a keras model with the data augmentation layer - https://github.com/huggingface/huggingface_hub/issues/593\n",
            "876: Make it possible to skip large files when doing push_to_hub - https://github.com/huggingface/huggingface_hub/issues/591\n",
            "877: Add warning when cloning/downloading from repos we've marked as unsafe - https://github.com/huggingface/huggingface_hub/issues/586\n",
            "878: Can't upload custom TF models e.g. TFBERT, TFRoBERTa due to model.save() failing - https://github.com/huggingface/huggingface_hub/issues/582\n",
            "879: 'num_return_sequences' Parameter for Text2Text generation Models - https://github.com/huggingface/huggingface_hub/issues/565\n",
            "880: Push to Hub Callback for non-transformer Keras models - https://github.com/huggingface/huggingface_hub/issues/559\n",
            "881: Allow changing remote origin with Repository - https://github.com/huggingface/huggingface_hub/issues/558\n",
            "882: Creating a repo does not allow using model as repo_type - https://github.com/huggingface/huggingface_hub/issues/557\n",
            "883: website error - https://github.com/huggingface/huggingface_hub/issues/550\n",
            "884: Model template - https://github.com/huggingface/huggingface_hub/issues/549\n",
            "885: Housekeeping: Improve the time for the base test suite + Randomly Failing Tests - https://github.com/huggingface/huggingface_hub/issues/544\n",
            "886: New Filtering API for HfApi - https://github.com/huggingface/huggingface_hub/issues/535\n",
            "887: (Spaces) Unable to sync with github actions - https://github.com/huggingface/huggingface_hub/issues/534\n",
            "888: push_to_hub_keras should support multiple models - https://github.com/huggingface/huggingface_hub/issues/533\n",
            "889: [Quick poll] Give your opinion on the future of the Hugging Face Open Source ecosystem! - https://github.com/huggingface/huggingface_hub/issues/521\n",
            "890: Since 0.2.0, git push fails (on Colab) - https://github.com/huggingface/huggingface_hub/issues/520\n",
            "891: List models by author from HfApi - https://github.com/huggingface/huggingface_hub/issues/513\n",
            "892: Selecting a sample in audio widgets automatically plays the audio - https://github.com/huggingface/huggingface_hub/issues/508\n",
            "893: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/506\n",
            "894: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/503\n",
            "895: Add ability to upload folder with upload_file - https://github.com/huggingface/huggingface_hub/issues/497\n",
            "896: Enable snapshot_download for different repo types - https://github.com/huggingface/huggingface_hub/issues/496\n",
            "897: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/495\n",
            "898: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/492\n",
            "899: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/491\n",
            "900: Minor improvements to audio to audio widget output - https://github.com/huggingface/huggingface_hub/issues/480\n",
            "901: [Spaces: Feature Request] Add option to select sdk: static when creating new space - https://github.com/huggingface/huggingface_hub/issues/476\n",
            "902: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/471\n",
            "903: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/469\n",
            "904: CORs headers for model files - https://github.com/huggingface/huggingface_hub/issues/468\n",
            "905: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/466\n",
            "906: Chat bot empty replies - https://github.com/huggingface/huggingface_hub/issues/465\n",
            "907: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/463\n",
            "908: Implement a download_file for symmetry with upload_file - https://github.com/huggingface/huggingface_hub/issues/460\n",
            "909: ImportError: cannot import name 'TypeAlias' from 'typing_extensions' - https://github.com/huggingface/huggingface_hub/issues/452\n",
            "910: Document static Spaces - https://github.com/huggingface/huggingface_hub/issues/440\n",
            "911: Document user roles on the hub - https://github.com/huggingface/huggingface_hub/issues/439\n",
            "912: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/437\n",
            "913: Add dropdown inputs on widgets - https://github.com/huggingface/huggingface_hub/issues/432\n",
            "914: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/427\n",
            "915: [Feature request] Add ability to skip large files when creating Repository - https://github.com/huggingface/huggingface_hub/issues/424\n",
            "916: Adding models from tf-transformers to hub - https://github.com/huggingface/huggingface_hub/issues/410\n",
            "917: Don't import tensorflow on startup - https://github.com/huggingface/huggingface_hub/issues/406\n",
            "918: Run CI on Windows? - https://github.com/huggingface/huggingface_hub/issues/404\n",
            "919: Document model card Inference API parameters capability - https://github.com/huggingface/huggingface_hub/issues/402\n",
            "920: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/401\n",
            "921: A new community image was pushed, need to update API - https://github.com/huggingface/huggingface_hub/issues/400\n",
            "922: Cant create new space from huggingface-cli - https://github.com/huggingface/huggingface_hub/issues/396\n",
            "923: Cannot create new space with HfApi.create_repo - https://github.com/huggingface/huggingface_hub/issues/393\n",
            "924: Suppress warning for 'config.json not found in HuggingFace Hub' for Keras models - https://github.com/huggingface/huggingface_hub/issues/375\n",
            "925: Add timeout parameter to HfApi.dataset_info to work on JZ - https://github.com/huggingface/huggingface_hub/issues/372\n",
            "926: Error message display user tokens - https://github.com/huggingface/huggingface_hub/issues/351\n",
            "927: InferenceApi for TokenClassification does not work - https://github.com/huggingface/huggingface_hub/issues/344\n",
            "928: Should we use functions instead of mix-ins to define integration helpers? - https://github.com/huggingface/huggingface_hub/issues/328\n",
            "929: Should we use functions instead of mix-ins to define integration helpers? - https://github.com/huggingface/huggingface_hub/issues/327\n",
            "930: Generic push_to_hub by adding optional save_fn - https://github.com/huggingface/huggingface_hub/issues/310\n",
            "931: PyTorchModelHubMixin cannot push local files to the Hub - https://github.com/huggingface/huggingface_hub/issues/309\n",
            "932: Adding Mistral Checkpoints to HF Hub - https://github.com/huggingface/huggingface_hub/issues/300\n",
            "933: Document how to add a new task - https://github.com/huggingface/huggingface_hub/issues/295\n",
            "934: Splinter's Inference API doesn't work - https://github.com/huggingface/huggingface_hub/issues/289\n",
            "935: 🚀 Feature request: Add git instructions to dataset repos - https://github.com/huggingface/huggingface_hub/issues/279\n",
            "936: Space - Failed to process a Websocket message - https://github.com/huggingface/huggingface_hub/issues/275\n",
            "937: Add TensorFlow \"saved_model\" folder to .gitattributes to new model repos by default - https://github.com/huggingface/huggingface_hub/issues/274\n",
            "938: Enable Pipeline arguments for the models on the widget - https://github.com/huggingface/huggingface_hub/issues/270\n",
            "939: Enable branch to be specified in Repository.push_to_hub - https://github.com/huggingface/huggingface_hub/issues/268\n",
            "940: generalizing the git status file retrieval functions - https://github.com/huggingface/huggingface_hub/issues/264\n",
            "941: Feature request: overwrite local dir when using Repository - https://github.com/huggingface/huggingface_hub/issues/260\n",
            "942: Asteroid Inference API unexpectedly slow - https://github.com/huggingface/huggingface_hub/issues/258\n",
            "943: Inference API wrapper: adding parameters documentation - https://github.com/huggingface/huggingface_hub/issues/256\n",
            "944: Audio to Audio widget does not show results - https://github.com/huggingface/huggingface_hub/issues/250\n",
            "945: Add sentence similarity for spaCy - https://github.com/huggingface/huggingface_hub/issues/241\n",
            "946: huggingface_hub release notes include unrelated changes - https://github.com/huggingface/huggingface_hub/issues/237\n",
            "947: Error when creating a repository - https://github.com/huggingface/huggingface_hub/issues/233\n",
            "948: Profiling info to model hub? - https://github.com/huggingface/huggingface_hub/issues/218\n",
            "949: Doing push_to_hub with a large model fail by default - https://github.com/huggingface/huggingface_hub/issues/217\n",
            "950: Only use git credentials, no more token - https://github.com/huggingface/huggingface_hub/issues/211\n",
            "951: Malformed soundfile error when using ASR Widget - https://github.com/huggingface/huggingface_hub/issues/210\n",
            "952: Spaces: Feature Request: Include git submodules in codebase. - https://github.com/huggingface/huggingface_hub/issues/200\n",
            "953: Spaces: Gradio interface title and description is invisible in darkmode - https://github.com/huggingface/huggingface_hub/issues/199\n",
            "954: [Feature request] Allow specifying save_dir in snapshot_download - https://github.com/huggingface/huggingface_hub/issues/198\n",
            "955: HfApi.model_info(revision=) does not resolve hash prefix - https://github.com/huggingface/huggingface_hub/issues/197\n",
            "956: Spaces: Third party component streamlit-tags not loading - https://github.com/huggingface/huggingface_hub/issues/195\n",
            "957: Uniformized logging with other HF libs - https://github.com/huggingface/huggingface_hub/issues/191\n",
            "958: Mention the Tensorboard integration somewhere in ./docs/hub - https://github.com/huggingface/huggingface_hub/issues/190\n",
            "959: Repository use_auth_token should default to True when in a HF-hub specific workflow - https://github.com/huggingface/huggingface_hub/issues/187\n",
            "960: Better error message when cloning without token - https://github.com/huggingface/huggingface_hub/issues/186\n",
            "961: Give user a feedback when API returns something unepected with 200 - https://github.com/huggingface/huggingface_hub/issues/182\n",
            "962: Add text classification for spaCy - https://github.com/huggingface/huggingface_hub/issues/178\n",
            "963: Non-blocking git_push - https://github.com/huggingface/huggingface_hub/issues/169\n",
            "964: Add --filename support for git lfs track - https://github.com/huggingface/huggingface_hub/issues/168\n",
            "965: Add filtering and fields to /api/datasets - https://github.com/huggingface/huggingface_hub/issues/167\n",
            "966: Netlify authorization issue on JS Widgets action - https://github.com/huggingface/huggingface_hub/issues/160\n",
            "967: Should HfApi().login save the token? - https://github.com/huggingface/huggingface_hub/issues/154\n",
            "968: [Feature request] Add a hook to allow user to manipulate model init kwargs - https://github.com/huggingface/huggingface_hub/issues/149\n",
            "969: 404 at 'Adding a Library' documentation page - https://github.com/huggingface/huggingface_hub/issues/146\n",
            "970: Repository can't be used with datasets repositories anymore - https://github.com/huggingface/huggingface_hub/issues/144\n",
            "971: [Feature proposal] Ignore files for snapshot_download - https://github.com/huggingface/huggingface_hub/issues/139\n",
            "972: Detect and track large files automatically - https://github.com/huggingface/huggingface_hub/issues/138\n",
            "973: Widget for Structured Data Classification - https://github.com/huggingface/huggingface_hub/issues/134\n",
            "974: Add Documentation page on list_models, model_info, and similar methods - https://github.com/huggingface/huggingface_hub/issues/129\n",
            "975: Feature request: enable search and download of dataset metadata - https://github.com/huggingface/huggingface_hub/issues/126\n",
            "976: Widget for text to image generation - https://github.com/huggingface/huggingface_hub/issues/113\n",
            "977: Add to pipeline \"feature-extration\": \"sentence-transformers / paraphrase-multilingual-MiniLM-L12-v2\" model - https://github.com/huggingface/huggingface_hub/issues/92\n",
            "978: push was rejected because it contains files larger than 10M - https://github.com/huggingface/huggingface_hub/issues/91\n",
            "979: [Feature request] Scikit learn integration - https://github.com/huggingface/huggingface_hub/issues/90\n",
            "980: Python versions with rc... suffix are not recognized - https://github.com/huggingface/huggingface_hub/issues/79\n",
            "981: [Feature request] linking docs - https://github.com/huggingface/huggingface_hub/issues/77\n",
            "982: TTS models response header is sent as application/json - https://github.com/huggingface/huggingface_hub/issues/64\n",
            "983: Open source our Inference widgets - https://github.com/huggingface/huggingface_hub/issues/56\n",
            "984: Feature Request: Have programatic way of adding metadata to a repo - https://github.com/huggingface/huggingface_hub/issues/54\n",
            "985: Feature request: Change repo from private to public (or viceversa) programatically - https://github.com/huggingface/huggingface_hub/issues/53\n",
            "986: Discussion: download and upload models have a somewhat inconsistent API - https://github.com/huggingface/huggingface_hub/issues/47\n",
            "987: Feature request: snapshot_download - add library_name and library_version - https://github.com/huggingface/huggingface_hub/issues/37\n",
            "988: Feature request: Check if repo / file exists - https://github.com/huggingface/huggingface_hub/issues/36\n",
            "989: [Issue]: Too long file-paths for Windows - https://github.com/huggingface/huggingface_hub/issues/35\n",
            "990: Add model card via the APIs - https://github.com/huggingface/huggingface_hub/issues/33\n",
            "991: Feature Request: Be able to search the hub by substring and author - https://github.com/huggingface/huggingface_hub/issues/31\n",
            "992: ModelHubMixin 400 Client Error - https://github.com/huggingface/huggingface_hub/issues/29\n",
            "993: Added to conda-forge - https://github.com/huggingface/huggingface_hub/issues/18\n",
            "994: cached_download, but with custom file name - https://github.com/huggingface/huggingface_hub/issues/15\n",
            "995: Support for private models? - https://github.com/huggingface/huggingface_hub/issues/10\n",
            "996: Add simple class to ease saving loading from HF hub - https://github.com/huggingface/huggingface_hub/issues/9\n",
            "997: Try wrapping git-lfs to improve its ergonomics for large files - https://github.com/huggingface/huggingface_hub/issues/6\n",
            "998: Only expose toplevel imports, not \"nested\" ones - https://github.com/huggingface/huggingface_hub/issues/5\n"
          ]
        }
      ],
      "source": [
        "# Set up Chrome options for Colab\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "import time\n",
        "\n",
        "# Set up Chrome options to use headless mode (for Colab)\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "\n",
        "# Add extra options\n",
        "chrome_options.add_argument(\"--window-size=1920,1080\")  # Set the window size\n",
        "chrome_options.add_argument(\"--disable-infobars\")  # Disable the infobars\n",
        "chrome_options.add_argument(\"--disable-popup-blocking\")  # Disable pop-ups\n",
        "chrome_options.add_argument(\"--ignore-certificate-errors\")  # Ignore certificate errors\n",
        "chrome_options.add_argument(\"--incognito\")  # Use Chrome in incognito mode\n",
        "\n",
        "# Set the path to chromedriver explicitly (installed by apt)\n",
        "chrome_path = \"/usr/bin/chromedriver\"\n",
        "\n",
        "# Initialize the WebDriver with the updated path\n",
        "driver = gs.Chrome(options=chrome_options)\n",
        "\n",
        "# Open the GitHub Issues page\n",
        "url = \"https://github.com/huggingface/huggingface_hub/issues?q=is%3Aissue\"  # URL for GitHub Issues\n",
        "driver.get(url)\n",
        "\n",
        "# Wait for the page to load\n",
        "time.sleep(5)\n",
        "\n",
        "# Base URL of GitHub\n",
        "# base_url = \"https://github.com\"\n",
        "\n",
        "# Function to scrape titles and links, handling pagination\n",
        "def scrape_github_issues():\n",
        "    all_issues = []\n",
        "    while True:\n",
        "        # Find all issue titles using the correct class (GitHub's issue titles)\n",
        "        elements = driver.find_elements(By.CSS_SELECTOR, 'a.Link--primary')\n",
        "\n",
        "        # Extract and store the titles and links\n",
        "        for elem in elements:\n",
        "            issue_title = elem.text.strip()\n",
        "            issue_link = elem.get_attribute('href')\n",
        "            #issue_link = base_url + elem.get_attribute('href')\n",
        "            all_issues.append((issue_title, issue_link))\n",
        "\n",
        "        # Check if there's a \"Next\" button to navigate to the next page\n",
        "        try:\n",
        "            next_button = driver.find_element(By.XPATH, '//a[@class=\"next_page\"]')\n",
        "            next_button.click()  # Click the \"Next\" button\n",
        "            time.sleep(5)  # Wait for the next page to load\n",
        "        except Exception as e:\n",
        "            # If there's no \"Next\" button, we break out of the loop (end of pagination)\n",
        "            print(\"No more pages or an error occurred.\")\n",
        "            break\n",
        "\n",
        "    return all_issues\n",
        "\n",
        "# Scrape the GitHub issues across all pages\n",
        "issues = scrape_github_issues()\n",
        "\n",
        "# Print all scraped issues\n",
        "print(\"Scraped GitHub Issue Titles and Links:\")\n",
        "for i, (title, link) in enumerate(issues, 1):\n",
        "    print(f\"{i}: {title} - {link}\")\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6Xz1ZFtoV3zY",
        "outputId": "99bfbc70-3e1e-4cd0-a3a1-e51d0eefa70a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"53eb6bf8-c794-4b96-9176-ab2484d0fb9a-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"53eb6bf8-c794-4b96-9176-ab2484d0fb9a-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"53eb6bf8-c794-4b96-9176-ab2484d0fb9a-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"53eb6bf8-c794-4b96-9176-ab2484d0fb9a-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraped Titles and Links:\n",
            "1: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "2: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "3: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "4: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "5: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "6: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "7: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "8: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "9: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "10: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "11: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "12: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "13: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "14: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "15: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "16: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "17: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "18: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "19: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "20: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "21: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "22: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "23: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "24: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "25: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "26: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "27: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "28: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "29: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "30: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "31: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "32: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "33: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "34: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "35: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "36: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "37: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "38: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "39: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "40: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "41: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "42: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "43: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "44: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "45: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "46: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "47: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "48: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "49: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "50: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "51: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "52: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "53: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "54: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "55: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "56: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "57: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "58: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "59: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "60: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "61: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "62: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "63: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "64: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "65: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "66: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "67: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "68: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "69: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "70: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "71: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "72: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "73: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "74: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "75: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "76: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "77: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "78: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "79: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "80: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "81: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "82: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "83: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "84: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "85: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "86: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "87: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "88: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "89: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "90: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "91: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "92: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "93: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "94: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "95: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "96: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "97: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "98: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "99: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "100: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "101: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "102: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "103: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "104: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "105: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "106: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "107: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "108: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "109: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "110: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "111: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "112: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "113: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "114: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "115: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "116: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "117: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "118: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "119: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "120: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "121: BertTokenizer.decode not understanding new vocabulary - https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "122: Tokenizer tend to choose added tokens first rather than token in vocab - https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "123: Special token printed out as output - https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948\n",
            "124: [NER][Japanese] labeled segment shorter than token - https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "125: T5Tokenizer add a whitespace token after added special tokens - https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "126: Unable to register my own tokenizer - https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "127: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab! - https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "128: Use Unicode blocks in regex (in Replace normalizer) - https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904\n",
            "129: How to handle translations one source language to many target sentences for the same language - https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "130: NER Label tokenization with overflowing tokens - https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561\n",
            "131: How to use a trained tokenizer for semantic search? - https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "132: Unk_token not set after training a BPETokenizer tokenizer - https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733\n",
            "133: AutoTokenizer is very slow when loading llama tokenizer - https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547\n",
            "134: Offset mappings differ for tokenizers - https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424\n",
            "135: The process for tokenizing concatenated dataset is slow st the end of tokenizing - https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "136: Batch tokenize (split into tokens, without processing) - https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238\n",
            "137: Does AutoTokenizer uploads data to HuggingFace - https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "138: RobertaTokenizer decode and tokenize do not have the same output - https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "139: Training tokenizers with padding in between tokens - https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "140: Leaving unknown words untokenized like in OpenMNT - https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "141: Keeping special chars in translations - https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270\n",
            "142: Should cls_token be [CLS] or <cls>? - https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "143: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False - https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "144: I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error - https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742\n",
            "145: OSError: Can’t load tokenizer for ‘facebook/xmod-base’ - https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471\n",
            "146: `GPT2Tokenizer` Tokenizer handling `\\n\\n` differently in different settings - https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289\n",
            "147: DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‘padding=True’ ‘truncation=True’ to have batched tensors with the same length - https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625\n",
            "148: Decode token IDs into a list (not a single string) - https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991\n",
            "149: Error training MLM with Roberta Tokenizer - https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878\n",
            "150: Are the slow and fast tokenizer results the same output for the same input? - https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "151: “Add_tokens” breaks words when encoding - https://discuss.huggingface.co/t/add-tokens-breaks-words-when-encoding/21154\n",
            "152: 2 tokens for one character in T5 - https://discuss.huggingface.co/t/2-tokens-for-one-character-in-t5/14960\n",
            "153: OSError: Model name ‘gpt2’ was not found in tokenizers model name list (gpt2,…) - https://discuss.huggingface.co/t/oserror-model-name-gpt2-was-not-found-in-tokenizers-model-name-list-gpt2/2164\n",
            "154: DNA long sequence tokenization - https://discuss.huggingface.co/t/dna-long-sequence-tokenization/16154\n",
            "155: SentencePiece tokenizer encodes to unknown token - https://discuss.huggingface.co/t/sentencepiece-tokenizer-encodes-to-unknown-token/49113\n",
            "156: Tokenizer behaviour with pipeline - https://discuss.huggingface.co/t/tokenizer-behaviour-with-pipeline/48969\n",
            "157: Load tokenizer from file : Exception: data did not match any variant of untagged enum ModelWrapper - https://discuss.huggingface.co/t/load-tokenizer-from-file-exception-data-did-not-match-any-variant-of-untagged-enum-modelwrapper/25325\n",
            "158: ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076 - https://discuss.huggingface.co/t/arrowinvalid-column-3-named-attention-mask-expected-length-1000-but-got-length-1076/6904\n",
            "159: Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model - https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829\n",
            "160: Initialize Vocabulary for Unigram Tokenizer - https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371\n",
            "161: Make correct padding for text generation with GPT-NEO - https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800\n",
            "162: How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers? - https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639\n",
            "163: Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF - https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602\n",
            "164: Writing custom tokenizer and wrapping it in tokenizer object - https://discuss.huggingface.co/t/writing-custom-tokenizer-and-wrapping-it-in-tokenizer-object/39679\n",
            "165: Tokenizer for German lang - https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222\n",
            "166: Chunk tokens into desired chunk length without simply getting rid of rest of tokens - https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399\n",
            "167: Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers - https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872\n",
            "168: LlamaTokenizerFast returns token_type_ids but the forward pass of the LlamaModel does not receive token_type_ids - https://discuss.huggingface.co/t/llamatokenizerfast-returns-token-type-ids-but-the-forward-pass-of-the-llamamodel-does-not-receive-token-type-ids/42431\n",
            "169: GPT2Tokenizer not working in Kaggle Notebook - https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508\n",
            "170: Exploring the Majestic Temples in Karnataka - https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952\n",
            "171: Tokenizer producing token index greater than size of the dictionary - https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989\n",
            "172: How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer - https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843\n",
            "173: How to create a HF tokenizer’s vocab file from a BPE model’s merges.txt file? - https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737\n",
            "174: Scala/JVM Bindings for Tokenizers - https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393\n",
            "175: Tokenizers Wheel Takes Forever to Build - https://discuss.huggingface.co/t/tokenizers-wheel-takes-forever-to-build/17114\n",
            "176: Where the introduction of tokenizers.implementations? - https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959\n",
            "177: How to return custom `token_type_ids` or other values from a tokenizer? - https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525\n",
            "178: Easy way to compare tokenizers - https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347\n",
            "179: Unable to load image using llama-index - https://discuss.huggingface.co/t/unable-to-load-image-using-llama-index/38304\n",
            "180: Help defining tokenizer - https://discuss.huggingface.co/t/help-defining-tokenizer/38091\n",
            "181: Token Offsets in Rust vs. Python - https://discuss.huggingface.co/t/token-offsets-in-rust-vs-python/37949\n",
            "182: “OSError: Model name ‘./XX’ was not found in tokenizers model name list” - cannot load custom tokenizer in Transformers - https://discuss.huggingface.co/t/oserror-model-name-xx-was-not-found-in-tokenizers-model-name-list-cannot-load-custom-tokenizer-in-transformers/2714\n",
            "183: Converting JSON/dict to flatten string with indicator tokens - https://discuss.huggingface.co/t/converting-json-dict-to-flatten-string-with-indicator-tokens/37387\n",
            "184: Train Retry Tokenizer - https://discuss.huggingface.co/t/train-retry-tokenizer/37031\n",
            "185: Pretokenise on punctuation except hyphens - https://discuss.huggingface.co/t/pretokenise-on-punctuation-except-hyphens/36691\n",
            "186: Tokenizer Trainer Crashing - https://discuss.huggingface.co/t/tokenizer-trainer-crashing/36645\n",
            "187: Tokenizer extremely slow when deployed to a container - https://discuss.huggingface.co/t/tokenizer-extremely-slow-when-deployed-to-a-container/36569\n",
            "188: Dealing with Decimal and Fractions - https://discuss.huggingface.co/t/dealing-with-decimal-and-fractions/23377\n",
            "189: `add_tokens` with argument `special_tokens=True` vs `add_special_tokens` - https://discuss.huggingface.co/t/add-tokens-with-argument-special-tokens-true-vs-add-special-tokens/35648\n",
            "190: Unable to upload custom Pytorch model in huggingface - https://discuss.huggingface.co/t/unable-to-upload-custom-pytorch-model-in-huggingface/35545\n",
            "191: RuntimeError: Cannot re-initialize CUDA in forked subprocess - https://discuss.huggingface.co/t/runtimeerror-cannot-re-initialize-cuda-in-forked-subprocess/28392\n",
            "192: Overflowing Tokens in MarkupLM - https://discuss.huggingface.co/t/overflowing-tokens-in-markuplm/35217\n",
            "193: I get the predicted token as ` े` . What am I doing wrong? - https://discuss.huggingface.co/t/i-get-the-predicted-token-as-what-am-i-doing-wrong/29038\n",
            "194: <unk> token in the output instead curly braces - https://discuss.huggingface.co/t/unk-token-in-the-output-instead-curly-braces/34653\n",
            "195: How to add a new token without expanding the vocabulary - https://discuss.huggingface.co/t/how-to-add-a-new-token-without-expanding-the-vocabulary/34536\n",
            "196: Does the ByteLevelBPETokenizer need to be wrapped in a normal Tokenizer? - https://discuss.huggingface.co/t/does-the-bytelevelbpetokenizer-need-to-be-wrapped-in-a-normal-tokenizer/34121\n",
            "197: What is required to create a fast tokenizer? For example for a Marian model - https://discuss.huggingface.co/t/what-is-required-to-create-a-fast-tokenizer-for-example-for-a-marian-model/33941\n",
            "198: GPT2Tokenizer.decode maps unicode sequences to the same string ‘�’ - https://discuss.huggingface.co/t/gpt2tokenizer-decode-maps-unicode-sequences-to-the-same-string/32453\n",
            "199: Issue with Tokenizer - https://discuss.huggingface.co/t/issue-with-tokenizer/33796\n",
            "200: Tokenizing my novel for GPT model - https://discuss.huggingface.co/t/tokenizing-my-novel-for-gpt-model/33532\n",
            "201: How to add additional custom pre-tokenization processing? - https://discuss.huggingface.co/t/how-to-add-additional-custom-pre-tokenization-processing/1637\n",
            "202: Customize FlauBERT tokenizer to split line breaks - https://discuss.huggingface.co/t/customize-flaubert-tokenizer-to-split-line-breaks/33011\n",
            "203: How to change the size of model_max_length? - https://discuss.huggingface.co/t/how-to-change-the-size-of-model-max-length/32942\n",
            "204: Can’t get to the source code of `tokenizer.convert_tokens_to_string` - https://discuss.huggingface.co/t/cant-get-to-the-source-code-of-tokenizer-convert-tokens-to-string/32714\n",
            "205: Why I’m getting same result with or without using Wav2Vec2Processor? - https://discuss.huggingface.co/t/why-im-getting-same-result-with-or-without-using-wav2vec2processor/32482\n",
            "206: How does `tokenizer().input_ids` work and how different it is from tokenizer.encode() before `model.generate()` and decoding step? - https://discuss.huggingface.co/t/how-does-tokenizer-input-ids-work-and-how-different-it-is-from-tokenizer-encode-before-model-generate-and-decoding-step/30476\n",
            "207: What file type should my training data be? - https://discuss.huggingface.co/t/what-file-type-should-my-training-data-be/32075\n",
            "208: Best way to get the closest token indices of input of char_to_token is a whitespace - https://discuss.huggingface.co/t/best-way-to-get-the-closest-token-indices-of-input-of-char-to-token-is-a-whitespace/32004\n",
            "209: Token indices sequence length is longer than the specified maximum sequence length - https://discuss.huggingface.co/t/token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length/25133\n",
            "210: Create a simple tokenizer - https://discuss.huggingface.co/t/create-a-simple-tokenizer/31677\n",
            "211: Sliding window for Long Documents - https://discuss.huggingface.co/t/sliding-window-for-long-documents/19367\n",
            "212: Creating tokenizer from counts file? - https://discuss.huggingface.co/t/creating-tokenizer-from-counts-file/31399\n",
            "213: Tokenizer.train() running out of memory - https://discuss.huggingface.co/t/tokenizer-train-running-out-of-memory/31355\n",
            "214: Tokenizing Float Tensor? - https://discuss.huggingface.co/t/tokenizing-float-tensor/30604\n",
            "215: Padding and truncation for custom tokenizer - https://discuss.huggingface.co/t/padding-and-truncation-for-custom-tokenizer/30180\n",
            "216: Incorporate SARI score into run_summarization.py example script - https://discuss.huggingface.co/t/incorporate-sari-score-into-run-summarization-py-example-script/29511\n",
            "217: Is that possible to embed the tokenizer into the model to have it running on GCP using TensorFlow Serving? - https://discuss.huggingface.co/t/is-that-possible-to-embed-the-tokenizer-into-the-model-to-have-it-running-on-gcp-using-tensorflow-serving/10532\n",
            "218: Huggingface inference API issue - https://discuss.huggingface.co/t/huggingface-inference-api-issue/29307\n",
            "219: Using Tokenizer for integer data - https://discuss.huggingface.co/t/using-tokenizer-for-integer-data/28835\n",
            "220: GPT2 long text approach - https://discuss.huggingface.co/t/gpt2-long-text-approach/28165\n",
            "221: Huggingface t5 models seem to not download a tokenizer file - https://discuss.huggingface.co/t/huggingface-t5-models-seem-to-not-download-a-tokenizer-file/27935\n",
            "222: How to save a fast tokenizer using the transformer library and then load it using Tokenizers? - https://discuss.huggingface.co/t/how-to-save-a-fast-tokenizer-using-the-transformer-library-and-then-load-it-using-tokenizers/9567\n",
            "223: Using a BertTokenizer when training a RobertaForMaskedLM - https://discuss.huggingface.co/t/using-a-berttokenizer-when-training-a-robertaformaskedlm/27456\n",
            "224: Need clarity on “padding” parameter in Bert Tokenizer - https://discuss.huggingface.co/t/need-clarity-on-padding-parameter-in-bert-tokenizer/27444\n",
            "225: How to convert HuggingFace tokenizers into ONNX format? - https://discuss.huggingface.co/t/how-to-convert-huggingface-tokenizers-into-onnx-format/27272\n",
            "226: Can’t save ConvBert tokenizer - https://discuss.huggingface.co/t/cant-save-convbert-tokenizer/16006\n",
            "227: RoBERTa Tokenizer Java Implementation - https://discuss.huggingface.co/t/roberta-tokenizer-java-implementation/16737\n",
            "228: Unigram vocab_size doesn’t fit - https://discuss.huggingface.co/t/unigram-vocab-size-doesnt-fit/26856\n",
            "229: Option to load only tokenizer and model configuration into “token-classification” pipeline - https://discuss.huggingface.co/t/option-to-load-only-tokenizer-and-model-configuration-into-token-classification-pipeline/26697\n",
            "230: Encode_plus Pretokenized input seuqence must be Union - https://discuss.huggingface.co/t/encode-plus-pretokenized-input-seuqence-must-be-union/26449\n",
            "231: Application of TFBertTokenizer - https://discuss.huggingface.co/t/application-of-tfberttokenizer/26437\n",
            "232: TemplateProcessing for encoder-decoder - https://discuss.huggingface.co/t/templateprocessing-for-encoder-decoder/26135\n",
            "233: Using `TFBertTokenizer` instead of `BertTokenizer` with `TFBertForQuestionAnswering` - https://discuss.huggingface.co/t/using-tfberttokenizer-instead-of-berttokenizer-with-tfbertforquestionanswering/26127\n",
            "234: How to concatenate an answer to multiple choices after padded tokenization - https://discuss.huggingface.co/t/how-to-concatenate-an-answer-to-multiple-choices-after-padded-tokenization/26110\n",
            "235: Maximum recursion depth exceeded when using DataCollator - https://discuss.huggingface.co/t/maximum-recursion-depth-exceeded-when-using-datacollator/25937\n",
            "236: Adding a special language token to MBART - https://discuss.huggingface.co/t/adding-a-special-language-token-to-mbart/25967\n",
            "237: Custom PostProcessor? - https://discuss.huggingface.co/t/custom-postprocessor/25847\n",
            "238: Tokenizer.pad_token=what? - https://discuss.huggingface.co/t/tokenizer-pad-token-what/24367\n",
            "239: Using HuggingFace Tokenizers Without Special Characters - https://discuss.huggingface.co/t/using-huggingface-tokenizers-without-special-characters/21962\n",
            "240: How to get sp_model variable from T5Tokenizer? - https://discuss.huggingface.co/t/how-to-get-sp-model-variable-from-t5tokenizer/25171\n",
            "241: Wav2vec2CTCTokenizer and vocab.json - https://discuss.huggingface.co/t/wav2vec2ctctokenizer-and-vocab-json/25146\n",
            "242: Period ID in RobertaTokenizer with is_split_into_words - https://discuss.huggingface.co/t/period-id-in-robertatokenizer-with-is-split-into-words/24142\n",
            "243: WordLevel error: Missing [UNK] token from the vocabulary - https://discuss.huggingface.co/t/wordlevel-error-missing-unk-token-from-the-vocabulary/5107\n",
            "244: Tokenizer post_processor help - https://discuss.huggingface.co/t/tokenizer-post-processor-help/21926\n",
            "245: Preprocessing raw text - https://discuss.huggingface.co/t/preprocessing-raw-text/24559\n",
            "246: Save tokenizer with argument - https://discuss.huggingface.co/t/save-tokenizer-with-argument/17389\n",
            "247: Trained tokenizer API as PretrainedTokenizer - https://discuss.huggingface.co/t/trained-tokenizer-api-as-pretrainedtokenizer/23639\n",
            "248: Remove only certain special token id during tokenizer decode - https://discuss.huggingface.co/t/remove-only-certain-special-token-id-during-tokenizer-decode/20436\n",
            "249: Convert_tokens_to_ids produces <unk> - https://discuss.huggingface.co/t/convert-tokens-to-ids-produces-unk/24771\n",
            "250: Text preprocessing for fitting Tokenizer model - https://discuss.huggingface.co/t/text-preprocessing-for-fitting-tokenizer-model/24869\n",
            "251: Special tokens warning - https://discuss.huggingface.co/t/special-tokens-warning/24939\n",
            "252: Simple Transformers Multilabelclassification - https://discuss.huggingface.co/t/simple-transformers-multilabelclassification/24568\n",
            "253: Cannot initialize deberta-v3-base tokenizer - https://discuss.huggingface.co/t/cannot-initialize-deberta-v3-base-tokenizer/15322\n",
            "254: Getting Wholeword corresponding to a subword in a text? - https://discuss.huggingface.co/t/getting-wholeword-corresponding-to-a-subword-in-a-text/24152\n",
            "255: Issue with pushing tokenizer to hub - https://discuss.huggingface.co/t/issue-with-pushing-tokenizer-to-hub/24120\n",
            "256: How do we customize the number of entites for NER pretrained model? - https://discuss.huggingface.co/t/how-do-we-customize-the-number-of-entites-for-ner-pretrained-model/24085\n",
            "257: Configure RobertaTokenizer - https://discuss.huggingface.co/t/configure-robertatokenizer/23969\n",
            "258: How to properly clean vocabulary from BBPE tokenizer - https://discuss.huggingface.co/t/how-to-properly-clean-vocabulary-from-bbpe-tokenizer/22827\n",
            "259: Map tokenization and posterior to smaller substrings - https://discuss.huggingface.co/t/map-tokenization-and-posterior-to-smaller-substrings/23792\n",
            "260: T5 model tokenizer - https://discuss.huggingface.co/t/t5-model-tokenizer/23640\n",
            "261: Fast tokenizer for marianMTModel - https://discuss.huggingface.co/t/fast-tokenizer-for-marianmtmodel/23638\n",
            "262: Word tokenizers for text generators - https://discuss.huggingface.co/t/word-tokenizers-for-text-generators/23464\n",
            "263: SentencePieceUnigramTokenizer - https://discuss.huggingface.co/t/sentencepieceunigramtokenizer/23509\n",
            "264: Tokenizer is not being loaded on Huggingface Inference - https://discuss.huggingface.co/t/tokenizer-is-not-being-loaded-on-huggingface-inference/23505\n",
            "265: Why is BertNormalizer not exposed on the tokenizers library? - https://discuss.huggingface.co/t/why-is-bertnormalizer-not-exposed-on-the-tokenizers-library/23340\n",
            "266: Sentence splitting - https://discuss.huggingface.co/t/sentence-splitting/5393\n",
            "267: Average time to train a SentencePieceBPETokenizer - https://discuss.huggingface.co/t/average-time-to-train-a-sentencepiecebpetokenizer/23081\n",
            "268: 1 line code for NER data set preparation using tokenizer library! - https://discuss.huggingface.co/t/1-line-code-for-ner-data-set-preparation-using-tokenizer-library/22816\n",
            "269: Microsoft/codebert-base produces two sep tokens - https://discuss.huggingface.co/t/microsoft-codebert-base-produces-two-sep-tokens/21366\n",
            "270: Padding with sliding window - https://discuss.huggingface.co/t/padding-with-sliding-window/18921\n",
            "271: Find which tokens are unknown in new data - https://discuss.huggingface.co/t/find-which-tokens-are-unknown-in-new-data/22466\n",
            "272: How to train target tokenizer - https://discuss.huggingface.co/t/how-to-train-target-tokenizer/22291\n",
            "273: How to know if a subtoken is a word or part of a word? - https://discuss.huggingface.co/t/how-to-know-if-a-subtoken-is-a-word-or-part-of-a-word/923\n",
            "274: BART Tokenizer tokenises same word differently? - https://discuss.huggingface.co/t/bart-tokenizer-tokenises-same-word-differently/21835\n",
            "275: Fine-tuned BERT tokenizer taking too long to load - https://discuss.huggingface.co/t/fine-tuned-bert-tokenizer-taking-too-long-to-load/9747\n",
            "276: Add BOS and EOS when encoding a sentence - https://discuss.huggingface.co/t/add-bos-and-eos-when-encoding-a-sentence/21833\n",
            "277: Customization of Wav2Vec2CTCTokenizer with rules - https://discuss.huggingface.co/t/customization-of-wav2vec2ctctokenizer-with-rules/21912\n",
            "278: Customized tokenization files in run_clm script - https://discuss.huggingface.co/t/customized-tokenization-files-in-run-clm-script/21460\n",
            "279: Using customized algorithm - https://discuss.huggingface.co/t/using-customized-algorithm/21753\n",
            "280: Issue with Flaubert Tokenizer as word_ids() method is not available for NER Task - https://discuss.huggingface.co/t/issue-with-flaubert-tokenizer-as-word-ids-method-is-not-available-for-ner-task/20374\n",
            "281: Word_ids not working with deberta_v2 - https://discuss.huggingface.co/t/word-ids-not-working-with-deberta-v2/21523\n",
            "282: How to tokenize large contexts without running out of memory - https://discuss.huggingface.co/t/how-to-tokenize-large-contexts-without-running-out-of-memory/5882\n",
            "283: Does Deberta tokenizer use wordpiece? - https://discuss.huggingface.co/t/does-deberta-tokenizer-use-wordpiece/21307\n",
            "284: Get vocabulary tokens in order to exclude them from generate function - https://discuss.huggingface.co/t/get-vocabulary-tokens-in-order-to-exclude-them-from-generate-function/5192\n",
            "285: Avoid creating certain tokens when training a tokenizer - https://discuss.huggingface.co/t/avoid-creating-certain-tokens-when-training-a-tokenizer/20864\n",
            "286: Error finetuning XLM-RoBERTa-Large when training - https://discuss.huggingface.co/t/error-finetuning-xlm-roberta-large-when-training/20412\n",
            "287: HuggingFace BPE Trainer Error - Training Tokenizer - https://discuss.huggingface.co/t/huggingface-bpe-trainer-error-training-tokenizer/10629\n",
            "288: Word_to_tokens() and word_ids() —- microsoft/deberta-v2/v3 - https://discuss.huggingface.co/t/word-to-tokens-and-word-ids-microsoft-deberta-v2-v3/19984\n",
            "289: No PreTrainedTokenizerFast for Deberta-V3, no doc_stride - https://discuss.huggingface.co/t/no-pretrainedtokenizerfast-for-deberta-v3-no-doc-stride/20345\n",
            "290: Tokenizer from own vocab - https://discuss.huggingface.co/t/tokenizer-from-own-vocab/20245\n",
            "291: No labels column for tokenized data - https://discuss.huggingface.co/t/no-labels-column-for-tokenized-data/19540\n",
            "292: Programmatic way to Tokenization on Custom Text Columns - https://discuss.huggingface.co/t/programmatic-way-to-tokenization-on-custom-text-columns/19679\n",
            "293: Bug in Offset generation for Rupee symbol - https://discuss.huggingface.co/t/bug-in-offset-generation-for-rupee-symbol/19655\n",
            "294: How to handle parenthesis, quotation marks, \\n etc when creating tokenizer from scratch - https://discuss.huggingface.co/t/how-to-handle-parenthesis-quotation-marks-n-etc-when-creating-tokenizer-from-scratch/19628\n",
            "295: EM training on unigram tokenizer taking way longer than predicted - https://discuss.huggingface.co/t/em-training-on-unigram-tokenizer-taking-way-longer-than-predicted/19502\n",
            "296: Training unigram on long sequences - https://discuss.huggingface.co/t/training-unigram-on-long-sequences/19188\n",
            "297: Issue with post-processing - https://discuss.huggingface.co/t/issue-with-post-processing/2703\n",
            "298: FutureWarning about BertTokenizer.from_pretrained() at latest version - https://discuss.huggingface.co/t/futurewarning-about-berttokenizer-from-pretrained-at-latest-version/18750\n",
            "299: Enhaced word_ids() API for Chinese or CJK languages? - https://discuss.huggingface.co/t/enhaced-word-ids-api-for-chinese-or-cjk-languages/18662\n",
            "300: Importing tokenizers version >0.10.3 fails due to openssl - https://discuss.huggingface.co/t/importing-tokenizers-version-0-10-3-fails-due-to-openssl/17820\n",
            "301: Lower case with input ids - https://discuss.huggingface.co/t/lower-case-with-input-ids/18490\n",
            "302: Dialogue classification - https://discuss.huggingface.co/t/dialogue-classification/18466\n",
            "303: Multilang bert vs translating to english - https://discuss.huggingface.co/t/multilang-bert-vs-translating-to-english/18454\n",
            "304: pyo3_runtime.PanicException: likelihood is NAN. Input sentence may be too long - https://discuss.huggingface.co/t/pyo3-runtime-panicexception-likelihood-is-nan-input-sentence-may-be-too-long/18398\n",
            "305: Pytorch_model.bin not working because of lfs - https://discuss.huggingface.co/t/pytorch-model-bin-not-working-because-of-lfs/18290\n",
            "306: Tokenizer ignores repeated whitespaces - https://discuss.huggingface.co/t/tokenizer-ignores-repeated-whitespaces/17864\n",
            "307: How truncation works when applying BERT tokenizer on the batch of sentence pairs in HuggingFace? - https://discuss.huggingface.co/t/how-truncation-works-when-applying-bert-tokenizer-on-the-batch-of-sentence-pairs-in-huggingface/17977\n",
            "308: How to save a tokenizer only consisting of added tokens - https://discuss.huggingface.co/t/how-to-save-a-tokenizer-only-consisting-of-added-tokens/17825\n",
            "309: Passing list of inputs to tokenize - https://discuss.huggingface.co/t/passing-list-of-inputs-to-tokenize/17499\n",
            "310: Issues with Data Collator and Tokenizing with NER Datasets - https://discuss.huggingface.co/t/issues-with-data-collator-and-tokenizing-with-ner-datasets/17489\n",
            "311: How to perform tokenization on an ONNX model in JS? - https://discuss.huggingface.co/t/how-to-perform-tokenization-on-an-onnx-model-in-js/17581\n",
            "312: Using the Tokenizers library in a Unity project - https://discuss.huggingface.co/t/using-the-tokenizers-library-in-a-unity-project/17533\n",
            "313: Further pre-training the tokenizer? - https://discuss.huggingface.co/t/further-pre-training-the-tokenizer/17360\n",
            "314: Error when doing tokenization - https://discuss.huggingface.co/t/error-when-doing-tokenization/17344\n",
            "315: How to decode with spaces? - https://discuss.huggingface.co/t/how-to-decode-with-spaces/17289\n",
            "316: Show Submodels of PegasusTokenizer - https://discuss.huggingface.co/t/show-submodels-of-pegasustokenizer/17286\n",
            "317: Load SentencePieceBPETokenizer in TF - https://discuss.huggingface.co/t/load-sentencepiecebpetokenizer-in-tf/17257\n",
            "318: Best way to mask a multi-token word when using `.*ForMaskedLM` models - https://discuss.huggingface.co/t/best-way-to-mask-a-multi-token-word-when-using-formaskedlm-models/6428\n",
            "319: Does a tokenizer keep the mapping between my labels to their encoding? - https://discuss.huggingface.co/t/does-a-tokenizer-keep-the-mapping-between-my-labels-to-their-encoding/16296\n",
            "320: What is Wav2Vec2FeatureExtractor doing? - https://discuss.huggingface.co/t/what-is-wav2vec2featureextractor-doing/16423\n",
            "321: What does this warning mean? -overflowing tokens are not returned for the setting you have chosen - https://discuss.huggingface.co/t/what-does-this-warning-mean-overflowing-tokens-are-not-returned-for-the-setting-you-have-chosen/11594\n",
            "322: How can I make sure Tokenizer pads to a fixed length? - https://discuss.huggingface.co/t/how-can-i-make-sure-tokenizer-pads-to-a-fixed-length/16116\n",
            "323: Issue with Decoding in HuggingFace - https://discuss.huggingface.co/t/issue-with-decoding-in-huggingface/15699\n",
            "324: ValueError: Unable to create tensor for 1 dataset but not the other of same type - https://discuss.huggingface.co/t/valueerror-unable-to-create-tensor-for-1-dataset-but-not-the-other-of-same-type/15995\n",
            "325: Disabling addition of CLS from BERT tokenizer - https://discuss.huggingface.co/t/disabling-addition-of-cls-from-bert-tokenizer/15538\n",
            "326: Tokenized sequence lengths - https://discuss.huggingface.co/t/tokenized-sequence-lengths/15117\n",
            "327: Finetuning GPT-J6B for custom dataset - https://discuss.huggingface.co/t/finetuning-gpt-j6b-for-custom-dataset/9924\n",
            "328: Training tokenizer takes too much RAM - https://discuss.huggingface.co/t/training-tokenizer-takes-too-much-ram/14256\n",
            "329: How to “further pretrain” a tokenizer (do I need to do so?) - https://discuss.huggingface.co/t/how-to-further-pretrain-a-tokenizer-do-i-need-to-do-so/14719\n",
            "330: Run_seq2seq_qa.py: Column 3 named labels expected length 1007 but got length 1000 - https://discuss.huggingface.co/t/run-seq2seq-qa-py-column-3-named-labels-expected-length-1007-but-got-length-1000/13237\n",
            "331: Issues with offset_mapping values - https://discuss.huggingface.co/t/issues-with-offset-mapping-values/4237\n",
            "332: How would you train a sentencepiece BPE tokenizer on this language with 400 “characters”? - https://discuss.huggingface.co/t/how-would-you-train-a-sentencepiece-bpe-tokenizer-on-this-language-with-400-characters/14669\n",
            "333: All my sequences get tokenized the same - https://discuss.huggingface.co/t/all-my-sequences-get-tokenized-the-same/14657\n",
            "334: Combine multiple sentences together during tokenization - https://discuss.huggingface.co/t/combine-multiple-sentences-together-during-tokenization/3430\n",
            "335: NER tag , aggregation stratergy - https://discuss.huggingface.co/t/ner-tag-aggregation-stratergy/14199\n",
            "336: How to ensure that tokenizers never truncate partial words? - https://discuss.huggingface.co/t/how-to-ensure-that-tokenizers-never-truncate-partial-words/14024\n",
            "337: How to ensure the `overflow` with `stride` always starts with a full word? - https://discuss.huggingface.co/t/how-to-ensure-the-overflow-with-stride-always-starts-with-a-full-word/14030\n",
            "338: Adding new tokens to a BERT tokenizer - Getting ValueError - https://discuss.huggingface.co/t/adding-new-tokens-to-a-bert-tokenizer-getting-valueerror/9253\n",
            "339: Adding token to t5-base vocab does not respect space - https://discuss.huggingface.co/t/adding-token-to-t5-base-vocab-does-not-respect-space/13662\n",
            "340: How can I change the token id of a special token? - https://discuss.huggingface.co/t/how-can-i-change-the-token-id-of-a-special-token/13445\n",
            "341: Import distilbert-base-uncased tokenizer to an android app along with the tflite model - https://discuss.huggingface.co/t/import-distilbert-base-uncased-tokenizer-to-an-android-app-along-with-the-tflite-model/3234\n",
            "342: What are the equivalent manner for using texts_to_sequences? - https://discuss.huggingface.co/t/what-are-the-equivalent-manner-for-using-texts-to-sequences/13220\n",
            "343: ERROR?why encoding [MASK] before ‘.’ would gain a idx 13? - https://discuss.huggingface.co/t/error-why-encoding-mask-before-would-gain-a-idx-13/2897\n",
            "344: LongFormer tokenizer has the same token_type_ids for sequence pairs - https://discuss.huggingface.co/t/longformer-tokenizer-has-the-same-token-type-ids-for-sequence-pairs/12992\n",
            "345: Batch encode plus in Rust Tokenizers - https://discuss.huggingface.co/t/batch-encode-plus-in-rust-tokenizers/12722\n",
            "346: Best solution for train tokenizer and MLM from scratch - https://discuss.huggingface.co/t/best-solution-for-train-tokenizer-and-mlm-from-scratch/12579\n",
            "347: Implementing custom tokenizer components (normalizers, processors) - https://discuss.huggingface.co/t/implementing-custom-tokenizer-components-normalizers-processors/12371\n",
            "348: Does T5Tokenizer support the Greek language? - https://discuss.huggingface.co/t/does-t5tokenizer-support-the-greek-language/12224\n",
            "349: How padding in huggingface tokenizer works? - https://discuss.huggingface.co/t/how-padding-in-huggingface-tokenizer-works/12161\n",
            "350: Why we need to add special tokens to tasks other than classification? - https://discuss.huggingface.co/t/why-we-need-to-add-special-tokens-to-tasks-other-than-classification/11975\n",
            "351: How to configure TokenizerFast for AutoTokenizer - https://discuss.huggingface.co/t/how-to-configure-tokenizerfast-for-autotokenizer/11353\n",
            "352: How to employ different vocabs for encoder and decoder respectively? - https://discuss.huggingface.co/t/how-to-employ-different-vocabs-for-encoder-and-decoder-respectively/11497\n",
            "353: How to use tokenizer.tokenize in Chinese data properly? - https://discuss.huggingface.co/t/how-to-use-tokenizer-tokenize-in-chinese-data-properly/11491\n",
            "354: Mask only specific words - https://discuss.huggingface.co/t/mask-only-specific-words/173\n",
            "355: Load custom pretrained tokenizer - https://discuss.huggingface.co/t/load-custom-pretrained-tokenizer/11148\n",
            "356: Using Custom Vocab.txt - https://discuss.huggingface.co/t/using-custom-vocab-txt/10847\n",
            "357: Tokenizer.encode not returning encodings - https://discuss.huggingface.co/t/tokenizer-encode-not-returning-encodings/10616\n",
            "358: There is no 0.11.0 tokenizers in pip - https://discuss.huggingface.co/t/there-is-no-0-11-0-tokenizers-in-pip/10381\n",
            "359: Performance difference between ByteLevelBPE and Wordpiece tokenizers - https://discuss.huggingface.co/t/performance-difference-between-bytelevelbpe-and-wordpiece-tokenizers/10203\n",
            "360: Should have a `model_type` key in its config.json - https://discuss.huggingface.co/t/should-have-a-model-type-key-in-its-config-json/10144\n",
            "361: Using a fixed vocab.txt with AutoTokenizer? - https://discuss.huggingface.co/t/using-a-fixed-vocab-txt-with-autotokenizer/9919\n",
            "362: Train wordpiece from scratch - https://discuss.huggingface.co/t/train-wordpiece-from-scratch/9843\n",
            "363: I set up a different batch_size, but the time of data processing has not changed - https://discuss.huggingface.co/t/i-set-up-a-different-batch-size-but-the-time-of-data-processing-has-not-changed/9668\n",
            "364: Cannot create an identical PretrainedTokenizerFast object from a Tokenizer created by tokenizers library - https://discuss.huggingface.co/t/cannot-create-an-identical-pretrainedtokenizerfast-object-from-a-tokenizer-created-by-tokenizers-library/9317\n",
            "365: Index of wordpieces (subwords) after tokenization by transformers - https://discuss.huggingface.co/t/index-of-wordpieces-subwords-after-tokenization-by-transformers/9552\n",
            "366: A problem about FutureWarning？ - https://discuss.huggingface.co/t/a-problem-about-futurewarning/9323\n",
            "367: Extracting embedding values of NLP pertained models from tokenized strings - https://discuss.huggingface.co/t/extracting-embedding-values-of-nlp-pertained-models-from-tokenized-strings/9287\n",
            "368: Tokenization in a NER context - https://discuss.huggingface.co/t/tokenization-in-a-ner-context/5635\n",
            "369: Unable to convert output to interpretable format - https://discuss.huggingface.co/t/unable-to-convert-output-to-interpretable-format/8882\n",
            "370: BpeTrainer implementation in Python - https://discuss.huggingface.co/t/bpetrainer-implementation-in-python/8625\n",
            "371: MBart50Tokenizer vs XLMRobertaTokenizer - https://discuss.huggingface.co/t/mbart50tokenizer-vs-xlmrobertatokenizer/8498\n",
            "372: Why multilingual BERT tokenizer doesn’t remove accent markers? - https://discuss.huggingface.co/t/why-multilingual-bert-tokenizer-doesnt-remove-accent-markers/8468\n",
            "373: TypeError when loading tokenizer with from_pretrained method for bart-large-mnli model - https://discuss.huggingface.co/t/typeerror-when-loading-tokenizer-with-from-pretrained-method-for-bart-large-mnli-model/3378\n",
            "374: Is it okay to split ids sequence when it is encoded using Byte-level BPE - https://discuss.huggingface.co/t/is-it-okay-to-split-ids-sequence-when-it-is-encoded-using-byte-level-bpe/8129\n",
            "375: Using truncated fragments as input samples in training - https://discuss.huggingface.co/t/using-truncated-fragments-as-input-samples-in-training/6978\n",
            "376: Using whitespace tokenizer for training models - https://discuss.huggingface.co/t/using-whitespace-tokenizer-for-training-models/6591\n",
            "377: Save custom components - https://discuss.huggingface.co/t/save-custom-components/6458\n",
            "378: How to see contents of a normalizer - https://discuss.huggingface.co/t/how-to-see-contents-of-a-normalizer/6045\n",
            "379: Newbie: Main difference between tokenizers? - https://discuss.huggingface.co/t/newbie-main-difference-between-tokenizers/6035\n",
            "380: Can’t load tokenizer for ‘sshleifer/student_blarge_12_3’ - https://discuss.huggingface.co/t/cant-load-tokenizer-for-sshleifer-student-blarge-12-3/6027\n",
            "381: How to create a Huggingface tokenizer from a non-Huggingface tokenizer? - https://discuss.huggingface.co/t/how-to-create-a-huggingface-tokenizer-from-a-non-huggingface-tokenizer/5983\n",
            "382: Add new tokens and learn the embeddings of the new tokens and keeping all the other parametes frozen - https://discuss.huggingface.co/t/add-new-tokens-and-learn-the-embeddings-of-the-new-tokens-and-keeping-all-the-other-parametes-frozen/5896\n",
            "383: How do you use SentencePiece for BPE of sequences with no whitespace - https://discuss.huggingface.co/t/how-do-you-use-sentencepiece-for-bpe-of-sequences-with-no-whitespace/1895\n",
            "384: BOS tokens for mBERT tokenizer - https://discuss.huggingface.co/t/bos-tokens-for-mbert-tokenizer/5467\n",
            "385: BertTokenizerFast for stsb-xlm-r-multilingual model - https://discuss.huggingface.co/t/berttokenizerfast-for-stsb-xlm-r-multilingual-model/4742\n",
            "386: Skip-gram tokens - https://discuss.huggingface.co/t/skip-gram-tokens/5294\n",
            "387: Using a BertWordPieceTokenizer trained from scratch from transformers - https://discuss.huggingface.co/t/using-a-bertwordpiecetokenizer-trained-from-scratch-from-transformers/4391\n",
            "388: Questions on model’s tokens - https://discuss.huggingface.co/t/questions-on-models-tokens/5023\n",
            "389: Space token ’ ’ cannot be add when is_split_into_words = True - https://discuss.huggingface.co/t/space-token-cannot-be-add-when-is-split-into-words-true/4305\n",
            "390: Are special_tokens the only tokens guaranteed to be atomic? - https://discuss.huggingface.co/t/are-special-tokens-the-only-tokens-guaranteed-to-be-atomic/4124\n",
            "391: Does AutoTokenizer.from_pretrained add [cls] tokens? - https://discuss.huggingface.co/t/does-autotokenizer-from-pretrained-add-cls-tokens/4056\n",
            "392: BertTokenizer’s encode_plus returns 2d tensor when printing ‘input_ids’/ ‘attention_mask’ - https://discuss.huggingface.co/t/berttokenizers-encode-plus-returns-2d-tensor-when-printing-input-ids-attention-mask/3530\n",
            "393: Tunning tokenizer on my own dataset - https://discuss.huggingface.co/t/tunning-tokenizer-on-my-own-dataset/3367\n",
            "394: Why Bert-chinese use do_lower_case=False? - https://discuss.huggingface.co/t/why-bert-chinese-use-do-lower-case-false/2952\n",
            "395: Bug with tokernizer’s offset mapping for NER problems? - https://discuss.huggingface.co/t/bug-with-tokernizers-offset-mapping-for-ner-problems/2928\n",
            "396: BERT WordPiece Tokenizer: some matras missing after tokenization for Hindi Language #572 - https://discuss.huggingface.co/t/bert-wordpiece-tokenizer-some-matras-missing-after-tokenization-for-hindi-language-572/2936\n",
            "397: Error with <|endoftext|> in Tokenizer GPT2 - https://discuss.huggingface.co/t/error-with-endoftext-in-tokenizer-gpt2/2838\n",
            "398: Build a RoBERTa tokenizer from scratch - https://discuss.huggingface.co/t/build-a-roberta-tokenizer-from-scratch/2758\n",
            "399: Couldn’t instantiate the backend tokenizer - https://discuss.huggingface.co/t/couldnt-instantiate-the-backend-tokenizer/2662\n",
            "400: Bypassing tokenizers - https://discuss.huggingface.co/t/bypassing-tokenizers/2162\n",
            "401: Tokenizing Domain Specific Text - https://discuss.huggingface.co/t/tokenizing-domain-specific-text/1978\n",
            "402: Issue with tokenizer.tokenize - https://discuss.huggingface.co/t/issue-with-tokenizer-tokenize/1891\n",
            "403: Tokenizer taking extremely long time to train - https://discuss.huggingface.co/t/tokenizer-taking-extremely-long-time-to-train/1875\n",
            "404: Where to find the “wiki-big.train.raw” data as mentioned in the snippet for tokenizers 0.9? - https://discuss.huggingface.co/t/where-to-find-the-wiki-big-train-raw-data-as-mentioned-in-the-snippet-for-tokenizers-0-9/1796\n",
            "405: Change bpe-dropout value on the fly? - https://discuss.huggingface.co/t/change-bpe-dropout-value-on-the-fly/1721\n",
            "406: Loading pretrained SentencePiece tokenizer from Fairseq - https://discuss.huggingface.co/t/loading-pretrained-sentencepiece-tokenizer-from-fairseq/1326\n",
            "407: What does `tokenizers.normalizer.normalize` do? - https://discuss.huggingface.co/t/what-does-tokenizers-normalizer-normalize-do/1463\n",
            "408: Automatic sentence segmentation and encoding - https://discuss.huggingface.co/t/automatic-sentence-segmentation-and-encoding/1479\n",
            "409: How to truncate from the head in AutoTokenizer? - https://discuss.huggingface.co/t/how-to-truncate-from-the-head-in-autotokenizer/676\n",
            "410: How much memory is needed for training ByteLevelBPETokenizer? - https://discuss.huggingface.co/t/how-much-memory-is-needed-for-training-bytelevelbpetokenizer/1165\n",
            "411: How to make tokenizer convert subword token to an independent token? - https://discuss.huggingface.co/t/how-to-make-tokenizer-convert-subword-token-to-an-independent-token/1015\n",
            "412: Using a pretrained tokenizer vs training a one from scratch - https://discuss.huggingface.co/t/using-a-pretrained-tokenizer-vs-training-a-one-from-scratch/783\n",
            "413: Masking Probability - https://discuss.huggingface.co/t/masking-probability/746\n",
            "414: Tokenizer not found - https://discuss.huggingface.co/t/tokenizer-not-found/757\n",
            "415: Add new tokens for subwords - https://discuss.huggingface.co/t/add-new-tokens-for-subwords/489\n",
            "416: Token alignment for word-level tasks - https://discuss.huggingface.co/t/token-alignment-for-word-level-tasks/577\n",
            "417: ByteLevelBPETokenizer inconsistent behavior - https://discuss.huggingface.co/t/bytelevelbpetokenizer-inconsistent-behavior/442\n",
            "418: Use a pretrained ByteLevelBPETokenizer on text - https://discuss.huggingface.co/t/use-a-pretrained-bytelevelbpetokenizer-on-text/348\n",
            "419: Continuation token in pertained tokenizer bert-base-chinese - https://discuss.huggingface.co/t/continuation-token-in-pertained-tokenizer-bert-base-chinese/218\n",
            "420: Tokenizers v0.8.0 is out! - https://discuss.huggingface.co/t/tokenizers-v0-8-0-is-out/51\n",
            "421: About the Tokenizers category - https://discuss.huggingface.co/t/about-the-tokenizers-category/30\n"
          ]
        }
      ],
      "source": [
        "# Set up Chrome options for Colab\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "import time\n",
        "\n",
        "# Set up Chrome options to use headless mode (for Colab)\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "\n",
        "# Add extra options\n",
        "chrome_options.add_argument(\"--window-size=1920,1080\")  # Set the window size\n",
        "chrome_options.add_argument(\"--disable-infobars\")  # Disable the infobars\n",
        "chrome_options.add_argument(\"--disable-popup-blocking\")  # Disable pop-ups\n",
        "chrome_options.add_argument(\"--ignore-certificate-errors\")  # Ignore certificate errors\n",
        "chrome_options.add_argument(\"--incognito\")  # Use Chrome in incognito mode\n",
        "\n",
        "# Set the path to chromedriver explicitly (installed by apt)\n",
        "chrome_path = \"/usr/bin/chromedriver\"\n",
        "\n",
        "# Initialize the WebDriver with the updated path\n",
        "driver = gs.Chrome(options=chrome_options)\n",
        "\n",
        "# Open the specific forum page\n",
        "url = \"https://discuss.huggingface.co/c/tokenizers/11\"  # Replace with your desired URL\n",
        "driver.get(url)\n",
        "\n",
        "# Wait for the page to load\n",
        "time.sleep(5)\n",
        "\n",
        "# Base URL of the site (to handle relative links)\n",
        "#base_url = \"https://discuss.huggingface.co\"\n",
        "\n",
        "# Function to scrape titles and links, ensuring no duplicates\n",
        "def scrape_titles_and_links():\n",
        "    titles_and_links = []\n",
        "    seen_titles_and_links = set()  # Create a set to track seen (title, link) pairs\n",
        "\n",
        "    while True:\n",
        "        # Find all links inside span.link-top-line\n",
        "        elements = driver.find_elements(By.CSS_SELECTOR, \"span.link-top-line a.title.raw-link.raw-topic-link\")\n",
        "\n",
        "        for elem in elements:\n",
        "            title = elem.text.strip()  # Get the title text\n",
        "            relative_link = elem.get_attribute('href')  # Get the relative URL from the href attribute\n",
        "\n",
        "            # Construct the absolute URL by appending the relative path to the base URL\n",
        "            full_link = relative_link\n",
        "            #full_link = base_url + relative_link\n",
        "\n",
        "            # Check if the (title, full_link) pair is already in the set of seen titles and links\n",
        "            if (title, full_link) not in seen_titles_and_links:\n",
        "                titles_and_links.append((title, full_link))  # Add the unique pair to the list\n",
        "                seen_titles_and_links.add((title, full_link))  # Add the pair to the set to track it\n",
        "\n",
        "        # Scroll down to load more content (if the forum uses infinite scroll)\n",
        "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
        "        time.sleep(3)  # Adjust based on loading speed\n",
        "\n",
        "        # Break the loop if no new titles are loaded\n",
        "        if len(elements) == len(driver.find_elements(By.CSS_SELECTOR, \"span.link-top-line a.title.raw-link.raw-topic-link\")):\n",
        "            break\n",
        "\n",
        "    return titles_and_links\n",
        "\n",
        "# Scrape and print the titles and their links\n",
        "titles_and_links = scrape_titles_and_links()\n",
        "print(\"Scraped Titles and Links:\")\n",
        "for i, (title, link) in enumerate(titles_and_links, 1):\n",
        "    print(f\"{i}: {title} - {link}\")\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BiQApObeYaMq",
        "outputId": "2672e011-ace1-4b3c-f385-124b47604582"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"81581e1d-b7ac-4cdd-adb9-20dd333771de-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"81581e1d-b7ac-4cdd-adb9-20dd333771de-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"81581e1d-b7ac-4cdd-adb9-20dd333771de-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"81581e1d-b7ac-4cdd-adb9-20dd333771de-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraped Titles and Links:\n",
            "1: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "2: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "3: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "4: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "5: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "6: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "7: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "8: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "9: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "10: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "11: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "12: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "13: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "14: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "15: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "16: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "17: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "18: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "19: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "20: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "21: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "22: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "23: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "24: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "25: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "26: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "27: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "28: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "29: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "30: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "31: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "32: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "33: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "34: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "35: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "36: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "37: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "38: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "39: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "40: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "41: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "42: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "43: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "44: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "45: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "46: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "47: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "48: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "49: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "50: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "51: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "52: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "53: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "54: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "55: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "56: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "57: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "58: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "59: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "60: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "61: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "62: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "63: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "64: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "65: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "66: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "67: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "68: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "69: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "70: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "71: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "72: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "73: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "74: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "75: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "76: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "77: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "78: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "79: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "80: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "81: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "82: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "83: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "84: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "85: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "86: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "87: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "88: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "89: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "90: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "91: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "92: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "93: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "94: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "95: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "96: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "97: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "98: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "99: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "100: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "101: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "102: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "103: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "104: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "105: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "106: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "107: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "108: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "109: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "110: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "111: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "112: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "113: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "114: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "115: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "116: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "117: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "118: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "119: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "120: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "121: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "122: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "123: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "124: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "125: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "126: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "127: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "128: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "129: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "130: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "131: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "132: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "133: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "134: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "135: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "136: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "137: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "138: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "139: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "140: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "141: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "142: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "143: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "144: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "145: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "146: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "147: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "148: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "149: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "150: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "151: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "152: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "153: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "154: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "155: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "156: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "157: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "158: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "159: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "160: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "161: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "162: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "163: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "164: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "165: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "166: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "167: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "168: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "169: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "170: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "171: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "172: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "173: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "174: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "175: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "176: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "177: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "178: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "179: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "180: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "181: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "182: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "183: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "184: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "185: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "186: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "187: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "188: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "189: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "190: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "191: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "192: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "193: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "194: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "195: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "196: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "197: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "198: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "199: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "200: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "201: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "202: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "203: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "204: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "205: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "206: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "207: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "208: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "209: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "210: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "211: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "212: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "213: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "214: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "215: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "216: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "217: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "218: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "219: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "220: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "221: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "222: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "223: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "224: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "225: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "226: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "227: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "228: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "229: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "230: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "231: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "232: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "233: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "234: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "235: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "236: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "237: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "238: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "239: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "240: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "241: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "242: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "243: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "244: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "245: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "246: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "247: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "248: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "249: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "250: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "251: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "252: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "253: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "254: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "255: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "256: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "257: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "258: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "259: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "260: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "261: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "262: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "263: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "264: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "265: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "266: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "267: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "268: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "269: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "270: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "271: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "272: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "273: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "274: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "275: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "276: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "277: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "278: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "279: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "280: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "281: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "282: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "283: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "284: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "285: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "286: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "287: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "288: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "289: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "290: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "291: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "292: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "293: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "294: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "295: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "296: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "297: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "298: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "299: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "300: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "301: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "302: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "303: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "304: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "305: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "306: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "307: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "308: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "309: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "310: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "311: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "312: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "313: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "314: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "315: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "316: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "317: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "318: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "319: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "320: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "321: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "322: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "323: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "324: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "325: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "326: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "327: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "328: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "329: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "330: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "331: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "332: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "333: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "334: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "335: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "336: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "337: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "338: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "339: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "340: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "341: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "342: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "343: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "344: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "345: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "346: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "347: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "348: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "349: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "350: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "351: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "352: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "353: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "354: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "355: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "356: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "357: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "358: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "359: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "360: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "361: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "362: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "363: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "364: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "365: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "366: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "367: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "368: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "369: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "370: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "371: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "372: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "373: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "374: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "375: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "376: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "377: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "378: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "379: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "380: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "381: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "382: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "383: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "384: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "385: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "386: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "387: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "388: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "389: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "390: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "391: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "392: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "393: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "394: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "395: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "396: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "397: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "398: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "399: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "400: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "401: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "402: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "403: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "404: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "405: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "406: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "407: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "408: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "409: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "410: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "411: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "412: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "413: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "414: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "415: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "416: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "417: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "418: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "419: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "420: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "421: BertTokenizer.decode not understanding new vocabulary - https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "422: Tokenizer tend to choose added tokens first rather than token in vocab - https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "423: Special token printed out as output - https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948\n",
            "424: [NER][Japanese] labeled segment shorter than token - https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "425: T5Tokenizer add a whitespace token after added special tokens - https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "426: Unable to register my own tokenizer - https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "427: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab! - https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "428: Use Unicode blocks in regex (in Replace normalizer) - https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904\n",
            "429: How to handle translations one source language to many target sentences for the same language - https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "430: NER Label tokenization with overflowing tokens - https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561\n",
            "431: How to use a trained tokenizer for semantic search? - https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "432: Unk_token not set after training a BPETokenizer tokenizer - https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733\n",
            "433: AutoTokenizer is very slow when loading llama tokenizer - https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547\n",
            "434: Offset mappings differ for tokenizers - https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424\n",
            "435: The process for tokenizing concatenated dataset is slow st the end of tokenizing - https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "436: Batch tokenize (split into tokens, without processing) - https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238\n",
            "437: Does AutoTokenizer uploads data to HuggingFace - https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "438: RobertaTokenizer decode and tokenize do not have the same output - https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "439: Training tokenizers with padding in between tokens - https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "440: Leaving unknown words untokenized like in OpenMNT - https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "441: Keeping special chars in translations - https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270\n",
            "442: Should cls_token be [CLS] or <cls>? - https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "443: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False - https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "444: I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error - https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742\n",
            "445: OSError: Can’t load tokenizer for ‘facebook/xmod-base’ - https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471\n",
            "446: `GPT2Tokenizer` Tokenizer handling `\\n\\n` differently in different settings - https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289\n",
            "447: DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‘padding=True’ ‘truncation=True’ to have batched tensors with the same length - https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625\n",
            "448: Decode token IDs into a list (not a single string) - https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991\n",
            "449: Error training MLM with Roberta Tokenizer - https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878\n",
            "450: Are the slow and fast tokenizer results the same output for the same input? - https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "451: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "452: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "453: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "454: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "455: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "456: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "457: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "458: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "459: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "460: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "461: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "462: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "463: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "464: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "465: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "466: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "467: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "468: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "469: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "470: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "471: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "472: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "473: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "474: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "475: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "476: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "477: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "478: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "479: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "480: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "481: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "482: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "483: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "484: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "485: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "486: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "487: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "488: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "489: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "490: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "491: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "492: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "493: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "494: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "495: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "496: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "497: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "498: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "499: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "500: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "501: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "502: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "503: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "504: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "505: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "506: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "507: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "508: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "509: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "510: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "511: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "512: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "513: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "514: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "515: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "516: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "517: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "518: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "519: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "520: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "521: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "522: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "523: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "524: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "525: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "526: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "527: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "528: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "529: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "530: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "531: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "532: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "533: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "534: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "535: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "536: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "537: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "538: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "539: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "540: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "541: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "542: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "543: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "544: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "545: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "546: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "547: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "548: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "549: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "550: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "551: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "552: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "553: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "554: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "555: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "556: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "557: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "558: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "559: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "560: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "561: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "562: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "563: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "564: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "565: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "566: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "567: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "568: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "569: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "570: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "571: BertTokenizer.decode not understanding new vocabulary - https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "572: Tokenizer tend to choose added tokens first rather than token in vocab - https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "573: Special token printed out as output - https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948\n",
            "574: [NER][Japanese] labeled segment shorter than token - https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "575: T5Tokenizer add a whitespace token after added special tokens - https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "576: Unable to register my own tokenizer - https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "577: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab! - https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "578: Use Unicode blocks in regex (in Replace normalizer) - https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904\n",
            "579: How to handle translations one source language to many target sentences for the same language - https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "580: NER Label tokenization with overflowing tokens - https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561\n",
            "581: How to use a trained tokenizer for semantic search? - https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "582: Unk_token not set after training a BPETokenizer tokenizer - https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733\n",
            "583: AutoTokenizer is very slow when loading llama tokenizer - https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547\n",
            "584: Offset mappings differ for tokenizers - https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424\n",
            "585: The process for tokenizing concatenated dataset is slow st the end of tokenizing - https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "586: Batch tokenize (split into tokens, without processing) - https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238\n",
            "587: Does AutoTokenizer uploads data to HuggingFace - https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "588: RobertaTokenizer decode and tokenize do not have the same output - https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "589: Training tokenizers with padding in between tokens - https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "590: Leaving unknown words untokenized like in OpenMNT - https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "591: Keeping special chars in translations - https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270\n",
            "592: Should cls_token be [CLS] or <cls>? - https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "593: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False - https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "594: I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error - https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742\n",
            "595: OSError: Can’t load tokenizer for ‘facebook/xmod-base’ - https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471\n",
            "596: `GPT2Tokenizer` Tokenizer handling `\\n\\n` differently in different settings - https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289\n",
            "597: DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‘padding=True’ ‘truncation=True’ to have batched tensors with the same length - https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625\n",
            "598: Decode token IDs into a list (not a single string) - https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991\n",
            "599: Error training MLM with Roberta Tokenizer - https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878\n",
            "600: Are the slow and fast tokenizer results the same output for the same input? - https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "601: “Add_tokens” breaks words when encoding - https://discuss.huggingface.co/t/add-tokens-breaks-words-when-encoding/21154\n",
            "602: 2 tokens for one character in T5 - https://discuss.huggingface.co/t/2-tokens-for-one-character-in-t5/14960\n",
            "603: OSError: Model name ‘gpt2’ was not found in tokenizers model name list (gpt2,…) - https://discuss.huggingface.co/t/oserror-model-name-gpt2-was-not-found-in-tokenizers-model-name-list-gpt2/2164\n",
            "604: DNA long sequence tokenization - https://discuss.huggingface.co/t/dna-long-sequence-tokenization/16154\n",
            "605: SentencePiece tokenizer encodes to unknown token - https://discuss.huggingface.co/t/sentencepiece-tokenizer-encodes-to-unknown-token/49113\n",
            "606: Tokenizer behaviour with pipeline - https://discuss.huggingface.co/t/tokenizer-behaviour-with-pipeline/48969\n",
            "607: Load tokenizer from file : Exception: data did not match any variant of untagged enum ModelWrapper - https://discuss.huggingface.co/t/load-tokenizer-from-file-exception-data-did-not-match-any-variant-of-untagged-enum-modelwrapper/25325\n",
            "608: ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076 - https://discuss.huggingface.co/t/arrowinvalid-column-3-named-attention-mask-expected-length-1000-but-got-length-1076/6904\n",
            "609: Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model - https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829\n",
            "610: Initialize Vocabulary for Unigram Tokenizer - https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371\n",
            "611: Make correct padding for text generation with GPT-NEO - https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800\n",
            "612: How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers? - https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639\n",
            "613: Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF - https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602\n",
            "614: Writing custom tokenizer and wrapping it in tokenizer object - https://discuss.huggingface.co/t/writing-custom-tokenizer-and-wrapping-it-in-tokenizer-object/39679\n",
            "615: Tokenizer for German lang - https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222\n",
            "616: Chunk tokens into desired chunk length without simply getting rid of rest of tokens - https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399\n",
            "617: Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers - https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872\n",
            "618: LlamaTokenizerFast returns token_type_ids but the forward pass of the LlamaModel does not receive token_type_ids - https://discuss.huggingface.co/t/llamatokenizerfast-returns-token-type-ids-but-the-forward-pass-of-the-llamamodel-does-not-receive-token-type-ids/42431\n",
            "619: GPT2Tokenizer not working in Kaggle Notebook - https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508\n",
            "620: Exploring the Majestic Temples in Karnataka - https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952\n",
            "621: Tokenizer producing token index greater than size of the dictionary - https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989\n",
            "622: How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer - https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843\n",
            "623: How to create a HF tokenizer’s vocab file from a BPE model’s merges.txt file? - https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737\n",
            "624: Scala/JVM Bindings for Tokenizers - https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393\n",
            "625: Tokenizers Wheel Takes Forever to Build - https://discuss.huggingface.co/t/tokenizers-wheel-takes-forever-to-build/17114\n",
            "626: Where the introduction of tokenizers.implementations? - https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959\n",
            "627: How to return custom `token_type_ids` or other values from a tokenizer? - https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525\n",
            "628: Easy way to compare tokenizers - https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347\n",
            "629: Unable to load image using llama-index - https://discuss.huggingface.co/t/unable-to-load-image-using-llama-index/38304\n",
            "630: Help defining tokenizer - https://discuss.huggingface.co/t/help-defining-tokenizer/38091\n",
            "631: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "632: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "633: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "634: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "635: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "636: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "637: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "638: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "639: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "640: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "641: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "642: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "643: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "644: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "645: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "646: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "647: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "648: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "649: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "650: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "651: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "652: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "653: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "654: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "655: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "656: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "657: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "658: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "659: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "660: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "661: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "662: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "663: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "664: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "665: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "666: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "667: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "668: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "669: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "670: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "671: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "672: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "673: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "674: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "675: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "676: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "677: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "678: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "679: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "680: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "681: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "682: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "683: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "684: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "685: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "686: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "687: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "688: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "689: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "690: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "691: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "692: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "693: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "694: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "695: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "696: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "697: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "698: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "699: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "700: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "701: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "702: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "703: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "704: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "705: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "706: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "707: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "708: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "709: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "710: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "711: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "712: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "713: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "714: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "715: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "716: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "717: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "718: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "719: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "720: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "721: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "722: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "723: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "724: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "725: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "726: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "727: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "728: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "729: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "730: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "731: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "732: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "733: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "734: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "735: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "736: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "737: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "738: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "739: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "740: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "741: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "742: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "743: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "744: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "745: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "746: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "747: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "748: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "749: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "750: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "751: BertTokenizer.decode not understanding new vocabulary - https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "752: Tokenizer tend to choose added tokens first rather than token in vocab - https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "753: Special token printed out as output - https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948\n",
            "754: [NER][Japanese] labeled segment shorter than token - https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "755: T5Tokenizer add a whitespace token after added special tokens - https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "756: Unable to register my own tokenizer - https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "757: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab! - https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "758: Use Unicode blocks in regex (in Replace normalizer) - https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904\n",
            "759: How to handle translations one source language to many target sentences for the same language - https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "760: NER Label tokenization with overflowing tokens - https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561\n",
            "761: How to use a trained tokenizer for semantic search? - https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "762: Unk_token not set after training a BPETokenizer tokenizer - https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733\n",
            "763: AutoTokenizer is very slow when loading llama tokenizer - https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547\n",
            "764: Offset mappings differ for tokenizers - https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424\n",
            "765: The process for tokenizing concatenated dataset is slow st the end of tokenizing - https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "766: Batch tokenize (split into tokens, without processing) - https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238\n",
            "767: Does AutoTokenizer uploads data to HuggingFace - https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "768: RobertaTokenizer decode and tokenize do not have the same output - https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "769: Training tokenizers with padding in between tokens - https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "770: Leaving unknown words untokenized like in OpenMNT - https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "771: Keeping special chars in translations - https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270\n",
            "772: Should cls_token be [CLS] or <cls>? - https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "773: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False - https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "774: I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error - https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742\n",
            "775: OSError: Can’t load tokenizer for ‘facebook/xmod-base’ - https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471\n",
            "776: `GPT2Tokenizer` Tokenizer handling `\\n\\n` differently in different settings - https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289\n",
            "777: DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‘padding=True’ ‘truncation=True’ to have batched tensors with the same length - https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625\n",
            "778: Decode token IDs into a list (not a single string) - https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991\n",
            "779: Error training MLM with Roberta Tokenizer - https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878\n",
            "780: Are the slow and fast tokenizer results the same output for the same input? - https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "781: “Add_tokens” breaks words when encoding - https://discuss.huggingface.co/t/add-tokens-breaks-words-when-encoding/21154\n",
            "782: 2 tokens for one character in T5 - https://discuss.huggingface.co/t/2-tokens-for-one-character-in-t5/14960\n",
            "783: OSError: Model name ‘gpt2’ was not found in tokenizers model name list (gpt2,…) - https://discuss.huggingface.co/t/oserror-model-name-gpt2-was-not-found-in-tokenizers-model-name-list-gpt2/2164\n",
            "784: DNA long sequence tokenization - https://discuss.huggingface.co/t/dna-long-sequence-tokenization/16154\n",
            "785: SentencePiece tokenizer encodes to unknown token - https://discuss.huggingface.co/t/sentencepiece-tokenizer-encodes-to-unknown-token/49113\n",
            "786: Tokenizer behaviour with pipeline - https://discuss.huggingface.co/t/tokenizer-behaviour-with-pipeline/48969\n",
            "787: Load tokenizer from file : Exception: data did not match any variant of untagged enum ModelWrapper - https://discuss.huggingface.co/t/load-tokenizer-from-file-exception-data-did-not-match-any-variant-of-untagged-enum-modelwrapper/25325\n",
            "788: ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076 - https://discuss.huggingface.co/t/arrowinvalid-column-3-named-attention-mask-expected-length-1000-but-got-length-1076/6904\n",
            "789: Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model - https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829\n",
            "790: Initialize Vocabulary for Unigram Tokenizer - https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371\n",
            "791: Make correct padding for text generation with GPT-NEO - https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800\n",
            "792: How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers? - https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639\n",
            "793: Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF - https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602\n",
            "794: Writing custom tokenizer and wrapping it in tokenizer object - https://discuss.huggingface.co/t/writing-custom-tokenizer-and-wrapping-it-in-tokenizer-object/39679\n",
            "795: Tokenizer for German lang - https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222\n",
            "796: Chunk tokens into desired chunk length without simply getting rid of rest of tokens - https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399\n",
            "797: Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers - https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872\n",
            "798: LlamaTokenizerFast returns token_type_ids but the forward pass of the LlamaModel does not receive token_type_ids - https://discuss.huggingface.co/t/llamatokenizerfast-returns-token-type-ids-but-the-forward-pass-of-the-llamamodel-does-not-receive-token-type-ids/42431\n",
            "799: GPT2Tokenizer not working in Kaggle Notebook - https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508\n",
            "800: Exploring the Majestic Temples in Karnataka - https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952\n",
            "801: Tokenizer producing token index greater than size of the dictionary - https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989\n",
            "802: How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer - https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843\n",
            "803: How to create a HF tokenizer’s vocab file from a BPE model’s merges.txt file? - https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737\n",
            "804: Scala/JVM Bindings for Tokenizers - https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393\n",
            "805: Tokenizers Wheel Takes Forever to Build - https://discuss.huggingface.co/t/tokenizers-wheel-takes-forever-to-build/17114\n",
            "806: Where the introduction of tokenizers.implementations? - https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959\n",
            "807: How to return custom `token_type_ids` or other values from a tokenizer? - https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525\n",
            "808: Easy way to compare tokenizers - https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347\n",
            "809: Unable to load image using llama-index - https://discuss.huggingface.co/t/unable-to-load-image-using-llama-index/38304\n",
            "810: Help defining tokenizer - https://discuss.huggingface.co/t/help-defining-tokenizer/38091\n",
            "811: Token Offsets in Rust vs. Python - https://discuss.huggingface.co/t/token-offsets-in-rust-vs-python/37949\n",
            "812: “OSError: Model name ‘./XX’ was not found in tokenizers model name list” - cannot load custom tokenizer in Transformers - https://discuss.huggingface.co/t/oserror-model-name-xx-was-not-found-in-tokenizers-model-name-list-cannot-load-custom-tokenizer-in-transformers/2714\n",
            "813: Converting JSON/dict to flatten string with indicator tokens - https://discuss.huggingface.co/t/converting-json-dict-to-flatten-string-with-indicator-tokens/37387\n",
            "814: Train Retry Tokenizer - https://discuss.huggingface.co/t/train-retry-tokenizer/37031\n",
            "815: Pretokenise on punctuation except hyphens - https://discuss.huggingface.co/t/pretokenise-on-punctuation-except-hyphens/36691\n",
            "816: Tokenizer Trainer Crashing - https://discuss.huggingface.co/t/tokenizer-trainer-crashing/36645\n",
            "817: Tokenizer extremely slow when deployed to a container - https://discuss.huggingface.co/t/tokenizer-extremely-slow-when-deployed-to-a-container/36569\n",
            "818: Dealing with Decimal and Fractions - https://discuss.huggingface.co/t/dealing-with-decimal-and-fractions/23377\n",
            "819: `add_tokens` with argument `special_tokens=True` vs `add_special_tokens` - https://discuss.huggingface.co/t/add-tokens-with-argument-special-tokens-true-vs-add-special-tokens/35648\n",
            "820: Unable to upload custom Pytorch model in huggingface - https://discuss.huggingface.co/t/unable-to-upload-custom-pytorch-model-in-huggingface/35545\n",
            "821: RuntimeError: Cannot re-initialize CUDA in forked subprocess - https://discuss.huggingface.co/t/runtimeerror-cannot-re-initialize-cuda-in-forked-subprocess/28392\n",
            "822: Overflowing Tokens in MarkupLM - https://discuss.huggingface.co/t/overflowing-tokens-in-markuplm/35217\n",
            "823: I get the predicted token as ` े` . What am I doing wrong? - https://discuss.huggingface.co/t/i-get-the-predicted-token-as-what-am-i-doing-wrong/29038\n",
            "824: <unk> token in the output instead curly braces - https://discuss.huggingface.co/t/unk-token-in-the-output-instead-curly-braces/34653\n",
            "825: How to add a new token without expanding the vocabulary - https://discuss.huggingface.co/t/how-to-add-a-new-token-without-expanding-the-vocabulary/34536\n",
            "826: Does the ByteLevelBPETokenizer need to be wrapped in a normal Tokenizer? - https://discuss.huggingface.co/t/does-the-bytelevelbpetokenizer-need-to-be-wrapped-in-a-normal-tokenizer/34121\n",
            "827: What is required to create a fast tokenizer? For example for a Marian model - https://discuss.huggingface.co/t/what-is-required-to-create-a-fast-tokenizer-for-example-for-a-marian-model/33941\n",
            "828: GPT2Tokenizer.decode maps unicode sequences to the same string ‘�’ - https://discuss.huggingface.co/t/gpt2tokenizer-decode-maps-unicode-sequences-to-the-same-string/32453\n",
            "829: Issue with Tokenizer - https://discuss.huggingface.co/t/issue-with-tokenizer/33796\n",
            "830: Tokenizing my novel for GPT model - https://discuss.huggingface.co/t/tokenizing-my-novel-for-gpt-model/33532\n",
            "831: How to add additional custom pre-tokenization processing? - https://discuss.huggingface.co/t/how-to-add-additional-custom-pre-tokenization-processing/1637\n",
            "832: Customize FlauBERT tokenizer to split line breaks - https://discuss.huggingface.co/t/customize-flaubert-tokenizer-to-split-line-breaks/33011\n",
            "833: How to change the size of model_max_length? - https://discuss.huggingface.co/t/how-to-change-the-size-of-model-max-length/32942\n",
            "834: Can’t get to the source code of `tokenizer.convert_tokens_to_string` - https://discuss.huggingface.co/t/cant-get-to-the-source-code-of-tokenizer-convert-tokens-to-string/32714\n",
            "835: Why I’m getting same result with or without using Wav2Vec2Processor? - https://discuss.huggingface.co/t/why-im-getting-same-result-with-or-without-using-wav2vec2processor/32482\n",
            "836: How does `tokenizer().input_ids` work and how different it is from tokenizer.encode() before `model.generate()` and decoding step? - https://discuss.huggingface.co/t/how-does-tokenizer-input-ids-work-and-how-different-it-is-from-tokenizer-encode-before-model-generate-and-decoding-step/30476\n",
            "837: What file type should my training data be? - https://discuss.huggingface.co/t/what-file-type-should-my-training-data-be/32075\n",
            "838: Best way to get the closest token indices of input of char_to_token is a whitespace - https://discuss.huggingface.co/t/best-way-to-get-the-closest-token-indices-of-input-of-char-to-token-is-a-whitespace/32004\n",
            "839: Token indices sequence length is longer than the specified maximum sequence length - https://discuss.huggingface.co/t/token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length/25133\n",
            "840: Create a simple tokenizer - https://discuss.huggingface.co/t/create-a-simple-tokenizer/31677\n",
            "841: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "842: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "843: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "844: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "845: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "846: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "847: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "848: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "849: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "850: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "851: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "852: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "853: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "854: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "855: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "856: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "857: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "858: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "859: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "860: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "861: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "862: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "863: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "864: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "865: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "866: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "867: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "868: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "869: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "870: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "871: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "872: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "873: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "874: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "875: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "876: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "877: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "878: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "879: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "880: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "881: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "882: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "883: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "884: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "885: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "886: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "887: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "888: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "889: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "890: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "891: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "892: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "893: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "894: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "895: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "896: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "897: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "898: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "899: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "900: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "901: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "902: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "903: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "904: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "905: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "906: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "907: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "908: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "909: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "910: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "911: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "912: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "913: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "914: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "915: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "916: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "917: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "918: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "919: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "920: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "921: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "922: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "923: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "924: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "925: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "926: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "927: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "928: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "929: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "930: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "931: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "932: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "933: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "934: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "935: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "936: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "937: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "938: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "939: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "940: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "941: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "942: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "943: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "944: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "945: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "946: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "947: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "948: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "949: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "950: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "951: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "952: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "953: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "954: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "955: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "956: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "957: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "958: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "959: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "960: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "961: BertTokenizer.decode not understanding new vocabulary - https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "962: Tokenizer tend to choose added tokens first rather than token in vocab - https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "963: Special token printed out as output - https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948\n",
            "964: [NER][Japanese] labeled segment shorter than token - https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "965: T5Tokenizer add a whitespace token after added special tokens - https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "966: Unable to register my own tokenizer - https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "967: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab! - https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "968: Use Unicode blocks in regex (in Replace normalizer) - https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904\n",
            "969: How to handle translations one source language to many target sentences for the same language - https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "970: NER Label tokenization with overflowing tokens - https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561\n",
            "971: How to use a trained tokenizer for semantic search? - https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "972: Unk_token not set after training a BPETokenizer tokenizer - https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733\n",
            "973: AutoTokenizer is very slow when loading llama tokenizer - https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547\n",
            "974: Offset mappings differ for tokenizers - https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424\n",
            "975: The process for tokenizing concatenated dataset is slow st the end of tokenizing - https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "976: Batch tokenize (split into tokens, without processing) - https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238\n",
            "977: Does AutoTokenizer uploads data to HuggingFace - https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "978: RobertaTokenizer decode and tokenize do not have the same output - https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "979: Training tokenizers with padding in between tokens - https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "980: Leaving unknown words untokenized like in OpenMNT - https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "981: Keeping special chars in translations - https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270\n",
            "982: Should cls_token be [CLS] or <cls>? - https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "983: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False - https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "984: I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error - https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742\n",
            "985: OSError: Can’t load tokenizer for ‘facebook/xmod-base’ - https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471\n",
            "986: `GPT2Tokenizer` Tokenizer handling `\\n\\n` differently in different settings - https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289\n",
            "987: DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‘padding=True’ ‘truncation=True’ to have batched tensors with the same length - https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625\n",
            "988: Decode token IDs into a list (not a single string) - https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991\n",
            "989: Error training MLM with Roberta Tokenizer - https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878\n",
            "990: Are the slow and fast tokenizer results the same output for the same input? - https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "991: “Add_tokens” breaks words when encoding - https://discuss.huggingface.co/t/add-tokens-breaks-words-when-encoding/21154\n",
            "992: 2 tokens for one character in T5 - https://discuss.huggingface.co/t/2-tokens-for-one-character-in-t5/14960\n",
            "993: OSError: Model name ‘gpt2’ was not found in tokenizers model name list (gpt2,…) - https://discuss.huggingface.co/t/oserror-model-name-gpt2-was-not-found-in-tokenizers-model-name-list-gpt2/2164\n",
            "994: DNA long sequence tokenization - https://discuss.huggingface.co/t/dna-long-sequence-tokenization/16154\n",
            "995: SentencePiece tokenizer encodes to unknown token - https://discuss.huggingface.co/t/sentencepiece-tokenizer-encodes-to-unknown-token/49113\n",
            "996: Tokenizer behaviour with pipeline - https://discuss.huggingface.co/t/tokenizer-behaviour-with-pipeline/48969\n",
            "997: Load tokenizer from file : Exception: data did not match any variant of untagged enum ModelWrapper - https://discuss.huggingface.co/t/load-tokenizer-from-file-exception-data-did-not-match-any-variant-of-untagged-enum-modelwrapper/25325\n",
            "998: ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076 - https://discuss.huggingface.co/t/arrowinvalid-column-3-named-attention-mask-expected-length-1000-but-got-length-1076/6904\n",
            "999: Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model - https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829\n",
            "1000: Initialize Vocabulary for Unigram Tokenizer - https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371\n",
            "1001: Make correct padding for text generation with GPT-NEO - https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800\n",
            "1002: How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers? - https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639\n",
            "1003: Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF - https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602\n",
            "1004: Writing custom tokenizer and wrapping it in tokenizer object - https://discuss.huggingface.co/t/writing-custom-tokenizer-and-wrapping-it-in-tokenizer-object/39679\n",
            "1005: Tokenizer for German lang - https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222\n",
            "1006: Chunk tokens into desired chunk length without simply getting rid of rest of tokens - https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399\n",
            "1007: Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers - https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872\n",
            "1008: LlamaTokenizerFast returns token_type_ids but the forward pass of the LlamaModel does not receive token_type_ids - https://discuss.huggingface.co/t/llamatokenizerfast-returns-token-type-ids-but-the-forward-pass-of-the-llamamodel-does-not-receive-token-type-ids/42431\n",
            "1009: GPT2Tokenizer not working in Kaggle Notebook - https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508\n",
            "1010: Exploring the Majestic Temples in Karnataka - https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952\n",
            "1011: Tokenizer producing token index greater than size of the dictionary - https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989\n",
            "1012: How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer - https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843\n",
            "1013: How to create a HF tokenizer’s vocab file from a BPE model’s merges.txt file? - https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737\n",
            "1014: Scala/JVM Bindings for Tokenizers - https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393\n",
            "1015: Tokenizers Wheel Takes Forever to Build - https://discuss.huggingface.co/t/tokenizers-wheel-takes-forever-to-build/17114\n",
            "1016: Where the introduction of tokenizers.implementations? - https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959\n",
            "1017: How to return custom `token_type_ids` or other values from a tokenizer? - https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525\n",
            "1018: Easy way to compare tokenizers - https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347\n",
            "1019: Unable to load image using llama-index - https://discuss.huggingface.co/t/unable-to-load-image-using-llama-index/38304\n",
            "1020: Help defining tokenizer - https://discuss.huggingface.co/t/help-defining-tokenizer/38091\n",
            "1021: Token Offsets in Rust vs. Python - https://discuss.huggingface.co/t/token-offsets-in-rust-vs-python/37949\n",
            "1022: “OSError: Model name ‘./XX’ was not found in tokenizers model name list” - cannot load custom tokenizer in Transformers - https://discuss.huggingface.co/t/oserror-model-name-xx-was-not-found-in-tokenizers-model-name-list-cannot-load-custom-tokenizer-in-transformers/2714\n",
            "1023: Converting JSON/dict to flatten string with indicator tokens - https://discuss.huggingface.co/t/converting-json-dict-to-flatten-string-with-indicator-tokens/37387\n",
            "1024: Train Retry Tokenizer - https://discuss.huggingface.co/t/train-retry-tokenizer/37031\n",
            "1025: Pretokenise on punctuation except hyphens - https://discuss.huggingface.co/t/pretokenise-on-punctuation-except-hyphens/36691\n",
            "1026: Tokenizer Trainer Crashing - https://discuss.huggingface.co/t/tokenizer-trainer-crashing/36645\n",
            "1027: Tokenizer extremely slow when deployed to a container - https://discuss.huggingface.co/t/tokenizer-extremely-slow-when-deployed-to-a-container/36569\n",
            "1028: Dealing with Decimal and Fractions - https://discuss.huggingface.co/t/dealing-with-decimal-and-fractions/23377\n",
            "1029: `add_tokens` with argument `special_tokens=True` vs `add_special_tokens` - https://discuss.huggingface.co/t/add-tokens-with-argument-special-tokens-true-vs-add-special-tokens/35648\n",
            "1030: Unable to upload custom Pytorch model in huggingface - https://discuss.huggingface.co/t/unable-to-upload-custom-pytorch-model-in-huggingface/35545\n",
            "1031: RuntimeError: Cannot re-initialize CUDA in forked subprocess - https://discuss.huggingface.co/t/runtimeerror-cannot-re-initialize-cuda-in-forked-subprocess/28392\n",
            "1032: Overflowing Tokens in MarkupLM - https://discuss.huggingface.co/t/overflowing-tokens-in-markuplm/35217\n",
            "1033: I get the predicted token as ` े` . What am I doing wrong? - https://discuss.huggingface.co/t/i-get-the-predicted-token-as-what-am-i-doing-wrong/29038\n",
            "1034: <unk> token in the output instead curly braces - https://discuss.huggingface.co/t/unk-token-in-the-output-instead-curly-braces/34653\n",
            "1035: How to add a new token without expanding the vocabulary - https://discuss.huggingface.co/t/how-to-add-a-new-token-without-expanding-the-vocabulary/34536\n",
            "1036: Does the ByteLevelBPETokenizer need to be wrapped in a normal Tokenizer? - https://discuss.huggingface.co/t/does-the-bytelevelbpetokenizer-need-to-be-wrapped-in-a-normal-tokenizer/34121\n",
            "1037: What is required to create a fast tokenizer? For example for a Marian model - https://discuss.huggingface.co/t/what-is-required-to-create-a-fast-tokenizer-for-example-for-a-marian-model/33941\n",
            "1038: GPT2Tokenizer.decode maps unicode sequences to the same string ‘�’ - https://discuss.huggingface.co/t/gpt2tokenizer-decode-maps-unicode-sequences-to-the-same-string/32453\n",
            "1039: Issue with Tokenizer - https://discuss.huggingface.co/t/issue-with-tokenizer/33796\n",
            "1040: Tokenizing my novel for GPT model - https://discuss.huggingface.co/t/tokenizing-my-novel-for-gpt-model/33532\n",
            "1041: How to add additional custom pre-tokenization processing? - https://discuss.huggingface.co/t/how-to-add-additional-custom-pre-tokenization-processing/1637\n",
            "1042: Customize FlauBERT tokenizer to split line breaks - https://discuss.huggingface.co/t/customize-flaubert-tokenizer-to-split-line-breaks/33011\n",
            "1043: How to change the size of model_max_length? - https://discuss.huggingface.co/t/how-to-change-the-size-of-model-max-length/32942\n",
            "1044: Can’t get to the source code of `tokenizer.convert_tokens_to_string` - https://discuss.huggingface.co/t/cant-get-to-the-source-code-of-tokenizer-convert-tokens-to-string/32714\n",
            "1045: Why I’m getting same result with or without using Wav2Vec2Processor? - https://discuss.huggingface.co/t/why-im-getting-same-result-with-or-without-using-wav2vec2processor/32482\n",
            "1046: How does `tokenizer().input_ids` work and how different it is from tokenizer.encode() before `model.generate()` and decoding step? - https://discuss.huggingface.co/t/how-does-tokenizer-input-ids-work-and-how-different-it-is-from-tokenizer-encode-before-model-generate-and-decoding-step/30476\n",
            "1047: What file type should my training data be? - https://discuss.huggingface.co/t/what-file-type-should-my-training-data-be/32075\n",
            "1048: Best way to get the closest token indices of input of char_to_token is a whitespace - https://discuss.huggingface.co/t/best-way-to-get-the-closest-token-indices-of-input-of-char-to-token-is-a-whitespace/32004\n",
            "1049: Token indices sequence length is longer than the specified maximum sequence length - https://discuss.huggingface.co/t/token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length/25133\n",
            "1050: Create a simple tokenizer - https://discuss.huggingface.co/t/create-a-simple-tokenizer/31677\n",
            "1051: Sliding window for Long Documents - https://discuss.huggingface.co/t/sliding-window-for-long-documents/19367\n",
            "1052: Creating tokenizer from counts file? - https://discuss.huggingface.co/t/creating-tokenizer-from-counts-file/31399\n",
            "1053: Tokenizer.train() running out of memory - https://discuss.huggingface.co/t/tokenizer-train-running-out-of-memory/31355\n",
            "1054: Tokenizing Float Tensor? - https://discuss.huggingface.co/t/tokenizing-float-tensor/30604\n",
            "1055: Padding and truncation for custom tokenizer - https://discuss.huggingface.co/t/padding-and-truncation-for-custom-tokenizer/30180\n",
            "1056: Incorporate SARI score into run_summarization.py example script - https://discuss.huggingface.co/t/incorporate-sari-score-into-run-summarization-py-example-script/29511\n",
            "1057: Is that possible to embed the tokenizer into the model to have it running on GCP using TensorFlow Serving? - https://discuss.huggingface.co/t/is-that-possible-to-embed-the-tokenizer-into-the-model-to-have-it-running-on-gcp-using-tensorflow-serving/10532\n",
            "1058: Huggingface inference API issue - https://discuss.huggingface.co/t/huggingface-inference-api-issue/29307\n",
            "1059: Using Tokenizer for integer data - https://discuss.huggingface.co/t/using-tokenizer-for-integer-data/28835\n",
            "1060: GPT2 long text approach - https://discuss.huggingface.co/t/gpt2-long-text-approach/28165\n",
            "1061: Huggingface t5 models seem to not download a tokenizer file - https://discuss.huggingface.co/t/huggingface-t5-models-seem-to-not-download-a-tokenizer-file/27935\n",
            "1062: How to save a fast tokenizer using the transformer library and then load it using Tokenizers? - https://discuss.huggingface.co/t/how-to-save-a-fast-tokenizer-using-the-transformer-library-and-then-load-it-using-tokenizers/9567\n",
            "1063: Using a BertTokenizer when training a RobertaForMaskedLM - https://discuss.huggingface.co/t/using-a-berttokenizer-when-training-a-robertaformaskedlm/27456\n",
            "1064: Need clarity on “padding” parameter in Bert Tokenizer - https://discuss.huggingface.co/t/need-clarity-on-padding-parameter-in-bert-tokenizer/27444\n",
            "1065: How to convert HuggingFace tokenizers into ONNX format? - https://discuss.huggingface.co/t/how-to-convert-huggingface-tokenizers-into-onnx-format/27272\n",
            "1066: Can’t save ConvBert tokenizer - https://discuss.huggingface.co/t/cant-save-convbert-tokenizer/16006\n",
            "1067: RoBERTa Tokenizer Java Implementation - https://discuss.huggingface.co/t/roberta-tokenizer-java-implementation/16737\n",
            "1068: Unigram vocab_size doesn’t fit - https://discuss.huggingface.co/t/unigram-vocab-size-doesnt-fit/26856\n",
            "1069: Option to load only tokenizer and model configuration into “token-classification” pipeline - https://discuss.huggingface.co/t/option-to-load-only-tokenizer-and-model-configuration-into-token-classification-pipeline/26697\n",
            "1070: Encode_plus Pretokenized input seuqence must be Union - https://discuss.huggingface.co/t/encode-plus-pretokenized-input-seuqence-must-be-union/26449\n",
            "1071: Application of TFBertTokenizer - https://discuss.huggingface.co/t/application-of-tfberttokenizer/26437\n",
            "1072: TemplateProcessing for encoder-decoder - https://discuss.huggingface.co/t/templateprocessing-for-encoder-decoder/26135\n",
            "1073: Using `TFBertTokenizer` instead of `BertTokenizer` with `TFBertForQuestionAnswering` - https://discuss.huggingface.co/t/using-tfberttokenizer-instead-of-berttokenizer-with-tfbertforquestionanswering/26127\n",
            "1074: How to concatenate an answer to multiple choices after padded tokenization - https://discuss.huggingface.co/t/how-to-concatenate-an-answer-to-multiple-choices-after-padded-tokenization/26110\n",
            "1075: Maximum recursion depth exceeded when using DataCollator - https://discuss.huggingface.co/t/maximum-recursion-depth-exceeded-when-using-datacollator/25937\n",
            "1076: Adding a special language token to MBART - https://discuss.huggingface.co/t/adding-a-special-language-token-to-mbart/25967\n",
            "1077: Custom PostProcessor? - https://discuss.huggingface.co/t/custom-postprocessor/25847\n",
            "1078: Tokenizer.pad_token=what? - https://discuss.huggingface.co/t/tokenizer-pad-token-what/24367\n",
            "1079: Using HuggingFace Tokenizers Without Special Characters - https://discuss.huggingface.co/t/using-huggingface-tokenizers-without-special-characters/21962\n",
            "1080: How to get sp_model variable from T5Tokenizer? - https://discuss.huggingface.co/t/how-to-get-sp-model-variable-from-t5tokenizer/25171\n",
            "1081: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "1082: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "1083: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "1084: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "1085: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "1086: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "1087: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "1088: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "1089: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "1090: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "1091: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "1092: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "1093: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "1094: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "1095: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "1096: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "1097: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "1098: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "1099: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "1100: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "1101: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "1102: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "1103: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "1104: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "1105: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "1106: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "1107: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "1108: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "1109: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "1110: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "1111: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "1112: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "1113: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "1114: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "1115: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "1116: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "1117: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "1118: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "1119: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "1120: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "1121: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "1122: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "1123: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "1124: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "1125: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "1126: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "1127: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "1128: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "1129: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "1130: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "1131: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "1132: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "1133: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "1134: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "1135: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "1136: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "1137: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "1138: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "1139: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "1140: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "1141: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "1142: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "1143: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "1144: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "1145: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "1146: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "1147: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "1148: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "1149: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "1150: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "1151: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "1152: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "1153: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "1154: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "1155: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "1156: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "1157: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "1158: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "1159: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "1160: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "1161: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "1162: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "1163: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "1164: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "1165: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "1166: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "1167: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "1168: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "1169: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "1170: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "1171: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "1172: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "1173: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "1174: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "1175: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "1176: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "1177: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "1178: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "1179: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "1180: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "1181: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "1182: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "1183: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "1184: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "1185: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "1186: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "1187: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "1188: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "1189: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "1190: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "1191: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "1192: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "1193: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "1194: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "1195: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "1196: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "1197: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "1198: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "1199: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "1200: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "1201: BertTokenizer.decode not understanding new vocabulary - https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "1202: Tokenizer tend to choose added tokens first rather than token in vocab - https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "1203: Special token printed out as output - https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948\n",
            "1204: [NER][Japanese] labeled segment shorter than token - https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "1205: T5Tokenizer add a whitespace token after added special tokens - https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "1206: Unable to register my own tokenizer - https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "1207: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab! - https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "1208: Use Unicode blocks in regex (in Replace normalizer) - https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904\n",
            "1209: How to handle translations one source language to many target sentences for the same language - https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "1210: NER Label tokenization with overflowing tokens - https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561\n",
            "1211: How to use a trained tokenizer for semantic search? - https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "1212: Unk_token not set after training a BPETokenizer tokenizer - https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733\n",
            "1213: AutoTokenizer is very slow when loading llama tokenizer - https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547\n",
            "1214: Offset mappings differ for tokenizers - https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424\n",
            "1215: The process for tokenizing concatenated dataset is slow st the end of tokenizing - https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "1216: Batch tokenize (split into tokens, without processing) - https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238\n",
            "1217: Does AutoTokenizer uploads data to HuggingFace - https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "1218: RobertaTokenizer decode and tokenize do not have the same output - https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "1219: Training tokenizers with padding in between tokens - https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "1220: Leaving unknown words untokenized like in OpenMNT - https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "1221: Keeping special chars in translations - https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270\n",
            "1222: Should cls_token be [CLS] or <cls>? - https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "1223: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False - https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "1224: I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error - https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742\n",
            "1225: OSError: Can’t load tokenizer for ‘facebook/xmod-base’ - https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471\n",
            "1226: `GPT2Tokenizer` Tokenizer handling `\\n\\n` differently in different settings - https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289\n",
            "1227: DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‘padding=True’ ‘truncation=True’ to have batched tensors with the same length - https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625\n",
            "1228: Decode token IDs into a list (not a single string) - https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991\n",
            "1229: Error training MLM with Roberta Tokenizer - https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878\n",
            "1230: Are the slow and fast tokenizer results the same output for the same input? - https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "1231: “Add_tokens” breaks words when encoding - https://discuss.huggingface.co/t/add-tokens-breaks-words-when-encoding/21154\n",
            "1232: 2 tokens for one character in T5 - https://discuss.huggingface.co/t/2-tokens-for-one-character-in-t5/14960\n",
            "1233: OSError: Model name ‘gpt2’ was not found in tokenizers model name list (gpt2,…) - https://discuss.huggingface.co/t/oserror-model-name-gpt2-was-not-found-in-tokenizers-model-name-list-gpt2/2164\n",
            "1234: DNA long sequence tokenization - https://discuss.huggingface.co/t/dna-long-sequence-tokenization/16154\n",
            "1235: SentencePiece tokenizer encodes to unknown token - https://discuss.huggingface.co/t/sentencepiece-tokenizer-encodes-to-unknown-token/49113\n",
            "1236: Tokenizer behaviour with pipeline - https://discuss.huggingface.co/t/tokenizer-behaviour-with-pipeline/48969\n",
            "1237: Load tokenizer from file : Exception: data did not match any variant of untagged enum ModelWrapper - https://discuss.huggingface.co/t/load-tokenizer-from-file-exception-data-did-not-match-any-variant-of-untagged-enum-modelwrapper/25325\n",
            "1238: ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076 - https://discuss.huggingface.co/t/arrowinvalid-column-3-named-attention-mask-expected-length-1000-but-got-length-1076/6904\n",
            "1239: Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model - https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829\n",
            "1240: Initialize Vocabulary for Unigram Tokenizer - https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371\n",
            "1241: Make correct padding for text generation with GPT-NEO - https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800\n",
            "1242: How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers? - https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639\n",
            "1243: Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF - https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602\n",
            "1244: Writing custom tokenizer and wrapping it in tokenizer object - https://discuss.huggingface.co/t/writing-custom-tokenizer-and-wrapping-it-in-tokenizer-object/39679\n",
            "1245: Tokenizer for German lang - https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222\n",
            "1246: Chunk tokens into desired chunk length without simply getting rid of rest of tokens - https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399\n",
            "1247: Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers - https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872\n",
            "1248: LlamaTokenizerFast returns token_type_ids but the forward pass of the LlamaModel does not receive token_type_ids - https://discuss.huggingface.co/t/llamatokenizerfast-returns-token-type-ids-but-the-forward-pass-of-the-llamamodel-does-not-receive-token-type-ids/42431\n",
            "1249: GPT2Tokenizer not working in Kaggle Notebook - https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508\n",
            "1250: Exploring the Majestic Temples in Karnataka - https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952\n",
            "1251: Tokenizer producing token index greater than size of the dictionary - https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989\n",
            "1252: How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer - https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843\n",
            "1253: How to create a HF tokenizer’s vocab file from a BPE model’s merges.txt file? - https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737\n",
            "1254: Scala/JVM Bindings for Tokenizers - https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393\n",
            "1255: Tokenizers Wheel Takes Forever to Build - https://discuss.huggingface.co/t/tokenizers-wheel-takes-forever-to-build/17114\n",
            "1256: Where the introduction of tokenizers.implementations? - https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959\n",
            "1257: How to return custom `token_type_ids` or other values from a tokenizer? - https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525\n",
            "1258: Easy way to compare tokenizers - https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347\n",
            "1259: Unable to load image using llama-index - https://discuss.huggingface.co/t/unable-to-load-image-using-llama-index/38304\n",
            "1260: Help defining tokenizer - https://discuss.huggingface.co/t/help-defining-tokenizer/38091\n",
            "1261: Token Offsets in Rust vs. Python - https://discuss.huggingface.co/t/token-offsets-in-rust-vs-python/37949\n",
            "1262: “OSError: Model name ‘./XX’ was not found in tokenizers model name list” - cannot load custom tokenizer in Transformers - https://discuss.huggingface.co/t/oserror-model-name-xx-was-not-found-in-tokenizers-model-name-list-cannot-load-custom-tokenizer-in-transformers/2714\n",
            "1263: Converting JSON/dict to flatten string with indicator tokens - https://discuss.huggingface.co/t/converting-json-dict-to-flatten-string-with-indicator-tokens/37387\n",
            "1264: Train Retry Tokenizer - https://discuss.huggingface.co/t/train-retry-tokenizer/37031\n",
            "1265: Pretokenise on punctuation except hyphens - https://discuss.huggingface.co/t/pretokenise-on-punctuation-except-hyphens/36691\n",
            "1266: Tokenizer Trainer Crashing - https://discuss.huggingface.co/t/tokenizer-trainer-crashing/36645\n",
            "1267: Tokenizer extremely slow when deployed to a container - https://discuss.huggingface.co/t/tokenizer-extremely-slow-when-deployed-to-a-container/36569\n",
            "1268: Dealing with Decimal and Fractions - https://discuss.huggingface.co/t/dealing-with-decimal-and-fractions/23377\n",
            "1269: `add_tokens` with argument `special_tokens=True` vs `add_special_tokens` - https://discuss.huggingface.co/t/add-tokens-with-argument-special-tokens-true-vs-add-special-tokens/35648\n",
            "1270: Unable to upload custom Pytorch model in huggingface - https://discuss.huggingface.co/t/unable-to-upload-custom-pytorch-model-in-huggingface/35545\n",
            "1271: RuntimeError: Cannot re-initialize CUDA in forked subprocess - https://discuss.huggingface.co/t/runtimeerror-cannot-re-initialize-cuda-in-forked-subprocess/28392\n",
            "1272: Overflowing Tokens in MarkupLM - https://discuss.huggingface.co/t/overflowing-tokens-in-markuplm/35217\n",
            "1273: I get the predicted token as ` े` . What am I doing wrong? - https://discuss.huggingface.co/t/i-get-the-predicted-token-as-what-am-i-doing-wrong/29038\n",
            "1274: <unk> token in the output instead curly braces - https://discuss.huggingface.co/t/unk-token-in-the-output-instead-curly-braces/34653\n",
            "1275: How to add a new token without expanding the vocabulary - https://discuss.huggingface.co/t/how-to-add-a-new-token-without-expanding-the-vocabulary/34536\n",
            "1276: Does the ByteLevelBPETokenizer need to be wrapped in a normal Tokenizer? - https://discuss.huggingface.co/t/does-the-bytelevelbpetokenizer-need-to-be-wrapped-in-a-normal-tokenizer/34121\n",
            "1277: What is required to create a fast tokenizer? For example for a Marian model - https://discuss.huggingface.co/t/what-is-required-to-create-a-fast-tokenizer-for-example-for-a-marian-model/33941\n",
            "1278: GPT2Tokenizer.decode maps unicode sequences to the same string ‘�’ - https://discuss.huggingface.co/t/gpt2tokenizer-decode-maps-unicode-sequences-to-the-same-string/32453\n",
            "1279: Issue with Tokenizer - https://discuss.huggingface.co/t/issue-with-tokenizer/33796\n",
            "1280: Tokenizing my novel for GPT model - https://discuss.huggingface.co/t/tokenizing-my-novel-for-gpt-model/33532\n",
            "1281: How to add additional custom pre-tokenization processing? - https://discuss.huggingface.co/t/how-to-add-additional-custom-pre-tokenization-processing/1637\n",
            "1282: Customize FlauBERT tokenizer to split line breaks - https://discuss.huggingface.co/t/customize-flaubert-tokenizer-to-split-line-breaks/33011\n",
            "1283: How to change the size of model_max_length? - https://discuss.huggingface.co/t/how-to-change-the-size-of-model-max-length/32942\n",
            "1284: Can’t get to the source code of `tokenizer.convert_tokens_to_string` - https://discuss.huggingface.co/t/cant-get-to-the-source-code-of-tokenizer-convert-tokens-to-string/32714\n",
            "1285: Why I’m getting same result with or without using Wav2Vec2Processor? - https://discuss.huggingface.co/t/why-im-getting-same-result-with-or-without-using-wav2vec2processor/32482\n",
            "1286: How does `tokenizer().input_ids` work and how different it is from tokenizer.encode() before `model.generate()` and decoding step? - https://discuss.huggingface.co/t/how-does-tokenizer-input-ids-work-and-how-different-it-is-from-tokenizer-encode-before-model-generate-and-decoding-step/30476\n",
            "1287: What file type should my training data be? - https://discuss.huggingface.co/t/what-file-type-should-my-training-data-be/32075\n",
            "1288: Best way to get the closest token indices of input of char_to_token is a whitespace - https://discuss.huggingface.co/t/best-way-to-get-the-closest-token-indices-of-input-of-char-to-token-is-a-whitespace/32004\n",
            "1289: Token indices sequence length is longer than the specified maximum sequence length - https://discuss.huggingface.co/t/token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length/25133\n",
            "1290: Create a simple tokenizer - https://discuss.huggingface.co/t/create-a-simple-tokenizer/31677\n",
            "1291: Sliding window for Long Documents - https://discuss.huggingface.co/t/sliding-window-for-long-documents/19367\n",
            "1292: Creating tokenizer from counts file? - https://discuss.huggingface.co/t/creating-tokenizer-from-counts-file/31399\n",
            "1293: Tokenizer.train() running out of memory - https://discuss.huggingface.co/t/tokenizer-train-running-out-of-memory/31355\n",
            "1294: Tokenizing Float Tensor? - https://discuss.huggingface.co/t/tokenizing-float-tensor/30604\n",
            "1295: Padding and truncation for custom tokenizer - https://discuss.huggingface.co/t/padding-and-truncation-for-custom-tokenizer/30180\n",
            "1296: Incorporate SARI score into run_summarization.py example script - https://discuss.huggingface.co/t/incorporate-sari-score-into-run-summarization-py-example-script/29511\n",
            "1297: Is that possible to embed the tokenizer into the model to have it running on GCP using TensorFlow Serving? - https://discuss.huggingface.co/t/is-that-possible-to-embed-the-tokenizer-into-the-model-to-have-it-running-on-gcp-using-tensorflow-serving/10532\n",
            "1298: Huggingface inference API issue - https://discuss.huggingface.co/t/huggingface-inference-api-issue/29307\n",
            "1299: Using Tokenizer for integer data - https://discuss.huggingface.co/t/using-tokenizer-for-integer-data/28835\n",
            "1300: GPT2 long text approach - https://discuss.huggingface.co/t/gpt2-long-text-approach/28165\n",
            "1301: Huggingface t5 models seem to not download a tokenizer file - https://discuss.huggingface.co/t/huggingface-t5-models-seem-to-not-download-a-tokenizer-file/27935\n",
            "1302: How to save a fast tokenizer using the transformer library and then load it using Tokenizers? - https://discuss.huggingface.co/t/how-to-save-a-fast-tokenizer-using-the-transformer-library-and-then-load-it-using-tokenizers/9567\n",
            "1303: Using a BertTokenizer when training a RobertaForMaskedLM - https://discuss.huggingface.co/t/using-a-berttokenizer-when-training-a-robertaformaskedlm/27456\n",
            "1304: Need clarity on “padding” parameter in Bert Tokenizer - https://discuss.huggingface.co/t/need-clarity-on-padding-parameter-in-bert-tokenizer/27444\n",
            "1305: How to convert HuggingFace tokenizers into ONNX format? - https://discuss.huggingface.co/t/how-to-convert-huggingface-tokenizers-into-onnx-format/27272\n",
            "1306: Can’t save ConvBert tokenizer - https://discuss.huggingface.co/t/cant-save-convbert-tokenizer/16006\n",
            "1307: RoBERTa Tokenizer Java Implementation - https://discuss.huggingface.co/t/roberta-tokenizer-java-implementation/16737\n",
            "1308: Unigram vocab_size doesn’t fit - https://discuss.huggingface.co/t/unigram-vocab-size-doesnt-fit/26856\n",
            "1309: Option to load only tokenizer and model configuration into “token-classification” pipeline - https://discuss.huggingface.co/t/option-to-load-only-tokenizer-and-model-configuration-into-token-classification-pipeline/26697\n",
            "1310: Encode_plus Pretokenized input seuqence must be Union - https://discuss.huggingface.co/t/encode-plus-pretokenized-input-seuqence-must-be-union/26449\n",
            "1311: Application of TFBertTokenizer - https://discuss.huggingface.co/t/application-of-tfberttokenizer/26437\n",
            "1312: TemplateProcessing for encoder-decoder - https://discuss.huggingface.co/t/templateprocessing-for-encoder-decoder/26135\n",
            "1313: Using `TFBertTokenizer` instead of `BertTokenizer` with `TFBertForQuestionAnswering` - https://discuss.huggingface.co/t/using-tfberttokenizer-instead-of-berttokenizer-with-tfbertforquestionanswering/26127\n",
            "1314: How to concatenate an answer to multiple choices after padded tokenization - https://discuss.huggingface.co/t/how-to-concatenate-an-answer-to-multiple-choices-after-padded-tokenization/26110\n",
            "1315: Maximum recursion depth exceeded when using DataCollator - https://discuss.huggingface.co/t/maximum-recursion-depth-exceeded-when-using-datacollator/25937\n",
            "1316: Adding a special language token to MBART - https://discuss.huggingface.co/t/adding-a-special-language-token-to-mbart/25967\n",
            "1317: Custom PostProcessor? - https://discuss.huggingface.co/t/custom-postprocessor/25847\n",
            "1318: Tokenizer.pad_token=what? - https://discuss.huggingface.co/t/tokenizer-pad-token-what/24367\n",
            "1319: Using HuggingFace Tokenizers Without Special Characters - https://discuss.huggingface.co/t/using-huggingface-tokenizers-without-special-characters/21962\n",
            "1320: How to get sp_model variable from T5Tokenizer? - https://discuss.huggingface.co/t/how-to-get-sp-model-variable-from-t5tokenizer/25171\n",
            "1321: Wav2vec2CTCTokenizer and vocab.json - https://discuss.huggingface.co/t/wav2vec2ctctokenizer-and-vocab-json/25146\n",
            "1322: Period ID in RobertaTokenizer with is_split_into_words - https://discuss.huggingface.co/t/period-id-in-robertatokenizer-with-is-split-into-words/24142\n",
            "1323: WordLevel error: Missing [UNK] token from the vocabulary - https://discuss.huggingface.co/t/wordlevel-error-missing-unk-token-from-the-vocabulary/5107\n",
            "1324: Tokenizer post_processor help - https://discuss.huggingface.co/t/tokenizer-post-processor-help/21926\n",
            "1325: Preprocessing raw text - https://discuss.huggingface.co/t/preprocessing-raw-text/24559\n",
            "1326: Save tokenizer with argument - https://discuss.huggingface.co/t/save-tokenizer-with-argument/17389\n",
            "1327: Trained tokenizer API as PretrainedTokenizer - https://discuss.huggingface.co/t/trained-tokenizer-api-as-pretrainedtokenizer/23639\n",
            "1328: Remove only certain special token id during tokenizer decode - https://discuss.huggingface.co/t/remove-only-certain-special-token-id-during-tokenizer-decode/20436\n",
            "1329: Convert_tokens_to_ids produces <unk> - https://discuss.huggingface.co/t/convert-tokens-to-ids-produces-unk/24771\n",
            "1330: Text preprocessing for fitting Tokenizer model - https://discuss.huggingface.co/t/text-preprocessing-for-fitting-tokenizer-model/24869\n",
            "1331: Special tokens warning - https://discuss.huggingface.co/t/special-tokens-warning/24939\n",
            "1332: Simple Transformers Multilabelclassification - https://discuss.huggingface.co/t/simple-transformers-multilabelclassification/24568\n",
            "1333: Cannot initialize deberta-v3-base tokenizer - https://discuss.huggingface.co/t/cannot-initialize-deberta-v3-base-tokenizer/15322\n",
            "1334: Getting Wholeword corresponding to a subword in a text? - https://discuss.huggingface.co/t/getting-wholeword-corresponding-to-a-subword-in-a-text/24152\n",
            "1335: Issue with pushing tokenizer to hub - https://discuss.huggingface.co/t/issue-with-pushing-tokenizer-to-hub/24120\n",
            "1336: How do we customize the number of entites for NER pretrained model? - https://discuss.huggingface.co/t/how-do-we-customize-the-number-of-entites-for-ner-pretrained-model/24085\n",
            "1337: Configure RobertaTokenizer - https://discuss.huggingface.co/t/configure-robertatokenizer/23969\n",
            "1338: How to properly clean vocabulary from BBPE tokenizer - https://discuss.huggingface.co/t/how-to-properly-clean-vocabulary-from-bbpe-tokenizer/22827\n",
            "1339: Map tokenization and posterior to smaller substrings - https://discuss.huggingface.co/t/map-tokenization-and-posterior-to-smaller-substrings/23792\n",
            "1340: T5 model tokenizer - https://discuss.huggingface.co/t/t5-model-tokenizer/23640\n",
            "1341: Fast tokenizer for marianMTModel - https://discuss.huggingface.co/t/fast-tokenizer-for-marianmtmodel/23638\n",
            "1342: Word tokenizers for text generators - https://discuss.huggingface.co/t/word-tokenizers-for-text-generators/23464\n",
            "1343: SentencePieceUnigramTokenizer - https://discuss.huggingface.co/t/sentencepieceunigramtokenizer/23509\n",
            "1344: Tokenizer is not being loaded on Huggingface Inference - https://discuss.huggingface.co/t/tokenizer-is-not-being-loaded-on-huggingface-inference/23505\n",
            "1345: Why is BertNormalizer not exposed on the tokenizers library? - https://discuss.huggingface.co/t/why-is-bertnormalizer-not-exposed-on-the-tokenizers-library/23340\n",
            "1346: Sentence splitting - https://discuss.huggingface.co/t/sentence-splitting/5393\n",
            "1347: Average time to train a SentencePieceBPETokenizer - https://discuss.huggingface.co/t/average-time-to-train-a-sentencepiecebpetokenizer/23081\n",
            "1348: 1 line code for NER data set preparation using tokenizer library! - https://discuss.huggingface.co/t/1-line-code-for-ner-data-set-preparation-using-tokenizer-library/22816\n",
            "1349: Microsoft/codebert-base produces two sep tokens - https://discuss.huggingface.co/t/microsoft-codebert-base-produces-two-sep-tokens/21366\n",
            "1350: Padding with sliding window - https://discuss.huggingface.co/t/padding-with-sliding-window/18921\n",
            "1351: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "1352: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "1353: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "1354: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "1355: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "1356: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "1357: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "1358: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "1359: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "1360: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "1361: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "1362: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "1363: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "1364: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "1365: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "1366: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "1367: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "1368: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "1369: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "1370: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "1371: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "1372: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "1373: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "1374: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "1375: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "1376: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "1377: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "1378: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "1379: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "1380: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "1381: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "1382: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "1383: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "1384: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "1385: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "1386: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "1387: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "1388: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "1389: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "1390: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "1391: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "1392: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "1393: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "1394: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "1395: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "1396: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "1397: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "1398: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "1399: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "1400: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "1401: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "1402: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "1403: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "1404: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "1405: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "1406: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "1407: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "1408: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "1409: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "1410: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "1411: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "1412: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "1413: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "1414: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "1415: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "1416: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "1417: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "1418: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "1419: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "1420: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "1421: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "1422: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "1423: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "1424: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "1425: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "1426: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "1427: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "1428: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "1429: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "1430: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "1431: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "1432: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "1433: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "1434: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "1435: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "1436: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "1437: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "1438: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "1439: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "1440: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "1441: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "1442: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "1443: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "1444: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "1445: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "1446: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "1447: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "1448: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "1449: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "1450: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "1451: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "1452: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "1453: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "1454: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "1455: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "1456: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "1457: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "1458: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "1459: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "1460: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "1461: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "1462: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "1463: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "1464: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "1465: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "1466: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "1467: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "1468: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "1469: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "1470: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "1471: BertTokenizer.decode not understanding new vocabulary - https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "1472: Tokenizer tend to choose added tokens first rather than token in vocab - https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "1473: Special token printed out as output - https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948\n",
            "1474: [NER][Japanese] labeled segment shorter than token - https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "1475: T5Tokenizer add a whitespace token after added special tokens - https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "1476: Unable to register my own tokenizer - https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "1477: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab! - https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "1478: Use Unicode blocks in regex (in Replace normalizer) - https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904\n",
            "1479: How to handle translations one source language to many target sentences for the same language - https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "1480: NER Label tokenization with overflowing tokens - https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561\n",
            "1481: How to use a trained tokenizer for semantic search? - https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "1482: Unk_token not set after training a BPETokenizer tokenizer - https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733\n",
            "1483: AutoTokenizer is very slow when loading llama tokenizer - https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547\n",
            "1484: Offset mappings differ for tokenizers - https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424\n",
            "1485: The process for tokenizing concatenated dataset is slow st the end of tokenizing - https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "1486: Batch tokenize (split into tokens, without processing) - https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238\n",
            "1487: Does AutoTokenizer uploads data to HuggingFace - https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "1488: RobertaTokenizer decode and tokenize do not have the same output - https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "1489: Training tokenizers with padding in between tokens - https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "1490: Leaving unknown words untokenized like in OpenMNT - https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "1491: Keeping special chars in translations - https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270\n",
            "1492: Should cls_token be [CLS] or <cls>? - https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "1493: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False - https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "1494: I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error - https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742\n",
            "1495: OSError: Can’t load tokenizer for ‘facebook/xmod-base’ - https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471\n",
            "1496: `GPT2Tokenizer` Tokenizer handling `\\n\\n` differently in different settings - https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289\n",
            "1497: DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‘padding=True’ ‘truncation=True’ to have batched tensors with the same length - https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625\n",
            "1498: Decode token IDs into a list (not a single string) - https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991\n",
            "1499: Error training MLM with Roberta Tokenizer - https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878\n",
            "1500: Are the slow and fast tokenizer results the same output for the same input? - https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "1501: “Add_tokens” breaks words when encoding - https://discuss.huggingface.co/t/add-tokens-breaks-words-when-encoding/21154\n",
            "1502: 2 tokens for one character in T5 - https://discuss.huggingface.co/t/2-tokens-for-one-character-in-t5/14960\n",
            "1503: OSError: Model name ‘gpt2’ was not found in tokenizers model name list (gpt2,…) - https://discuss.huggingface.co/t/oserror-model-name-gpt2-was-not-found-in-tokenizers-model-name-list-gpt2/2164\n",
            "1504: DNA long sequence tokenization - https://discuss.huggingface.co/t/dna-long-sequence-tokenization/16154\n",
            "1505: SentencePiece tokenizer encodes to unknown token - https://discuss.huggingface.co/t/sentencepiece-tokenizer-encodes-to-unknown-token/49113\n",
            "1506: Tokenizer behaviour with pipeline - https://discuss.huggingface.co/t/tokenizer-behaviour-with-pipeline/48969\n",
            "1507: Load tokenizer from file : Exception: data did not match any variant of untagged enum ModelWrapper - https://discuss.huggingface.co/t/load-tokenizer-from-file-exception-data-did-not-match-any-variant-of-untagged-enum-modelwrapper/25325\n",
            "1508: ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076 - https://discuss.huggingface.co/t/arrowinvalid-column-3-named-attention-mask-expected-length-1000-but-got-length-1076/6904\n",
            "1509: Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model - https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829\n",
            "1510: Initialize Vocabulary for Unigram Tokenizer - https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371\n",
            "1511: Make correct padding for text generation with GPT-NEO - https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800\n",
            "1512: How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers? - https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639\n",
            "1513: Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF - https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602\n",
            "1514: Writing custom tokenizer and wrapping it in tokenizer object - https://discuss.huggingface.co/t/writing-custom-tokenizer-and-wrapping-it-in-tokenizer-object/39679\n",
            "1515: Tokenizer for German lang - https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222\n",
            "1516: Chunk tokens into desired chunk length without simply getting rid of rest of tokens - https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399\n",
            "1517: Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers - https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872\n",
            "1518: LlamaTokenizerFast returns token_type_ids but the forward pass of the LlamaModel does not receive token_type_ids - https://discuss.huggingface.co/t/llamatokenizerfast-returns-token-type-ids-but-the-forward-pass-of-the-llamamodel-does-not-receive-token-type-ids/42431\n",
            "1519: GPT2Tokenizer not working in Kaggle Notebook - https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508\n",
            "1520: Exploring the Majestic Temples in Karnataka - https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952\n",
            "1521: Tokenizer producing token index greater than size of the dictionary - https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989\n",
            "1522: How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer - https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843\n",
            "1523: How to create a HF tokenizer’s vocab file from a BPE model’s merges.txt file? - https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737\n",
            "1524: Scala/JVM Bindings for Tokenizers - https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393\n",
            "1525: Tokenizers Wheel Takes Forever to Build - https://discuss.huggingface.co/t/tokenizers-wheel-takes-forever-to-build/17114\n",
            "1526: Where the introduction of tokenizers.implementations? - https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959\n",
            "1527: How to return custom `token_type_ids` or other values from a tokenizer? - https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525\n",
            "1528: Easy way to compare tokenizers - https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347\n",
            "1529: Unable to load image using llama-index - https://discuss.huggingface.co/t/unable-to-load-image-using-llama-index/38304\n",
            "1530: Help defining tokenizer - https://discuss.huggingface.co/t/help-defining-tokenizer/38091\n",
            "1531: Token Offsets in Rust vs. Python - https://discuss.huggingface.co/t/token-offsets-in-rust-vs-python/37949\n",
            "1532: “OSError: Model name ‘./XX’ was not found in tokenizers model name list” - cannot load custom tokenizer in Transformers - https://discuss.huggingface.co/t/oserror-model-name-xx-was-not-found-in-tokenizers-model-name-list-cannot-load-custom-tokenizer-in-transformers/2714\n",
            "1533: Converting JSON/dict to flatten string with indicator tokens - https://discuss.huggingface.co/t/converting-json-dict-to-flatten-string-with-indicator-tokens/37387\n",
            "1534: Train Retry Tokenizer - https://discuss.huggingface.co/t/train-retry-tokenizer/37031\n",
            "1535: Pretokenise on punctuation except hyphens - https://discuss.huggingface.co/t/pretokenise-on-punctuation-except-hyphens/36691\n",
            "1536: Tokenizer Trainer Crashing - https://discuss.huggingface.co/t/tokenizer-trainer-crashing/36645\n",
            "1537: Tokenizer extremely slow when deployed to a container - https://discuss.huggingface.co/t/tokenizer-extremely-slow-when-deployed-to-a-container/36569\n",
            "1538: Dealing with Decimal and Fractions - https://discuss.huggingface.co/t/dealing-with-decimal-and-fractions/23377\n",
            "1539: `add_tokens` with argument `special_tokens=True` vs `add_special_tokens` - https://discuss.huggingface.co/t/add-tokens-with-argument-special-tokens-true-vs-add-special-tokens/35648\n",
            "1540: Unable to upload custom Pytorch model in huggingface - https://discuss.huggingface.co/t/unable-to-upload-custom-pytorch-model-in-huggingface/35545\n",
            "1541: RuntimeError: Cannot re-initialize CUDA in forked subprocess - https://discuss.huggingface.co/t/runtimeerror-cannot-re-initialize-cuda-in-forked-subprocess/28392\n",
            "1542: Overflowing Tokens in MarkupLM - https://discuss.huggingface.co/t/overflowing-tokens-in-markuplm/35217\n",
            "1543: I get the predicted token as ` े` . What am I doing wrong? - https://discuss.huggingface.co/t/i-get-the-predicted-token-as-what-am-i-doing-wrong/29038\n",
            "1544: <unk> token in the output instead curly braces - https://discuss.huggingface.co/t/unk-token-in-the-output-instead-curly-braces/34653\n",
            "1545: How to add a new token without expanding the vocabulary - https://discuss.huggingface.co/t/how-to-add-a-new-token-without-expanding-the-vocabulary/34536\n",
            "1546: Does the ByteLevelBPETokenizer need to be wrapped in a normal Tokenizer? - https://discuss.huggingface.co/t/does-the-bytelevelbpetokenizer-need-to-be-wrapped-in-a-normal-tokenizer/34121\n",
            "1547: What is required to create a fast tokenizer? For example for a Marian model - https://discuss.huggingface.co/t/what-is-required-to-create-a-fast-tokenizer-for-example-for-a-marian-model/33941\n",
            "1548: GPT2Tokenizer.decode maps unicode sequences to the same string ‘�’ - https://discuss.huggingface.co/t/gpt2tokenizer-decode-maps-unicode-sequences-to-the-same-string/32453\n",
            "1549: Issue with Tokenizer - https://discuss.huggingface.co/t/issue-with-tokenizer/33796\n",
            "1550: Tokenizing my novel for GPT model - https://discuss.huggingface.co/t/tokenizing-my-novel-for-gpt-model/33532\n",
            "1551: How to add additional custom pre-tokenization processing? - https://discuss.huggingface.co/t/how-to-add-additional-custom-pre-tokenization-processing/1637\n",
            "1552: Customize FlauBERT tokenizer to split line breaks - https://discuss.huggingface.co/t/customize-flaubert-tokenizer-to-split-line-breaks/33011\n",
            "1553: How to change the size of model_max_length? - https://discuss.huggingface.co/t/how-to-change-the-size-of-model-max-length/32942\n",
            "1554: Can’t get to the source code of `tokenizer.convert_tokens_to_string` - https://discuss.huggingface.co/t/cant-get-to-the-source-code-of-tokenizer-convert-tokens-to-string/32714\n",
            "1555: Why I’m getting same result with or without using Wav2Vec2Processor? - https://discuss.huggingface.co/t/why-im-getting-same-result-with-or-without-using-wav2vec2processor/32482\n",
            "1556: How does `tokenizer().input_ids` work and how different it is from tokenizer.encode() before `model.generate()` and decoding step? - https://discuss.huggingface.co/t/how-does-tokenizer-input-ids-work-and-how-different-it-is-from-tokenizer-encode-before-model-generate-and-decoding-step/30476\n",
            "1557: What file type should my training data be? - https://discuss.huggingface.co/t/what-file-type-should-my-training-data-be/32075\n",
            "1558: Best way to get the closest token indices of input of char_to_token is a whitespace - https://discuss.huggingface.co/t/best-way-to-get-the-closest-token-indices-of-input-of-char-to-token-is-a-whitespace/32004\n",
            "1559: Token indices sequence length is longer than the specified maximum sequence length - https://discuss.huggingface.co/t/token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length/25133\n",
            "1560: Create a simple tokenizer - https://discuss.huggingface.co/t/create-a-simple-tokenizer/31677\n",
            "1561: Sliding window for Long Documents - https://discuss.huggingface.co/t/sliding-window-for-long-documents/19367\n",
            "1562: Creating tokenizer from counts file? - https://discuss.huggingface.co/t/creating-tokenizer-from-counts-file/31399\n",
            "1563: Tokenizer.train() running out of memory - https://discuss.huggingface.co/t/tokenizer-train-running-out-of-memory/31355\n",
            "1564: Tokenizing Float Tensor? - https://discuss.huggingface.co/t/tokenizing-float-tensor/30604\n",
            "1565: Padding and truncation for custom tokenizer - https://discuss.huggingface.co/t/padding-and-truncation-for-custom-tokenizer/30180\n",
            "1566: Incorporate SARI score into run_summarization.py example script - https://discuss.huggingface.co/t/incorporate-sari-score-into-run-summarization-py-example-script/29511\n",
            "1567: Is that possible to embed the tokenizer into the model to have it running on GCP using TensorFlow Serving? - https://discuss.huggingface.co/t/is-that-possible-to-embed-the-tokenizer-into-the-model-to-have-it-running-on-gcp-using-tensorflow-serving/10532\n",
            "1568: Huggingface inference API issue - https://discuss.huggingface.co/t/huggingface-inference-api-issue/29307\n",
            "1569: Using Tokenizer for integer data - https://discuss.huggingface.co/t/using-tokenizer-for-integer-data/28835\n",
            "1570: GPT2 long text approach - https://discuss.huggingface.co/t/gpt2-long-text-approach/28165\n",
            "1571: Huggingface t5 models seem to not download a tokenizer file - https://discuss.huggingface.co/t/huggingface-t5-models-seem-to-not-download-a-tokenizer-file/27935\n",
            "1572: How to save a fast tokenizer using the transformer library and then load it using Tokenizers? - https://discuss.huggingface.co/t/how-to-save-a-fast-tokenizer-using-the-transformer-library-and-then-load-it-using-tokenizers/9567\n",
            "1573: Using a BertTokenizer when training a RobertaForMaskedLM - https://discuss.huggingface.co/t/using-a-berttokenizer-when-training-a-robertaformaskedlm/27456\n",
            "1574: Need clarity on “padding” parameter in Bert Tokenizer - https://discuss.huggingface.co/t/need-clarity-on-padding-parameter-in-bert-tokenizer/27444\n",
            "1575: How to convert HuggingFace tokenizers into ONNX format? - https://discuss.huggingface.co/t/how-to-convert-huggingface-tokenizers-into-onnx-format/27272\n",
            "1576: Can’t save ConvBert tokenizer - https://discuss.huggingface.co/t/cant-save-convbert-tokenizer/16006\n",
            "1577: RoBERTa Tokenizer Java Implementation - https://discuss.huggingface.co/t/roberta-tokenizer-java-implementation/16737\n",
            "1578: Unigram vocab_size doesn’t fit - https://discuss.huggingface.co/t/unigram-vocab-size-doesnt-fit/26856\n",
            "1579: Option to load only tokenizer and model configuration into “token-classification” pipeline - https://discuss.huggingface.co/t/option-to-load-only-tokenizer-and-model-configuration-into-token-classification-pipeline/26697\n",
            "1580: Encode_plus Pretokenized input seuqence must be Union - https://discuss.huggingface.co/t/encode-plus-pretokenized-input-seuqence-must-be-union/26449\n",
            "1581: Application of TFBertTokenizer - https://discuss.huggingface.co/t/application-of-tfberttokenizer/26437\n",
            "1582: TemplateProcessing for encoder-decoder - https://discuss.huggingface.co/t/templateprocessing-for-encoder-decoder/26135\n",
            "1583: Using `TFBertTokenizer` instead of `BertTokenizer` with `TFBertForQuestionAnswering` - https://discuss.huggingface.co/t/using-tfberttokenizer-instead-of-berttokenizer-with-tfbertforquestionanswering/26127\n",
            "1584: How to concatenate an answer to multiple choices after padded tokenization - https://discuss.huggingface.co/t/how-to-concatenate-an-answer-to-multiple-choices-after-padded-tokenization/26110\n",
            "1585: Maximum recursion depth exceeded when using DataCollator - https://discuss.huggingface.co/t/maximum-recursion-depth-exceeded-when-using-datacollator/25937\n",
            "1586: Adding a special language token to MBART - https://discuss.huggingface.co/t/adding-a-special-language-token-to-mbart/25967\n",
            "1587: Custom PostProcessor? - https://discuss.huggingface.co/t/custom-postprocessor/25847\n",
            "1588: Tokenizer.pad_token=what? - https://discuss.huggingface.co/t/tokenizer-pad-token-what/24367\n",
            "1589: Using HuggingFace Tokenizers Without Special Characters - https://discuss.huggingface.co/t/using-huggingface-tokenizers-without-special-characters/21962\n",
            "1590: How to get sp_model variable from T5Tokenizer? - https://discuss.huggingface.co/t/how-to-get-sp-model-variable-from-t5tokenizer/25171\n",
            "1591: Wav2vec2CTCTokenizer and vocab.json - https://discuss.huggingface.co/t/wav2vec2ctctokenizer-and-vocab-json/25146\n",
            "1592: Period ID in RobertaTokenizer with is_split_into_words - https://discuss.huggingface.co/t/period-id-in-robertatokenizer-with-is-split-into-words/24142\n",
            "1593: WordLevel error: Missing [UNK] token from the vocabulary - https://discuss.huggingface.co/t/wordlevel-error-missing-unk-token-from-the-vocabulary/5107\n",
            "1594: Tokenizer post_processor help - https://discuss.huggingface.co/t/tokenizer-post-processor-help/21926\n",
            "1595: Preprocessing raw text - https://discuss.huggingface.co/t/preprocessing-raw-text/24559\n",
            "1596: Save tokenizer with argument - https://discuss.huggingface.co/t/save-tokenizer-with-argument/17389\n",
            "1597: Trained tokenizer API as PretrainedTokenizer - https://discuss.huggingface.co/t/trained-tokenizer-api-as-pretrainedtokenizer/23639\n",
            "1598: Remove only certain special token id during tokenizer decode - https://discuss.huggingface.co/t/remove-only-certain-special-token-id-during-tokenizer-decode/20436\n",
            "1599: Convert_tokens_to_ids produces <unk> - https://discuss.huggingface.co/t/convert-tokens-to-ids-produces-unk/24771\n",
            "1600: Text preprocessing for fitting Tokenizer model - https://discuss.huggingface.co/t/text-preprocessing-for-fitting-tokenizer-model/24869\n",
            "1601: Special tokens warning - https://discuss.huggingface.co/t/special-tokens-warning/24939\n",
            "1602: Simple Transformers Multilabelclassification - https://discuss.huggingface.co/t/simple-transformers-multilabelclassification/24568\n",
            "1603: Cannot initialize deberta-v3-base tokenizer - https://discuss.huggingface.co/t/cannot-initialize-deberta-v3-base-tokenizer/15322\n",
            "1604: Getting Wholeword corresponding to a subword in a text? - https://discuss.huggingface.co/t/getting-wholeword-corresponding-to-a-subword-in-a-text/24152\n",
            "1605: Issue with pushing tokenizer to hub - https://discuss.huggingface.co/t/issue-with-pushing-tokenizer-to-hub/24120\n",
            "1606: How do we customize the number of entites for NER pretrained model? - https://discuss.huggingface.co/t/how-do-we-customize-the-number-of-entites-for-ner-pretrained-model/24085\n",
            "1607: Configure RobertaTokenizer - https://discuss.huggingface.co/t/configure-robertatokenizer/23969\n",
            "1608: How to properly clean vocabulary from BBPE tokenizer - https://discuss.huggingface.co/t/how-to-properly-clean-vocabulary-from-bbpe-tokenizer/22827\n",
            "1609: Map tokenization and posterior to smaller substrings - https://discuss.huggingface.co/t/map-tokenization-and-posterior-to-smaller-substrings/23792\n",
            "1610: T5 model tokenizer - https://discuss.huggingface.co/t/t5-model-tokenizer/23640\n",
            "1611: Fast tokenizer for marianMTModel - https://discuss.huggingface.co/t/fast-tokenizer-for-marianmtmodel/23638\n",
            "1612: Word tokenizers for text generators - https://discuss.huggingface.co/t/word-tokenizers-for-text-generators/23464\n",
            "1613: SentencePieceUnigramTokenizer - https://discuss.huggingface.co/t/sentencepieceunigramtokenizer/23509\n",
            "1614: Tokenizer is not being loaded on Huggingface Inference - https://discuss.huggingface.co/t/tokenizer-is-not-being-loaded-on-huggingface-inference/23505\n",
            "1615: Why is BertNormalizer not exposed on the tokenizers library? - https://discuss.huggingface.co/t/why-is-bertnormalizer-not-exposed-on-the-tokenizers-library/23340\n",
            "1616: Sentence splitting - https://discuss.huggingface.co/t/sentence-splitting/5393\n",
            "1617: Average time to train a SentencePieceBPETokenizer - https://discuss.huggingface.co/t/average-time-to-train-a-sentencepiecebpetokenizer/23081\n",
            "1618: 1 line code for NER data set preparation using tokenizer library! - https://discuss.huggingface.co/t/1-line-code-for-ner-data-set-preparation-using-tokenizer-library/22816\n",
            "1619: Microsoft/codebert-base produces two sep tokens - https://discuss.huggingface.co/t/microsoft-codebert-base-produces-two-sep-tokens/21366\n",
            "1620: Padding with sliding window - https://discuss.huggingface.co/t/padding-with-sliding-window/18921\n",
            "1621: Find which tokens are unknown in new data - https://discuss.huggingface.co/t/find-which-tokens-are-unknown-in-new-data/22466\n",
            "1622: How to train target tokenizer - https://discuss.huggingface.co/t/how-to-train-target-tokenizer/22291\n",
            "1623: How to know if a subtoken is a word or part of a word? - https://discuss.huggingface.co/t/how-to-know-if-a-subtoken-is-a-word-or-part-of-a-word/923\n",
            "1624: BART Tokenizer tokenises same word differently? - https://discuss.huggingface.co/t/bart-tokenizer-tokenises-same-word-differently/21835\n",
            "1625: Fine-tuned BERT tokenizer taking too long to load - https://discuss.huggingface.co/t/fine-tuned-bert-tokenizer-taking-too-long-to-load/9747\n",
            "1626: Add BOS and EOS when encoding a sentence - https://discuss.huggingface.co/t/add-bos-and-eos-when-encoding-a-sentence/21833\n",
            "1627: Customization of Wav2Vec2CTCTokenizer with rules - https://discuss.huggingface.co/t/customization-of-wav2vec2ctctokenizer-with-rules/21912\n",
            "1628: Customized tokenization files in run_clm script - https://discuss.huggingface.co/t/customized-tokenization-files-in-run-clm-script/21460\n",
            "1629: Using customized algorithm - https://discuss.huggingface.co/t/using-customized-algorithm/21753\n",
            "1630: Issue with Flaubert Tokenizer as word_ids() method is not available for NER Task - https://discuss.huggingface.co/t/issue-with-flaubert-tokenizer-as-word-ids-method-is-not-available-for-ner-task/20374\n",
            "1631: Word_ids not working with deberta_v2 - https://discuss.huggingface.co/t/word-ids-not-working-with-deberta-v2/21523\n",
            "1632: How to tokenize large contexts without running out of memory - https://discuss.huggingface.co/t/how-to-tokenize-large-contexts-without-running-out-of-memory/5882\n",
            "1633: Does Deberta tokenizer use wordpiece? - https://discuss.huggingface.co/t/does-deberta-tokenizer-use-wordpiece/21307\n",
            "1634: Get vocabulary tokens in order to exclude them from generate function - https://discuss.huggingface.co/t/get-vocabulary-tokens-in-order-to-exclude-them-from-generate-function/5192\n",
            "1635: Avoid creating certain tokens when training a tokenizer - https://discuss.huggingface.co/t/avoid-creating-certain-tokens-when-training-a-tokenizer/20864\n",
            "1636: Error finetuning XLM-RoBERTa-Large when training - https://discuss.huggingface.co/t/error-finetuning-xlm-roberta-large-when-training/20412\n",
            "1637: HuggingFace BPE Trainer Error - Training Tokenizer - https://discuss.huggingface.co/t/huggingface-bpe-trainer-error-training-tokenizer/10629\n",
            "1638: Word_to_tokens() and word_ids() —- microsoft/deberta-v2/v3 - https://discuss.huggingface.co/t/word-to-tokens-and-word-ids-microsoft-deberta-v2-v3/19984\n",
            "1639: No PreTrainedTokenizerFast for Deberta-V3, no doc_stride - https://discuss.huggingface.co/t/no-pretrainedtokenizerfast-for-deberta-v3-no-doc-stride/20345\n",
            "1640: Tokenizer from own vocab - https://discuss.huggingface.co/t/tokenizer-from-own-vocab/20245\n",
            "1641: No labels column for tokenized data - https://discuss.huggingface.co/t/no-labels-column-for-tokenized-data/19540\n",
            "1642: Programmatic way to Tokenization on Custom Text Columns - https://discuss.huggingface.co/t/programmatic-way-to-tokenization-on-custom-text-columns/19679\n",
            "1643: Bug in Offset generation for Rupee symbol - https://discuss.huggingface.co/t/bug-in-offset-generation-for-rupee-symbol/19655\n",
            "1644: How to handle parenthesis, quotation marks, \\n etc when creating tokenizer from scratch - https://discuss.huggingface.co/t/how-to-handle-parenthesis-quotation-marks-n-etc-when-creating-tokenizer-from-scratch/19628\n",
            "1645: EM training on unigram tokenizer taking way longer than predicted - https://discuss.huggingface.co/t/em-training-on-unigram-tokenizer-taking-way-longer-than-predicted/19502\n",
            "1646: Training unigram on long sequences - https://discuss.huggingface.co/t/training-unigram-on-long-sequences/19188\n",
            "1647: Issue with post-processing - https://discuss.huggingface.co/t/issue-with-post-processing/2703\n",
            "1648: FutureWarning about BertTokenizer.from_pretrained() at latest version - https://discuss.huggingface.co/t/futurewarning-about-berttokenizer-from-pretrained-at-latest-version/18750\n",
            "1649: Enhaced word_ids() API for Chinese or CJK languages? - https://discuss.huggingface.co/t/enhaced-word-ids-api-for-chinese-or-cjk-languages/18662\n",
            "1650: Importing tokenizers version >0.10.3 fails due to openssl - https://discuss.huggingface.co/t/importing-tokenizers-version-0-10-3-fails-due-to-openssl/17820\n",
            "1651: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "1652: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "1653: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "1654: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "1655: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "1656: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "1657: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "1658: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "1659: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "1660: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "1661: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "1662: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "1663: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "1664: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "1665: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "1666: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "1667: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "1668: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "1669: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "1670: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "1671: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "1672: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "1673: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "1674: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "1675: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "1676: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "1677: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "1678: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "1679: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "1680: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "1681: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "1682: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "1683: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "1684: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "1685: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "1686: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "1687: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "1688: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "1689: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "1690: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "1691: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "1692: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "1693: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "1694: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "1695: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "1696: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "1697: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "1698: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "1699: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "1700: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "1701: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "1702: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "1703: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "1704: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "1705: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "1706: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "1707: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "1708: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "1709: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "1710: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "1711: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "1712: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "1713: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "1714: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "1715: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "1716: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "1717: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "1718: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "1719: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "1720: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "1721: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "1722: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "1723: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "1724: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "1725: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "1726: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "1727: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "1728: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "1729: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "1730: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "1731: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "1732: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "1733: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "1734: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "1735: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "1736: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "1737: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "1738: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "1739: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "1740: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "1741: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "1742: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "1743: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "1744: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "1745: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "1746: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "1747: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "1748: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "1749: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "1750: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "1751: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "1752: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "1753: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "1754: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "1755: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "1756: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "1757: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "1758: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "1759: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "1760: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "1761: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "1762: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "1763: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "1764: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "1765: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "1766: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "1767: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "1768: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "1769: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "1770: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "1771: BertTokenizer.decode not understanding new vocabulary - https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "1772: Tokenizer tend to choose added tokens first rather than token in vocab - https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "1773: Special token printed out as output - https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948\n",
            "1774: [NER][Japanese] labeled segment shorter than token - https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "1775: T5Tokenizer add a whitespace token after added special tokens - https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "1776: Unable to register my own tokenizer - https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "1777: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab! - https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "1778: Use Unicode blocks in regex (in Replace normalizer) - https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904\n",
            "1779: How to handle translations one source language to many target sentences for the same language - https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "1780: NER Label tokenization with overflowing tokens - https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561\n",
            "1781: How to use a trained tokenizer for semantic search? - https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "1782: Unk_token not set after training a BPETokenizer tokenizer - https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733\n",
            "1783: AutoTokenizer is very slow when loading llama tokenizer - https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547\n",
            "1784: Offset mappings differ for tokenizers - https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424\n",
            "1785: The process for tokenizing concatenated dataset is slow st the end of tokenizing - https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "1786: Batch tokenize (split into tokens, without processing) - https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238\n",
            "1787: Does AutoTokenizer uploads data to HuggingFace - https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "1788: RobertaTokenizer decode and tokenize do not have the same output - https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "1789: Training tokenizers with padding in between tokens - https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "1790: Leaving unknown words untokenized like in OpenMNT - https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "1791: Keeping special chars in translations - https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270\n",
            "1792: Should cls_token be [CLS] or <cls>? - https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "1793: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False - https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "1794: I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error - https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742\n",
            "1795: OSError: Can’t load tokenizer for ‘facebook/xmod-base’ - https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471\n",
            "1796: `GPT2Tokenizer` Tokenizer handling `\\n\\n` differently in different settings - https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289\n",
            "1797: DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‘padding=True’ ‘truncation=True’ to have batched tensors with the same length - https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625\n",
            "1798: Decode token IDs into a list (not a single string) - https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991\n",
            "1799: Error training MLM with Roberta Tokenizer - https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878\n",
            "1800: Are the slow and fast tokenizer results the same output for the same input? - https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "1801: “Add_tokens” breaks words when encoding - https://discuss.huggingface.co/t/add-tokens-breaks-words-when-encoding/21154\n",
            "1802: 2 tokens for one character in T5 - https://discuss.huggingface.co/t/2-tokens-for-one-character-in-t5/14960\n",
            "1803: OSError: Model name ‘gpt2’ was not found in tokenizers model name list (gpt2,…) - https://discuss.huggingface.co/t/oserror-model-name-gpt2-was-not-found-in-tokenizers-model-name-list-gpt2/2164\n",
            "1804: DNA long sequence tokenization - https://discuss.huggingface.co/t/dna-long-sequence-tokenization/16154\n",
            "1805: SentencePiece tokenizer encodes to unknown token - https://discuss.huggingface.co/t/sentencepiece-tokenizer-encodes-to-unknown-token/49113\n",
            "1806: Tokenizer behaviour with pipeline - https://discuss.huggingface.co/t/tokenizer-behaviour-with-pipeline/48969\n",
            "1807: Load tokenizer from file : Exception: data did not match any variant of untagged enum ModelWrapper - https://discuss.huggingface.co/t/load-tokenizer-from-file-exception-data-did-not-match-any-variant-of-untagged-enum-modelwrapper/25325\n",
            "1808: ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076 - https://discuss.huggingface.co/t/arrowinvalid-column-3-named-attention-mask-expected-length-1000-but-got-length-1076/6904\n",
            "1809: Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model - https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829\n",
            "1810: Initialize Vocabulary for Unigram Tokenizer - https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371\n",
            "1811: Make correct padding for text generation with GPT-NEO - https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800\n",
            "1812: How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers? - https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639\n",
            "1813: Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF - https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602\n",
            "1814: Writing custom tokenizer and wrapping it in tokenizer object - https://discuss.huggingface.co/t/writing-custom-tokenizer-and-wrapping-it-in-tokenizer-object/39679\n",
            "1815: Tokenizer for German lang - https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222\n",
            "1816: Chunk tokens into desired chunk length without simply getting rid of rest of tokens - https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399\n",
            "1817: Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers - https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872\n",
            "1818: LlamaTokenizerFast returns token_type_ids but the forward pass of the LlamaModel does not receive token_type_ids - https://discuss.huggingface.co/t/llamatokenizerfast-returns-token-type-ids-but-the-forward-pass-of-the-llamamodel-does-not-receive-token-type-ids/42431\n",
            "1819: GPT2Tokenizer not working in Kaggle Notebook - https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508\n",
            "1820: Exploring the Majestic Temples in Karnataka - https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952\n",
            "1821: Tokenizer producing token index greater than size of the dictionary - https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989\n",
            "1822: How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer - https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843\n",
            "1823: How to create a HF tokenizer’s vocab file from a BPE model’s merges.txt file? - https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737\n",
            "1824: Scala/JVM Bindings for Tokenizers - https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393\n",
            "1825: Tokenizers Wheel Takes Forever to Build - https://discuss.huggingface.co/t/tokenizers-wheel-takes-forever-to-build/17114\n",
            "1826: Where the introduction of tokenizers.implementations? - https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959\n",
            "1827: How to return custom `token_type_ids` or other values from a tokenizer? - https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525\n",
            "1828: Easy way to compare tokenizers - https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347\n",
            "1829: Unable to load image using llama-index - https://discuss.huggingface.co/t/unable-to-load-image-using-llama-index/38304\n",
            "1830: Help defining tokenizer - https://discuss.huggingface.co/t/help-defining-tokenizer/38091\n",
            "1831: Token Offsets in Rust vs. Python - https://discuss.huggingface.co/t/token-offsets-in-rust-vs-python/37949\n",
            "1832: “OSError: Model name ‘./XX’ was not found in tokenizers model name list” - cannot load custom tokenizer in Transformers - https://discuss.huggingface.co/t/oserror-model-name-xx-was-not-found-in-tokenizers-model-name-list-cannot-load-custom-tokenizer-in-transformers/2714\n",
            "1833: Converting JSON/dict to flatten string with indicator tokens - https://discuss.huggingface.co/t/converting-json-dict-to-flatten-string-with-indicator-tokens/37387\n",
            "1834: Train Retry Tokenizer - https://discuss.huggingface.co/t/train-retry-tokenizer/37031\n",
            "1835: Pretokenise on punctuation except hyphens - https://discuss.huggingface.co/t/pretokenise-on-punctuation-except-hyphens/36691\n",
            "1836: Tokenizer Trainer Crashing - https://discuss.huggingface.co/t/tokenizer-trainer-crashing/36645\n",
            "1837: Tokenizer extremely slow when deployed to a container - https://discuss.huggingface.co/t/tokenizer-extremely-slow-when-deployed-to-a-container/36569\n",
            "1838: Dealing with Decimal and Fractions - https://discuss.huggingface.co/t/dealing-with-decimal-and-fractions/23377\n",
            "1839: `add_tokens` with argument `special_tokens=True` vs `add_special_tokens` - https://discuss.huggingface.co/t/add-tokens-with-argument-special-tokens-true-vs-add-special-tokens/35648\n",
            "1840: Unable to upload custom Pytorch model in huggingface - https://discuss.huggingface.co/t/unable-to-upload-custom-pytorch-model-in-huggingface/35545\n",
            "1841: RuntimeError: Cannot re-initialize CUDA in forked subprocess - https://discuss.huggingface.co/t/runtimeerror-cannot-re-initialize-cuda-in-forked-subprocess/28392\n",
            "1842: Overflowing Tokens in MarkupLM - https://discuss.huggingface.co/t/overflowing-tokens-in-markuplm/35217\n",
            "1843: I get the predicted token as ` े` . What am I doing wrong? - https://discuss.huggingface.co/t/i-get-the-predicted-token-as-what-am-i-doing-wrong/29038\n",
            "1844: <unk> token in the output instead curly braces - https://discuss.huggingface.co/t/unk-token-in-the-output-instead-curly-braces/34653\n",
            "1845: How to add a new token without expanding the vocabulary - https://discuss.huggingface.co/t/how-to-add-a-new-token-without-expanding-the-vocabulary/34536\n",
            "1846: Does the ByteLevelBPETokenizer need to be wrapped in a normal Tokenizer? - https://discuss.huggingface.co/t/does-the-bytelevelbpetokenizer-need-to-be-wrapped-in-a-normal-tokenizer/34121\n",
            "1847: What is required to create a fast tokenizer? For example for a Marian model - https://discuss.huggingface.co/t/what-is-required-to-create-a-fast-tokenizer-for-example-for-a-marian-model/33941\n",
            "1848: GPT2Tokenizer.decode maps unicode sequences to the same string ‘�’ - https://discuss.huggingface.co/t/gpt2tokenizer-decode-maps-unicode-sequences-to-the-same-string/32453\n",
            "1849: Issue with Tokenizer - https://discuss.huggingface.co/t/issue-with-tokenizer/33796\n",
            "1850: Tokenizing my novel for GPT model - https://discuss.huggingface.co/t/tokenizing-my-novel-for-gpt-model/33532\n",
            "1851: How to add additional custom pre-tokenization processing? - https://discuss.huggingface.co/t/how-to-add-additional-custom-pre-tokenization-processing/1637\n",
            "1852: Customize FlauBERT tokenizer to split line breaks - https://discuss.huggingface.co/t/customize-flaubert-tokenizer-to-split-line-breaks/33011\n",
            "1853: How to change the size of model_max_length? - https://discuss.huggingface.co/t/how-to-change-the-size-of-model-max-length/32942\n",
            "1854: Can’t get to the source code of `tokenizer.convert_tokens_to_string` - https://discuss.huggingface.co/t/cant-get-to-the-source-code-of-tokenizer-convert-tokens-to-string/32714\n",
            "1855: Why I’m getting same result with or without using Wav2Vec2Processor? - https://discuss.huggingface.co/t/why-im-getting-same-result-with-or-without-using-wav2vec2processor/32482\n",
            "1856: How does `tokenizer().input_ids` work and how different it is from tokenizer.encode() before `model.generate()` and decoding step? - https://discuss.huggingface.co/t/how-does-tokenizer-input-ids-work-and-how-different-it-is-from-tokenizer-encode-before-model-generate-and-decoding-step/30476\n",
            "1857: What file type should my training data be? - https://discuss.huggingface.co/t/what-file-type-should-my-training-data-be/32075\n",
            "1858: Best way to get the closest token indices of input of char_to_token is a whitespace - https://discuss.huggingface.co/t/best-way-to-get-the-closest-token-indices-of-input-of-char-to-token-is-a-whitespace/32004\n",
            "1859: Token indices sequence length is longer than the specified maximum sequence length - https://discuss.huggingface.co/t/token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length/25133\n",
            "1860: Create a simple tokenizer - https://discuss.huggingface.co/t/create-a-simple-tokenizer/31677\n",
            "1861: Sliding window for Long Documents - https://discuss.huggingface.co/t/sliding-window-for-long-documents/19367\n",
            "1862: Creating tokenizer from counts file? - https://discuss.huggingface.co/t/creating-tokenizer-from-counts-file/31399\n",
            "1863: Tokenizer.train() running out of memory - https://discuss.huggingface.co/t/tokenizer-train-running-out-of-memory/31355\n",
            "1864: Tokenizing Float Tensor? - https://discuss.huggingface.co/t/tokenizing-float-tensor/30604\n",
            "1865: Padding and truncation for custom tokenizer - https://discuss.huggingface.co/t/padding-and-truncation-for-custom-tokenizer/30180\n",
            "1866: Incorporate SARI score into run_summarization.py example script - https://discuss.huggingface.co/t/incorporate-sari-score-into-run-summarization-py-example-script/29511\n",
            "1867: Is that possible to embed the tokenizer into the model to have it running on GCP using TensorFlow Serving? - https://discuss.huggingface.co/t/is-that-possible-to-embed-the-tokenizer-into-the-model-to-have-it-running-on-gcp-using-tensorflow-serving/10532\n",
            "1868: Huggingface inference API issue - https://discuss.huggingface.co/t/huggingface-inference-api-issue/29307\n",
            "1869: Using Tokenizer for integer data - https://discuss.huggingface.co/t/using-tokenizer-for-integer-data/28835\n",
            "1870: GPT2 long text approach - https://discuss.huggingface.co/t/gpt2-long-text-approach/28165\n",
            "1871: Huggingface t5 models seem to not download a tokenizer file - https://discuss.huggingface.co/t/huggingface-t5-models-seem-to-not-download-a-tokenizer-file/27935\n",
            "1872: How to save a fast tokenizer using the transformer library and then load it using Tokenizers? - https://discuss.huggingface.co/t/how-to-save-a-fast-tokenizer-using-the-transformer-library-and-then-load-it-using-tokenizers/9567\n",
            "1873: Using a BertTokenizer when training a RobertaForMaskedLM - https://discuss.huggingface.co/t/using-a-berttokenizer-when-training-a-robertaformaskedlm/27456\n",
            "1874: Need clarity on “padding” parameter in Bert Tokenizer - https://discuss.huggingface.co/t/need-clarity-on-padding-parameter-in-bert-tokenizer/27444\n",
            "1875: How to convert HuggingFace tokenizers into ONNX format? - https://discuss.huggingface.co/t/how-to-convert-huggingface-tokenizers-into-onnx-format/27272\n",
            "1876: Can’t save ConvBert tokenizer - https://discuss.huggingface.co/t/cant-save-convbert-tokenizer/16006\n",
            "1877: RoBERTa Tokenizer Java Implementation - https://discuss.huggingface.co/t/roberta-tokenizer-java-implementation/16737\n",
            "1878: Unigram vocab_size doesn’t fit - https://discuss.huggingface.co/t/unigram-vocab-size-doesnt-fit/26856\n",
            "1879: Option to load only tokenizer and model configuration into “token-classification” pipeline - https://discuss.huggingface.co/t/option-to-load-only-tokenizer-and-model-configuration-into-token-classification-pipeline/26697\n",
            "1880: Encode_plus Pretokenized input seuqence must be Union - https://discuss.huggingface.co/t/encode-plus-pretokenized-input-seuqence-must-be-union/26449\n",
            "1881: Application of TFBertTokenizer - https://discuss.huggingface.co/t/application-of-tfberttokenizer/26437\n",
            "1882: TemplateProcessing for encoder-decoder - https://discuss.huggingface.co/t/templateprocessing-for-encoder-decoder/26135\n",
            "1883: Using `TFBertTokenizer` instead of `BertTokenizer` with `TFBertForQuestionAnswering` - https://discuss.huggingface.co/t/using-tfberttokenizer-instead-of-berttokenizer-with-tfbertforquestionanswering/26127\n",
            "1884: How to concatenate an answer to multiple choices after padded tokenization - https://discuss.huggingface.co/t/how-to-concatenate-an-answer-to-multiple-choices-after-padded-tokenization/26110\n",
            "1885: Maximum recursion depth exceeded when using DataCollator - https://discuss.huggingface.co/t/maximum-recursion-depth-exceeded-when-using-datacollator/25937\n",
            "1886: Adding a special language token to MBART - https://discuss.huggingface.co/t/adding-a-special-language-token-to-mbart/25967\n",
            "1887: Custom PostProcessor? - https://discuss.huggingface.co/t/custom-postprocessor/25847\n",
            "1888: Tokenizer.pad_token=what? - https://discuss.huggingface.co/t/tokenizer-pad-token-what/24367\n",
            "1889: Using HuggingFace Tokenizers Without Special Characters - https://discuss.huggingface.co/t/using-huggingface-tokenizers-without-special-characters/21962\n",
            "1890: How to get sp_model variable from T5Tokenizer? - https://discuss.huggingface.co/t/how-to-get-sp-model-variable-from-t5tokenizer/25171\n",
            "1891: Wav2vec2CTCTokenizer and vocab.json - https://discuss.huggingface.co/t/wav2vec2ctctokenizer-and-vocab-json/25146\n",
            "1892: Period ID in RobertaTokenizer with is_split_into_words - https://discuss.huggingface.co/t/period-id-in-robertatokenizer-with-is-split-into-words/24142\n",
            "1893: WordLevel error: Missing [UNK] token from the vocabulary - https://discuss.huggingface.co/t/wordlevel-error-missing-unk-token-from-the-vocabulary/5107\n",
            "1894: Tokenizer post_processor help - https://discuss.huggingface.co/t/tokenizer-post-processor-help/21926\n",
            "1895: Preprocessing raw text - https://discuss.huggingface.co/t/preprocessing-raw-text/24559\n",
            "1896: Save tokenizer with argument - https://discuss.huggingface.co/t/save-tokenizer-with-argument/17389\n",
            "1897: Trained tokenizer API as PretrainedTokenizer - https://discuss.huggingface.co/t/trained-tokenizer-api-as-pretrainedtokenizer/23639\n",
            "1898: Remove only certain special token id during tokenizer decode - https://discuss.huggingface.co/t/remove-only-certain-special-token-id-during-tokenizer-decode/20436\n",
            "1899: Convert_tokens_to_ids produces <unk> - https://discuss.huggingface.co/t/convert-tokens-to-ids-produces-unk/24771\n",
            "1900: Text preprocessing for fitting Tokenizer model - https://discuss.huggingface.co/t/text-preprocessing-for-fitting-tokenizer-model/24869\n",
            "1901: Special tokens warning - https://discuss.huggingface.co/t/special-tokens-warning/24939\n",
            "1902: Simple Transformers Multilabelclassification - https://discuss.huggingface.co/t/simple-transformers-multilabelclassification/24568\n",
            "1903: Cannot initialize deberta-v3-base tokenizer - https://discuss.huggingface.co/t/cannot-initialize-deberta-v3-base-tokenizer/15322\n",
            "1904: Getting Wholeword corresponding to a subword in a text? - https://discuss.huggingface.co/t/getting-wholeword-corresponding-to-a-subword-in-a-text/24152\n",
            "1905: Issue with pushing tokenizer to hub - https://discuss.huggingface.co/t/issue-with-pushing-tokenizer-to-hub/24120\n",
            "1906: How do we customize the number of entites for NER pretrained model? - https://discuss.huggingface.co/t/how-do-we-customize-the-number-of-entites-for-ner-pretrained-model/24085\n",
            "1907: Configure RobertaTokenizer - https://discuss.huggingface.co/t/configure-robertatokenizer/23969\n",
            "1908: How to properly clean vocabulary from BBPE tokenizer - https://discuss.huggingface.co/t/how-to-properly-clean-vocabulary-from-bbpe-tokenizer/22827\n",
            "1909: Map tokenization and posterior to smaller substrings - https://discuss.huggingface.co/t/map-tokenization-and-posterior-to-smaller-substrings/23792\n",
            "1910: T5 model tokenizer - https://discuss.huggingface.co/t/t5-model-tokenizer/23640\n",
            "1911: Fast tokenizer for marianMTModel - https://discuss.huggingface.co/t/fast-tokenizer-for-marianmtmodel/23638\n",
            "1912: Word tokenizers for text generators - https://discuss.huggingface.co/t/word-tokenizers-for-text-generators/23464\n",
            "1913: SentencePieceUnigramTokenizer - https://discuss.huggingface.co/t/sentencepieceunigramtokenizer/23509\n",
            "1914: Tokenizer is not being loaded on Huggingface Inference - https://discuss.huggingface.co/t/tokenizer-is-not-being-loaded-on-huggingface-inference/23505\n",
            "1915: Why is BertNormalizer not exposed on the tokenizers library? - https://discuss.huggingface.co/t/why-is-bertnormalizer-not-exposed-on-the-tokenizers-library/23340\n",
            "1916: Sentence splitting - https://discuss.huggingface.co/t/sentence-splitting/5393\n",
            "1917: Average time to train a SentencePieceBPETokenizer - https://discuss.huggingface.co/t/average-time-to-train-a-sentencepiecebpetokenizer/23081\n",
            "1918: 1 line code for NER data set preparation using tokenizer library! - https://discuss.huggingface.co/t/1-line-code-for-ner-data-set-preparation-using-tokenizer-library/22816\n",
            "1919: Microsoft/codebert-base produces two sep tokens - https://discuss.huggingface.co/t/microsoft-codebert-base-produces-two-sep-tokens/21366\n",
            "1920: Padding with sliding window - https://discuss.huggingface.co/t/padding-with-sliding-window/18921\n",
            "1921: Find which tokens are unknown in new data - https://discuss.huggingface.co/t/find-which-tokens-are-unknown-in-new-data/22466\n",
            "1922: How to train target tokenizer - https://discuss.huggingface.co/t/how-to-train-target-tokenizer/22291\n",
            "1923: How to know if a subtoken is a word or part of a word? - https://discuss.huggingface.co/t/how-to-know-if-a-subtoken-is-a-word-or-part-of-a-word/923\n",
            "1924: BART Tokenizer tokenises same word differently? - https://discuss.huggingface.co/t/bart-tokenizer-tokenises-same-word-differently/21835\n",
            "1925: Fine-tuned BERT tokenizer taking too long to load - https://discuss.huggingface.co/t/fine-tuned-bert-tokenizer-taking-too-long-to-load/9747\n",
            "1926: Add BOS and EOS when encoding a sentence - https://discuss.huggingface.co/t/add-bos-and-eos-when-encoding-a-sentence/21833\n",
            "1927: Customization of Wav2Vec2CTCTokenizer with rules - https://discuss.huggingface.co/t/customization-of-wav2vec2ctctokenizer-with-rules/21912\n",
            "1928: Customized tokenization files in run_clm script - https://discuss.huggingface.co/t/customized-tokenization-files-in-run-clm-script/21460\n",
            "1929: Using customized algorithm - https://discuss.huggingface.co/t/using-customized-algorithm/21753\n",
            "1930: Issue with Flaubert Tokenizer as word_ids() method is not available for NER Task - https://discuss.huggingface.co/t/issue-with-flaubert-tokenizer-as-word-ids-method-is-not-available-for-ner-task/20374\n",
            "1931: Word_ids not working with deberta_v2 - https://discuss.huggingface.co/t/word-ids-not-working-with-deberta-v2/21523\n",
            "1932: How to tokenize large contexts without running out of memory - https://discuss.huggingface.co/t/how-to-tokenize-large-contexts-without-running-out-of-memory/5882\n",
            "1933: Does Deberta tokenizer use wordpiece? - https://discuss.huggingface.co/t/does-deberta-tokenizer-use-wordpiece/21307\n",
            "1934: Get vocabulary tokens in order to exclude them from generate function - https://discuss.huggingface.co/t/get-vocabulary-tokens-in-order-to-exclude-them-from-generate-function/5192\n",
            "1935: Avoid creating certain tokens when training a tokenizer - https://discuss.huggingface.co/t/avoid-creating-certain-tokens-when-training-a-tokenizer/20864\n",
            "1936: Error finetuning XLM-RoBERTa-Large when training - https://discuss.huggingface.co/t/error-finetuning-xlm-roberta-large-when-training/20412\n",
            "1937: HuggingFace BPE Trainer Error - Training Tokenizer - https://discuss.huggingface.co/t/huggingface-bpe-trainer-error-training-tokenizer/10629\n",
            "1938: Word_to_tokens() and word_ids() —- microsoft/deberta-v2/v3 - https://discuss.huggingface.co/t/word-to-tokens-and-word-ids-microsoft-deberta-v2-v3/19984\n",
            "1939: No PreTrainedTokenizerFast for Deberta-V3, no doc_stride - https://discuss.huggingface.co/t/no-pretrainedtokenizerfast-for-deberta-v3-no-doc-stride/20345\n",
            "1940: Tokenizer from own vocab - https://discuss.huggingface.co/t/tokenizer-from-own-vocab/20245\n",
            "1941: No labels column for tokenized data - https://discuss.huggingface.co/t/no-labels-column-for-tokenized-data/19540\n",
            "1942: Programmatic way to Tokenization on Custom Text Columns - https://discuss.huggingface.co/t/programmatic-way-to-tokenization-on-custom-text-columns/19679\n",
            "1943: Bug in Offset generation for Rupee symbol - https://discuss.huggingface.co/t/bug-in-offset-generation-for-rupee-symbol/19655\n",
            "1944: How to handle parenthesis, quotation marks, \\n etc when creating tokenizer from scratch - https://discuss.huggingface.co/t/how-to-handle-parenthesis-quotation-marks-n-etc-when-creating-tokenizer-from-scratch/19628\n",
            "1945: EM training on unigram tokenizer taking way longer than predicted - https://discuss.huggingface.co/t/em-training-on-unigram-tokenizer-taking-way-longer-than-predicted/19502\n",
            "1946: Training unigram on long sequences - https://discuss.huggingface.co/t/training-unigram-on-long-sequences/19188\n",
            "1947: Issue with post-processing - https://discuss.huggingface.co/t/issue-with-post-processing/2703\n",
            "1948: FutureWarning about BertTokenizer.from_pretrained() at latest version - https://discuss.huggingface.co/t/futurewarning-about-berttokenizer-from-pretrained-at-latest-version/18750\n",
            "1949: Enhaced word_ids() API for Chinese or CJK languages? - https://discuss.huggingface.co/t/enhaced-word-ids-api-for-chinese-or-cjk-languages/18662\n",
            "1950: Importing tokenizers version >0.10.3 fails due to openssl - https://discuss.huggingface.co/t/importing-tokenizers-version-0-10-3-fails-due-to-openssl/17820\n",
            "1951: Lower case with input ids - https://discuss.huggingface.co/t/lower-case-with-input-ids/18490\n",
            "1952: Dialogue classification - https://discuss.huggingface.co/t/dialogue-classification/18466\n",
            "1953: Multilang bert vs translating to english - https://discuss.huggingface.co/t/multilang-bert-vs-translating-to-english/18454\n",
            "1954: pyo3_runtime.PanicException: likelihood is NAN. Input sentence may be too long - https://discuss.huggingface.co/t/pyo3-runtime-panicexception-likelihood-is-nan-input-sentence-may-be-too-long/18398\n",
            "1955: Pytorch_model.bin not working because of lfs - https://discuss.huggingface.co/t/pytorch-model-bin-not-working-because-of-lfs/18290\n",
            "1956: Tokenizer ignores repeated whitespaces - https://discuss.huggingface.co/t/tokenizer-ignores-repeated-whitespaces/17864\n",
            "1957: How truncation works when applying BERT tokenizer on the batch of sentence pairs in HuggingFace? - https://discuss.huggingface.co/t/how-truncation-works-when-applying-bert-tokenizer-on-the-batch-of-sentence-pairs-in-huggingface/17977\n",
            "1958: How to save a tokenizer only consisting of added tokens - https://discuss.huggingface.co/t/how-to-save-a-tokenizer-only-consisting-of-added-tokens/17825\n",
            "1959: Passing list of inputs to tokenize - https://discuss.huggingface.co/t/passing-list-of-inputs-to-tokenize/17499\n",
            "1960: Issues with Data Collator and Tokenizing with NER Datasets - https://discuss.huggingface.co/t/issues-with-data-collator-and-tokenizing-with-ner-datasets/17489\n",
            "1961: How to perform tokenization on an ONNX model in JS? - https://discuss.huggingface.co/t/how-to-perform-tokenization-on-an-onnx-model-in-js/17581\n",
            "1962: Using the Tokenizers library in a Unity project - https://discuss.huggingface.co/t/using-the-tokenizers-library-in-a-unity-project/17533\n",
            "1963: Further pre-training the tokenizer? - https://discuss.huggingface.co/t/further-pre-training-the-tokenizer/17360\n",
            "1964: Error when doing tokenization - https://discuss.huggingface.co/t/error-when-doing-tokenization/17344\n",
            "1965: How to decode with spaces? - https://discuss.huggingface.co/t/how-to-decode-with-spaces/17289\n",
            "1966: Show Submodels of PegasusTokenizer - https://discuss.huggingface.co/t/show-submodels-of-pegasustokenizer/17286\n",
            "1967: Load SentencePieceBPETokenizer in TF - https://discuss.huggingface.co/t/load-sentencepiecebpetokenizer-in-tf/17257\n",
            "1968: Best way to mask a multi-token word when using `.*ForMaskedLM` models - https://discuss.huggingface.co/t/best-way-to-mask-a-multi-token-word-when-using-formaskedlm-models/6428\n",
            "1969: Does a tokenizer keep the mapping between my labels to their encoding? - https://discuss.huggingface.co/t/does-a-tokenizer-keep-the-mapping-between-my-labels-to-their-encoding/16296\n",
            "1970: What is Wav2Vec2FeatureExtractor doing? - https://discuss.huggingface.co/t/what-is-wav2vec2featureextractor-doing/16423\n",
            "1971: What does this warning mean? -overflowing tokens are not returned for the setting you have chosen - https://discuss.huggingface.co/t/what-does-this-warning-mean-overflowing-tokens-are-not-returned-for-the-setting-you-have-chosen/11594\n",
            "1972: How can I make sure Tokenizer pads to a fixed length? - https://discuss.huggingface.co/t/how-can-i-make-sure-tokenizer-pads-to-a-fixed-length/16116\n",
            "1973: Issue with Decoding in HuggingFace - https://discuss.huggingface.co/t/issue-with-decoding-in-huggingface/15699\n",
            "1974: ValueError: Unable to create tensor for 1 dataset but not the other of same type - https://discuss.huggingface.co/t/valueerror-unable-to-create-tensor-for-1-dataset-but-not-the-other-of-same-type/15995\n",
            "1975: Disabling addition of CLS from BERT tokenizer - https://discuss.huggingface.co/t/disabling-addition-of-cls-from-bert-tokenizer/15538\n",
            "1976: Tokenized sequence lengths - https://discuss.huggingface.co/t/tokenized-sequence-lengths/15117\n",
            "1977: Finetuning GPT-J6B for custom dataset - https://discuss.huggingface.co/t/finetuning-gpt-j6b-for-custom-dataset/9924\n",
            "1978: Training tokenizer takes too much RAM - https://discuss.huggingface.co/t/training-tokenizer-takes-too-much-ram/14256\n",
            "1979: How to “further pretrain” a tokenizer (do I need to do so?) - https://discuss.huggingface.co/t/how-to-further-pretrain-a-tokenizer-do-i-need-to-do-so/14719\n",
            "1980: Run_seq2seq_qa.py: Column 3 named labels expected length 1007 but got length 1000 - https://discuss.huggingface.co/t/run-seq2seq-qa-py-column-3-named-labels-expected-length-1007-but-got-length-1000/13237\n",
            "1981: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "1982: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "1983: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "1984: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "1985: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "1986: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "1987: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "1988: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "1989: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "1990: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "1991: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "1992: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "1993: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "1994: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "1995: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "1996: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "1997: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "1998: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "1999: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "2000: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "2001: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "2002: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "2003: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "2004: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "2005: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "2006: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "2007: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "2008: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "2009: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "2010: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "2011: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "2012: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "2013: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "2014: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "2015: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "2016: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "2017: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "2018: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "2019: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "2020: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "2021: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "2022: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "2023: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "2024: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "2025: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "2026: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "2027: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "2028: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "2029: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "2030: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "2031: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "2032: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "2033: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "2034: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "2035: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "2036: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "2037: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "2038: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "2039: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "2040: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "2041: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "2042: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "2043: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "2044: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "2045: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "2046: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "2047: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "2048: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "2049: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "2050: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "2051: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "2052: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "2053: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "2054: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "2055: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "2056: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "2057: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "2058: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "2059: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "2060: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "2061: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "2062: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "2063: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "2064: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "2065: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "2066: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "2067: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "2068: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "2069: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "2070: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "2071: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "2072: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "2073: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "2074: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "2075: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "2076: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "2077: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "2078: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "2079: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "2080: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "2081: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "2082: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "2083: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "2084: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "2085: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "2086: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "2087: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "2088: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "2089: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "2090: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "2091: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "2092: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "2093: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "2094: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "2095: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "2096: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "2097: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "2098: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "2099: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "2100: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "2101: BertTokenizer.decode not understanding new vocabulary - https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "2102: Tokenizer tend to choose added tokens first rather than token in vocab - https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "2103: Special token printed out as output - https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948\n",
            "2104: [NER][Japanese] labeled segment shorter than token - https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "2105: T5Tokenizer add a whitespace token after added special tokens - https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "2106: Unable to register my own tokenizer - https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "2107: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab! - https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "2108: Use Unicode blocks in regex (in Replace normalizer) - https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904\n",
            "2109: How to handle translations one source language to many target sentences for the same language - https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "2110: NER Label tokenization with overflowing tokens - https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561\n",
            "2111: How to use a trained tokenizer for semantic search? - https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "2112: Unk_token not set after training a BPETokenizer tokenizer - https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733\n",
            "2113: AutoTokenizer is very slow when loading llama tokenizer - https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547\n",
            "2114: Offset mappings differ for tokenizers - https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424\n",
            "2115: The process for tokenizing concatenated dataset is slow st the end of tokenizing - https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "2116: Batch tokenize (split into tokens, without processing) - https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238\n",
            "2117: Does AutoTokenizer uploads data to HuggingFace - https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "2118: RobertaTokenizer decode and tokenize do not have the same output - https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "2119: Training tokenizers with padding in between tokens - https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "2120: Leaving unknown words untokenized like in OpenMNT - https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "2121: Keeping special chars in translations - https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270\n",
            "2122: Should cls_token be [CLS] or <cls>? - https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "2123: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False - https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "2124: I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error - https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742\n",
            "2125: OSError: Can’t load tokenizer for ‘facebook/xmod-base’ - https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471\n",
            "2126: `GPT2Tokenizer` Tokenizer handling `\\n\\n` differently in different settings - https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289\n",
            "2127: DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‘padding=True’ ‘truncation=True’ to have batched tensors with the same length - https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625\n",
            "2128: Decode token IDs into a list (not a single string) - https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991\n",
            "2129: Error training MLM with Roberta Tokenizer - https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878\n",
            "2130: Are the slow and fast tokenizer results the same output for the same input? - https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "2131: “Add_tokens” breaks words when encoding - https://discuss.huggingface.co/t/add-tokens-breaks-words-when-encoding/21154\n",
            "2132: 2 tokens for one character in T5 - https://discuss.huggingface.co/t/2-tokens-for-one-character-in-t5/14960\n",
            "2133: OSError: Model name ‘gpt2’ was not found in tokenizers model name list (gpt2,…) - https://discuss.huggingface.co/t/oserror-model-name-gpt2-was-not-found-in-tokenizers-model-name-list-gpt2/2164\n",
            "2134: DNA long sequence tokenization - https://discuss.huggingface.co/t/dna-long-sequence-tokenization/16154\n",
            "2135: SentencePiece tokenizer encodes to unknown token - https://discuss.huggingface.co/t/sentencepiece-tokenizer-encodes-to-unknown-token/49113\n",
            "2136: Tokenizer behaviour with pipeline - https://discuss.huggingface.co/t/tokenizer-behaviour-with-pipeline/48969\n",
            "2137: Load tokenizer from file : Exception: data did not match any variant of untagged enum ModelWrapper - https://discuss.huggingface.co/t/load-tokenizer-from-file-exception-data-did-not-match-any-variant-of-untagged-enum-modelwrapper/25325\n",
            "2138: ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076 - https://discuss.huggingface.co/t/arrowinvalid-column-3-named-attention-mask-expected-length-1000-but-got-length-1076/6904\n",
            "2139: Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model - https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829\n",
            "2140: Initialize Vocabulary for Unigram Tokenizer - https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371\n",
            "2141: Make correct padding for text generation with GPT-NEO - https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800\n",
            "2142: How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers? - https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639\n",
            "2143: Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF - https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602\n",
            "2144: Writing custom tokenizer and wrapping it in tokenizer object - https://discuss.huggingface.co/t/writing-custom-tokenizer-and-wrapping-it-in-tokenizer-object/39679\n",
            "2145: Tokenizer for German lang - https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222\n",
            "2146: Chunk tokens into desired chunk length without simply getting rid of rest of tokens - https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399\n",
            "2147: Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers - https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872\n",
            "2148: LlamaTokenizerFast returns token_type_ids but the forward pass of the LlamaModel does not receive token_type_ids - https://discuss.huggingface.co/t/llamatokenizerfast-returns-token-type-ids-but-the-forward-pass-of-the-llamamodel-does-not-receive-token-type-ids/42431\n",
            "2149: GPT2Tokenizer not working in Kaggle Notebook - https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508\n",
            "2150: Exploring the Majestic Temples in Karnataka - https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952\n",
            "2151: Tokenizer producing token index greater than size of the dictionary - https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989\n",
            "2152: How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer - https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843\n",
            "2153: How to create a HF tokenizer’s vocab file from a BPE model’s merges.txt file? - https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737\n",
            "2154: Scala/JVM Bindings for Tokenizers - https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393\n",
            "2155: Tokenizers Wheel Takes Forever to Build - https://discuss.huggingface.co/t/tokenizers-wheel-takes-forever-to-build/17114\n",
            "2156: Where the introduction of tokenizers.implementations? - https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959\n",
            "2157: How to return custom `token_type_ids` or other values from a tokenizer? - https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525\n",
            "2158: Easy way to compare tokenizers - https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347\n",
            "2159: Unable to load image using llama-index - https://discuss.huggingface.co/t/unable-to-load-image-using-llama-index/38304\n",
            "2160: Help defining tokenizer - https://discuss.huggingface.co/t/help-defining-tokenizer/38091\n",
            "2161: Token Offsets in Rust vs. Python - https://discuss.huggingface.co/t/token-offsets-in-rust-vs-python/37949\n",
            "2162: “OSError: Model name ‘./XX’ was not found in tokenizers model name list” - cannot load custom tokenizer in Transformers - https://discuss.huggingface.co/t/oserror-model-name-xx-was-not-found-in-tokenizers-model-name-list-cannot-load-custom-tokenizer-in-transformers/2714\n",
            "2163: Converting JSON/dict to flatten string with indicator tokens - https://discuss.huggingface.co/t/converting-json-dict-to-flatten-string-with-indicator-tokens/37387\n",
            "2164: Train Retry Tokenizer - https://discuss.huggingface.co/t/train-retry-tokenizer/37031\n",
            "2165: Pretokenise on punctuation except hyphens - https://discuss.huggingface.co/t/pretokenise-on-punctuation-except-hyphens/36691\n",
            "2166: Tokenizer Trainer Crashing - https://discuss.huggingface.co/t/tokenizer-trainer-crashing/36645\n",
            "2167: Tokenizer extremely slow when deployed to a container - https://discuss.huggingface.co/t/tokenizer-extremely-slow-when-deployed-to-a-container/36569\n",
            "2168: Dealing with Decimal and Fractions - https://discuss.huggingface.co/t/dealing-with-decimal-and-fractions/23377\n",
            "2169: `add_tokens` with argument `special_tokens=True` vs `add_special_tokens` - https://discuss.huggingface.co/t/add-tokens-with-argument-special-tokens-true-vs-add-special-tokens/35648\n",
            "2170: Unable to upload custom Pytorch model in huggingface - https://discuss.huggingface.co/t/unable-to-upload-custom-pytorch-model-in-huggingface/35545\n",
            "2171: RuntimeError: Cannot re-initialize CUDA in forked subprocess - https://discuss.huggingface.co/t/runtimeerror-cannot-re-initialize-cuda-in-forked-subprocess/28392\n",
            "2172: Overflowing Tokens in MarkupLM - https://discuss.huggingface.co/t/overflowing-tokens-in-markuplm/35217\n",
            "2173: I get the predicted token as ` े` . What am I doing wrong? - https://discuss.huggingface.co/t/i-get-the-predicted-token-as-what-am-i-doing-wrong/29038\n",
            "2174: <unk> token in the output instead curly braces - https://discuss.huggingface.co/t/unk-token-in-the-output-instead-curly-braces/34653\n",
            "2175: How to add a new token without expanding the vocabulary - https://discuss.huggingface.co/t/how-to-add-a-new-token-without-expanding-the-vocabulary/34536\n",
            "2176: Does the ByteLevelBPETokenizer need to be wrapped in a normal Tokenizer? - https://discuss.huggingface.co/t/does-the-bytelevelbpetokenizer-need-to-be-wrapped-in-a-normal-tokenizer/34121\n",
            "2177: What is required to create a fast tokenizer? For example for a Marian model - https://discuss.huggingface.co/t/what-is-required-to-create-a-fast-tokenizer-for-example-for-a-marian-model/33941\n",
            "2178: GPT2Tokenizer.decode maps unicode sequences to the same string ‘�’ - https://discuss.huggingface.co/t/gpt2tokenizer-decode-maps-unicode-sequences-to-the-same-string/32453\n",
            "2179: Issue with Tokenizer - https://discuss.huggingface.co/t/issue-with-tokenizer/33796\n",
            "2180: Tokenizing my novel for GPT model - https://discuss.huggingface.co/t/tokenizing-my-novel-for-gpt-model/33532\n",
            "2181: How to add additional custom pre-tokenization processing? - https://discuss.huggingface.co/t/how-to-add-additional-custom-pre-tokenization-processing/1637\n",
            "2182: Customize FlauBERT tokenizer to split line breaks - https://discuss.huggingface.co/t/customize-flaubert-tokenizer-to-split-line-breaks/33011\n",
            "2183: How to change the size of model_max_length? - https://discuss.huggingface.co/t/how-to-change-the-size-of-model-max-length/32942\n",
            "2184: Can’t get to the source code of `tokenizer.convert_tokens_to_string` - https://discuss.huggingface.co/t/cant-get-to-the-source-code-of-tokenizer-convert-tokens-to-string/32714\n",
            "2185: Why I’m getting same result with or without using Wav2Vec2Processor? - https://discuss.huggingface.co/t/why-im-getting-same-result-with-or-without-using-wav2vec2processor/32482\n",
            "2186: How does `tokenizer().input_ids` work and how different it is from tokenizer.encode() before `model.generate()` and decoding step? - https://discuss.huggingface.co/t/how-does-tokenizer-input-ids-work-and-how-different-it-is-from-tokenizer-encode-before-model-generate-and-decoding-step/30476\n",
            "2187: What file type should my training data be? - https://discuss.huggingface.co/t/what-file-type-should-my-training-data-be/32075\n",
            "2188: Best way to get the closest token indices of input of char_to_token is a whitespace - https://discuss.huggingface.co/t/best-way-to-get-the-closest-token-indices-of-input-of-char-to-token-is-a-whitespace/32004\n",
            "2189: Token indices sequence length is longer than the specified maximum sequence length - https://discuss.huggingface.co/t/token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length/25133\n",
            "2190: Create a simple tokenizer - https://discuss.huggingface.co/t/create-a-simple-tokenizer/31677\n",
            "2191: Sliding window for Long Documents - https://discuss.huggingface.co/t/sliding-window-for-long-documents/19367\n",
            "2192: Creating tokenizer from counts file? - https://discuss.huggingface.co/t/creating-tokenizer-from-counts-file/31399\n",
            "2193: Tokenizer.train() running out of memory - https://discuss.huggingface.co/t/tokenizer-train-running-out-of-memory/31355\n",
            "2194: Tokenizing Float Tensor? - https://discuss.huggingface.co/t/tokenizing-float-tensor/30604\n",
            "2195: Padding and truncation for custom tokenizer - https://discuss.huggingface.co/t/padding-and-truncation-for-custom-tokenizer/30180\n",
            "2196: Incorporate SARI score into run_summarization.py example script - https://discuss.huggingface.co/t/incorporate-sari-score-into-run-summarization-py-example-script/29511\n",
            "2197: Is that possible to embed the tokenizer into the model to have it running on GCP using TensorFlow Serving? - https://discuss.huggingface.co/t/is-that-possible-to-embed-the-tokenizer-into-the-model-to-have-it-running-on-gcp-using-tensorflow-serving/10532\n",
            "2198: Huggingface inference API issue - https://discuss.huggingface.co/t/huggingface-inference-api-issue/29307\n",
            "2199: Using Tokenizer for integer data - https://discuss.huggingface.co/t/using-tokenizer-for-integer-data/28835\n",
            "2200: GPT2 long text approach - https://discuss.huggingface.co/t/gpt2-long-text-approach/28165\n",
            "2201: Huggingface t5 models seem to not download a tokenizer file - https://discuss.huggingface.co/t/huggingface-t5-models-seem-to-not-download-a-tokenizer-file/27935\n",
            "2202: How to save a fast tokenizer using the transformer library and then load it using Tokenizers? - https://discuss.huggingface.co/t/how-to-save-a-fast-tokenizer-using-the-transformer-library-and-then-load-it-using-tokenizers/9567\n",
            "2203: Using a BertTokenizer when training a RobertaForMaskedLM - https://discuss.huggingface.co/t/using-a-berttokenizer-when-training-a-robertaformaskedlm/27456\n",
            "2204: Need clarity on “padding” parameter in Bert Tokenizer - https://discuss.huggingface.co/t/need-clarity-on-padding-parameter-in-bert-tokenizer/27444\n",
            "2205: How to convert HuggingFace tokenizers into ONNX format? - https://discuss.huggingface.co/t/how-to-convert-huggingface-tokenizers-into-onnx-format/27272\n",
            "2206: Can’t save ConvBert tokenizer - https://discuss.huggingface.co/t/cant-save-convbert-tokenizer/16006\n",
            "2207: RoBERTa Tokenizer Java Implementation - https://discuss.huggingface.co/t/roberta-tokenizer-java-implementation/16737\n",
            "2208: Unigram vocab_size doesn’t fit - https://discuss.huggingface.co/t/unigram-vocab-size-doesnt-fit/26856\n",
            "2209: Option to load only tokenizer and model configuration into “token-classification” pipeline - https://discuss.huggingface.co/t/option-to-load-only-tokenizer-and-model-configuration-into-token-classification-pipeline/26697\n",
            "2210: Encode_plus Pretokenized input seuqence must be Union - https://discuss.huggingface.co/t/encode-plus-pretokenized-input-seuqence-must-be-union/26449\n",
            "2211: Application of TFBertTokenizer - https://discuss.huggingface.co/t/application-of-tfberttokenizer/26437\n",
            "2212: TemplateProcessing for encoder-decoder - https://discuss.huggingface.co/t/templateprocessing-for-encoder-decoder/26135\n",
            "2213: Using `TFBertTokenizer` instead of `BertTokenizer` with `TFBertForQuestionAnswering` - https://discuss.huggingface.co/t/using-tfberttokenizer-instead-of-berttokenizer-with-tfbertforquestionanswering/26127\n",
            "2214: How to concatenate an answer to multiple choices after padded tokenization - https://discuss.huggingface.co/t/how-to-concatenate-an-answer-to-multiple-choices-after-padded-tokenization/26110\n",
            "2215: Maximum recursion depth exceeded when using DataCollator - https://discuss.huggingface.co/t/maximum-recursion-depth-exceeded-when-using-datacollator/25937\n",
            "2216: Adding a special language token to MBART - https://discuss.huggingface.co/t/adding-a-special-language-token-to-mbart/25967\n",
            "2217: Custom PostProcessor? - https://discuss.huggingface.co/t/custom-postprocessor/25847\n",
            "2218: Tokenizer.pad_token=what? - https://discuss.huggingface.co/t/tokenizer-pad-token-what/24367\n",
            "2219: Using HuggingFace Tokenizers Without Special Characters - https://discuss.huggingface.co/t/using-huggingface-tokenizers-without-special-characters/21962\n",
            "2220: How to get sp_model variable from T5Tokenizer? - https://discuss.huggingface.co/t/how-to-get-sp-model-variable-from-t5tokenizer/25171\n",
            "2221: Wav2vec2CTCTokenizer and vocab.json - https://discuss.huggingface.co/t/wav2vec2ctctokenizer-and-vocab-json/25146\n",
            "2222: Period ID in RobertaTokenizer with is_split_into_words - https://discuss.huggingface.co/t/period-id-in-robertatokenizer-with-is-split-into-words/24142\n",
            "2223: WordLevel error: Missing [UNK] token from the vocabulary - https://discuss.huggingface.co/t/wordlevel-error-missing-unk-token-from-the-vocabulary/5107\n",
            "2224: Tokenizer post_processor help - https://discuss.huggingface.co/t/tokenizer-post-processor-help/21926\n",
            "2225: Preprocessing raw text - https://discuss.huggingface.co/t/preprocessing-raw-text/24559\n",
            "2226: Save tokenizer with argument - https://discuss.huggingface.co/t/save-tokenizer-with-argument/17389\n",
            "2227: Trained tokenizer API as PretrainedTokenizer - https://discuss.huggingface.co/t/trained-tokenizer-api-as-pretrainedtokenizer/23639\n",
            "2228: Remove only certain special token id during tokenizer decode - https://discuss.huggingface.co/t/remove-only-certain-special-token-id-during-tokenizer-decode/20436\n",
            "2229: Convert_tokens_to_ids produces <unk> - https://discuss.huggingface.co/t/convert-tokens-to-ids-produces-unk/24771\n",
            "2230: Text preprocessing for fitting Tokenizer model - https://discuss.huggingface.co/t/text-preprocessing-for-fitting-tokenizer-model/24869\n",
            "2231: Special tokens warning - https://discuss.huggingface.co/t/special-tokens-warning/24939\n",
            "2232: Simple Transformers Multilabelclassification - https://discuss.huggingface.co/t/simple-transformers-multilabelclassification/24568\n",
            "2233: Cannot initialize deberta-v3-base tokenizer - https://discuss.huggingface.co/t/cannot-initialize-deberta-v3-base-tokenizer/15322\n",
            "2234: Getting Wholeword corresponding to a subword in a text? - https://discuss.huggingface.co/t/getting-wholeword-corresponding-to-a-subword-in-a-text/24152\n",
            "2235: Issue with pushing tokenizer to hub - https://discuss.huggingface.co/t/issue-with-pushing-tokenizer-to-hub/24120\n",
            "2236: How do we customize the number of entites for NER pretrained model? - https://discuss.huggingface.co/t/how-do-we-customize-the-number-of-entites-for-ner-pretrained-model/24085\n",
            "2237: Configure RobertaTokenizer - https://discuss.huggingface.co/t/configure-robertatokenizer/23969\n",
            "2238: How to properly clean vocabulary from BBPE tokenizer - https://discuss.huggingface.co/t/how-to-properly-clean-vocabulary-from-bbpe-tokenizer/22827\n",
            "2239: Map tokenization and posterior to smaller substrings - https://discuss.huggingface.co/t/map-tokenization-and-posterior-to-smaller-substrings/23792\n",
            "2240: T5 model tokenizer - https://discuss.huggingface.co/t/t5-model-tokenizer/23640\n",
            "2241: Fast tokenizer for marianMTModel - https://discuss.huggingface.co/t/fast-tokenizer-for-marianmtmodel/23638\n",
            "2242: Word tokenizers for text generators - https://discuss.huggingface.co/t/word-tokenizers-for-text-generators/23464\n",
            "2243: SentencePieceUnigramTokenizer - https://discuss.huggingface.co/t/sentencepieceunigramtokenizer/23509\n",
            "2244: Tokenizer is not being loaded on Huggingface Inference - https://discuss.huggingface.co/t/tokenizer-is-not-being-loaded-on-huggingface-inference/23505\n",
            "2245: Why is BertNormalizer not exposed on the tokenizers library? - https://discuss.huggingface.co/t/why-is-bertnormalizer-not-exposed-on-the-tokenizers-library/23340\n",
            "2246: Sentence splitting - https://discuss.huggingface.co/t/sentence-splitting/5393\n",
            "2247: Average time to train a SentencePieceBPETokenizer - https://discuss.huggingface.co/t/average-time-to-train-a-sentencepiecebpetokenizer/23081\n",
            "2248: 1 line code for NER data set preparation using tokenizer library! - https://discuss.huggingface.co/t/1-line-code-for-ner-data-set-preparation-using-tokenizer-library/22816\n",
            "2249: Microsoft/codebert-base produces two sep tokens - https://discuss.huggingface.co/t/microsoft-codebert-base-produces-two-sep-tokens/21366\n",
            "2250: Padding with sliding window - https://discuss.huggingface.co/t/padding-with-sliding-window/18921\n",
            "2251: Find which tokens are unknown in new data - https://discuss.huggingface.co/t/find-which-tokens-are-unknown-in-new-data/22466\n",
            "2252: How to train target tokenizer - https://discuss.huggingface.co/t/how-to-train-target-tokenizer/22291\n",
            "2253: How to know if a subtoken is a word or part of a word? - https://discuss.huggingface.co/t/how-to-know-if-a-subtoken-is-a-word-or-part-of-a-word/923\n",
            "2254: BART Tokenizer tokenises same word differently? - https://discuss.huggingface.co/t/bart-tokenizer-tokenises-same-word-differently/21835\n",
            "2255: Fine-tuned BERT tokenizer taking too long to load - https://discuss.huggingface.co/t/fine-tuned-bert-tokenizer-taking-too-long-to-load/9747\n",
            "2256: Add BOS and EOS when encoding a sentence - https://discuss.huggingface.co/t/add-bos-and-eos-when-encoding-a-sentence/21833\n",
            "2257: Customization of Wav2Vec2CTCTokenizer with rules - https://discuss.huggingface.co/t/customization-of-wav2vec2ctctokenizer-with-rules/21912\n",
            "2258: Customized tokenization files in run_clm script - https://discuss.huggingface.co/t/customized-tokenization-files-in-run-clm-script/21460\n",
            "2259: Using customized algorithm - https://discuss.huggingface.co/t/using-customized-algorithm/21753\n",
            "2260: Issue with Flaubert Tokenizer as word_ids() method is not available for NER Task - https://discuss.huggingface.co/t/issue-with-flaubert-tokenizer-as-word-ids-method-is-not-available-for-ner-task/20374\n",
            "2261: Word_ids not working with deberta_v2 - https://discuss.huggingface.co/t/word-ids-not-working-with-deberta-v2/21523\n",
            "2262: How to tokenize large contexts without running out of memory - https://discuss.huggingface.co/t/how-to-tokenize-large-contexts-without-running-out-of-memory/5882\n",
            "2263: Does Deberta tokenizer use wordpiece? - https://discuss.huggingface.co/t/does-deberta-tokenizer-use-wordpiece/21307\n",
            "2264: Get vocabulary tokens in order to exclude them from generate function - https://discuss.huggingface.co/t/get-vocabulary-tokens-in-order-to-exclude-them-from-generate-function/5192\n",
            "2265: Avoid creating certain tokens when training a tokenizer - https://discuss.huggingface.co/t/avoid-creating-certain-tokens-when-training-a-tokenizer/20864\n",
            "2266: Error finetuning XLM-RoBERTa-Large when training - https://discuss.huggingface.co/t/error-finetuning-xlm-roberta-large-when-training/20412\n",
            "2267: HuggingFace BPE Trainer Error - Training Tokenizer - https://discuss.huggingface.co/t/huggingface-bpe-trainer-error-training-tokenizer/10629\n",
            "2268: Word_to_tokens() and word_ids() —- microsoft/deberta-v2/v3 - https://discuss.huggingface.co/t/word-to-tokens-and-word-ids-microsoft-deberta-v2-v3/19984\n",
            "2269: No PreTrainedTokenizerFast for Deberta-V3, no doc_stride - https://discuss.huggingface.co/t/no-pretrainedtokenizerfast-for-deberta-v3-no-doc-stride/20345\n",
            "2270: Tokenizer from own vocab - https://discuss.huggingface.co/t/tokenizer-from-own-vocab/20245\n",
            "2271: No labels column for tokenized data - https://discuss.huggingface.co/t/no-labels-column-for-tokenized-data/19540\n",
            "2272: Programmatic way to Tokenization on Custom Text Columns - https://discuss.huggingface.co/t/programmatic-way-to-tokenization-on-custom-text-columns/19679\n",
            "2273: Bug in Offset generation for Rupee symbol - https://discuss.huggingface.co/t/bug-in-offset-generation-for-rupee-symbol/19655\n",
            "2274: How to handle parenthesis, quotation marks, \\n etc when creating tokenizer from scratch - https://discuss.huggingface.co/t/how-to-handle-parenthesis-quotation-marks-n-etc-when-creating-tokenizer-from-scratch/19628\n",
            "2275: EM training on unigram tokenizer taking way longer than predicted - https://discuss.huggingface.co/t/em-training-on-unigram-tokenizer-taking-way-longer-than-predicted/19502\n",
            "2276: Training unigram on long sequences - https://discuss.huggingface.co/t/training-unigram-on-long-sequences/19188\n",
            "2277: Issue with post-processing - https://discuss.huggingface.co/t/issue-with-post-processing/2703\n",
            "2278: FutureWarning about BertTokenizer.from_pretrained() at latest version - https://discuss.huggingface.co/t/futurewarning-about-berttokenizer-from-pretrained-at-latest-version/18750\n",
            "2279: Enhaced word_ids() API for Chinese or CJK languages? - https://discuss.huggingface.co/t/enhaced-word-ids-api-for-chinese-or-cjk-languages/18662\n",
            "2280: Importing tokenizers version >0.10.3 fails due to openssl - https://discuss.huggingface.co/t/importing-tokenizers-version-0-10-3-fails-due-to-openssl/17820\n",
            "2281: Lower case with input ids - https://discuss.huggingface.co/t/lower-case-with-input-ids/18490\n",
            "2282: Dialogue classification - https://discuss.huggingface.co/t/dialogue-classification/18466\n",
            "2283: Multilang bert vs translating to english - https://discuss.huggingface.co/t/multilang-bert-vs-translating-to-english/18454\n",
            "2284: pyo3_runtime.PanicException: likelihood is NAN. Input sentence may be too long - https://discuss.huggingface.co/t/pyo3-runtime-panicexception-likelihood-is-nan-input-sentence-may-be-too-long/18398\n",
            "2285: Pytorch_model.bin not working because of lfs - https://discuss.huggingface.co/t/pytorch-model-bin-not-working-because-of-lfs/18290\n",
            "2286: Tokenizer ignores repeated whitespaces - https://discuss.huggingface.co/t/tokenizer-ignores-repeated-whitespaces/17864\n",
            "2287: How truncation works when applying BERT tokenizer on the batch of sentence pairs in HuggingFace? - https://discuss.huggingface.co/t/how-truncation-works-when-applying-bert-tokenizer-on-the-batch-of-sentence-pairs-in-huggingface/17977\n",
            "2288: How to save a tokenizer only consisting of added tokens - https://discuss.huggingface.co/t/how-to-save-a-tokenizer-only-consisting-of-added-tokens/17825\n",
            "2289: Passing list of inputs to tokenize - https://discuss.huggingface.co/t/passing-list-of-inputs-to-tokenize/17499\n",
            "2290: Issues with Data Collator and Tokenizing with NER Datasets - https://discuss.huggingface.co/t/issues-with-data-collator-and-tokenizing-with-ner-datasets/17489\n",
            "2291: How to perform tokenization on an ONNX model in JS? - https://discuss.huggingface.co/t/how-to-perform-tokenization-on-an-onnx-model-in-js/17581\n",
            "2292: Using the Tokenizers library in a Unity project - https://discuss.huggingface.co/t/using-the-tokenizers-library-in-a-unity-project/17533\n",
            "2293: Further pre-training the tokenizer? - https://discuss.huggingface.co/t/further-pre-training-the-tokenizer/17360\n",
            "2294: Error when doing tokenization - https://discuss.huggingface.co/t/error-when-doing-tokenization/17344\n",
            "2295: How to decode with spaces? - https://discuss.huggingface.co/t/how-to-decode-with-spaces/17289\n",
            "2296: Show Submodels of PegasusTokenizer - https://discuss.huggingface.co/t/show-submodels-of-pegasustokenizer/17286\n",
            "2297: Load SentencePieceBPETokenizer in TF - https://discuss.huggingface.co/t/load-sentencepiecebpetokenizer-in-tf/17257\n",
            "2298: Best way to mask a multi-token word when using `.*ForMaskedLM` models - https://discuss.huggingface.co/t/best-way-to-mask-a-multi-token-word-when-using-formaskedlm-models/6428\n",
            "2299: Does a tokenizer keep the mapping between my labels to their encoding? - https://discuss.huggingface.co/t/does-a-tokenizer-keep-the-mapping-between-my-labels-to-their-encoding/16296\n",
            "2300: What is Wav2Vec2FeatureExtractor doing? - https://discuss.huggingface.co/t/what-is-wav2vec2featureextractor-doing/16423\n",
            "2301: What does this warning mean? -overflowing tokens are not returned for the setting you have chosen - https://discuss.huggingface.co/t/what-does-this-warning-mean-overflowing-tokens-are-not-returned-for-the-setting-you-have-chosen/11594\n",
            "2302: How can I make sure Tokenizer pads to a fixed length? - https://discuss.huggingface.co/t/how-can-i-make-sure-tokenizer-pads-to-a-fixed-length/16116\n",
            "2303: Issue with Decoding in HuggingFace - https://discuss.huggingface.co/t/issue-with-decoding-in-huggingface/15699\n",
            "2304: ValueError: Unable to create tensor for 1 dataset but not the other of same type - https://discuss.huggingface.co/t/valueerror-unable-to-create-tensor-for-1-dataset-but-not-the-other-of-same-type/15995\n",
            "2305: Disabling addition of CLS from BERT tokenizer - https://discuss.huggingface.co/t/disabling-addition-of-cls-from-bert-tokenizer/15538\n",
            "2306: Tokenized sequence lengths - https://discuss.huggingface.co/t/tokenized-sequence-lengths/15117\n",
            "2307: Finetuning GPT-J6B for custom dataset - https://discuss.huggingface.co/t/finetuning-gpt-j6b-for-custom-dataset/9924\n",
            "2308: Training tokenizer takes too much RAM - https://discuss.huggingface.co/t/training-tokenizer-takes-too-much-ram/14256\n",
            "2309: How to “further pretrain” a tokenizer (do I need to do so?) - https://discuss.huggingface.co/t/how-to-further-pretrain-a-tokenizer-do-i-need-to-do-so/14719\n",
            "2310: Run_seq2seq_qa.py: Column 3 named labels expected length 1007 but got length 1000 - https://discuss.huggingface.co/t/run-seq2seq-qa-py-column-3-named-labels-expected-length-1007-but-got-length-1000/13237\n",
            "2311: Issues with offset_mapping values - https://discuss.huggingface.co/t/issues-with-offset-mapping-values/4237\n",
            "2312: How would you train a sentencepiece BPE tokenizer on this language with 400 “characters”? - https://discuss.huggingface.co/t/how-would-you-train-a-sentencepiece-bpe-tokenizer-on-this-language-with-400-characters/14669\n",
            "2313: All my sequences get tokenized the same - https://discuss.huggingface.co/t/all-my-sequences-get-tokenized-the-same/14657\n",
            "2314: Combine multiple sentences together during tokenization - https://discuss.huggingface.co/t/combine-multiple-sentences-together-during-tokenization/3430\n",
            "2315: NER tag , aggregation stratergy - https://discuss.huggingface.co/t/ner-tag-aggregation-stratergy/14199\n",
            "2316: How to ensure that tokenizers never truncate partial words? - https://discuss.huggingface.co/t/how-to-ensure-that-tokenizers-never-truncate-partial-words/14024\n",
            "2317: How to ensure the `overflow` with `stride` always starts with a full word? - https://discuss.huggingface.co/t/how-to-ensure-the-overflow-with-stride-always-starts-with-a-full-word/14030\n",
            "2318: Adding new tokens to a BERT tokenizer - Getting ValueError - https://discuss.huggingface.co/t/adding-new-tokens-to-a-bert-tokenizer-getting-valueerror/9253\n",
            "2319: Adding token to t5-base vocab does not respect space - https://discuss.huggingface.co/t/adding-token-to-t5-base-vocab-does-not-respect-space/13662\n",
            "2320: How can I change the token id of a special token? - https://discuss.huggingface.co/t/how-can-i-change-the-token-id-of-a-special-token/13445\n",
            "2321: Import distilbert-base-uncased tokenizer to an android app along with the tflite model - https://discuss.huggingface.co/t/import-distilbert-base-uncased-tokenizer-to-an-android-app-along-with-the-tflite-model/3234\n",
            "2322: What are the equivalent manner for using texts_to_sequences? - https://discuss.huggingface.co/t/what-are-the-equivalent-manner-for-using-texts-to-sequences/13220\n",
            "2323: ERROR?why encoding [MASK] before ‘.’ would gain a idx 13? - https://discuss.huggingface.co/t/error-why-encoding-mask-before-would-gain-a-idx-13/2897\n",
            "2324: LongFormer tokenizer has the same token_type_ids for sequence pairs - https://discuss.huggingface.co/t/longformer-tokenizer-has-the-same-token-type-ids-for-sequence-pairs/12992\n",
            "2325: Batch encode plus in Rust Tokenizers - https://discuss.huggingface.co/t/batch-encode-plus-in-rust-tokenizers/12722\n",
            "2326: Best solution for train tokenizer and MLM from scratch - https://discuss.huggingface.co/t/best-solution-for-train-tokenizer-and-mlm-from-scratch/12579\n",
            "2327: Implementing custom tokenizer components (normalizers, processors) - https://discuss.huggingface.co/t/implementing-custom-tokenizer-components-normalizers-processors/12371\n",
            "2328: Does T5Tokenizer support the Greek language? - https://discuss.huggingface.co/t/does-t5tokenizer-support-the-greek-language/12224\n",
            "2329: How padding in huggingface tokenizer works? - https://discuss.huggingface.co/t/how-padding-in-huggingface-tokenizer-works/12161\n",
            "2330: Why we need to add special tokens to tasks other than classification? - https://discuss.huggingface.co/t/why-we-need-to-add-special-tokens-to-tasks-other-than-classification/11975\n",
            "2331: How to configure TokenizerFast for AutoTokenizer - https://discuss.huggingface.co/t/how-to-configure-tokenizerfast-for-autotokenizer/11353\n",
            "2332: How to employ different vocabs for encoder and decoder respectively? - https://discuss.huggingface.co/t/how-to-employ-different-vocabs-for-encoder-and-decoder-respectively/11497\n",
            "2333: How to use tokenizer.tokenize in Chinese data properly? - https://discuss.huggingface.co/t/how-to-use-tokenizer-tokenize-in-chinese-data-properly/11491\n",
            "2334: Mask only specific words - https://discuss.huggingface.co/t/mask-only-specific-words/173\n",
            "2335: Load custom pretrained tokenizer - https://discuss.huggingface.co/t/load-custom-pretrained-tokenizer/11148\n",
            "2336: Using Custom Vocab.txt - https://discuss.huggingface.co/t/using-custom-vocab-txt/10847\n",
            "2337: Tokenizer.encode not returning encodings - https://discuss.huggingface.co/t/tokenizer-encode-not-returning-encodings/10616\n",
            "2338: There is no 0.11.0 tokenizers in pip - https://discuss.huggingface.co/t/there-is-no-0-11-0-tokenizers-in-pip/10381\n",
            "2339: Performance difference between ByteLevelBPE and Wordpiece tokenizers - https://discuss.huggingface.co/t/performance-difference-between-bytelevelbpe-and-wordpiece-tokenizers/10203\n",
            "2340: Should have a `model_type` key in its config.json - https://discuss.huggingface.co/t/should-have-a-model-type-key-in-its-config-json/10144\n",
            "2341: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "2342: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "2343: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "2344: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "2345: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "2346: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "2347: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "2348: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "2349: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "2350: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "2351: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "2352: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "2353: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "2354: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "2355: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "2356: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "2357: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "2358: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "2359: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "2360: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "2361: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "2362: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "2363: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "2364: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "2365: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "2366: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "2367: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "2368: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "2369: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "2370: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "2371: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "2372: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "2373: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "2374: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "2375: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "2376: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "2377: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "2378: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "2379: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "2380: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "2381: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "2382: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "2383: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "2384: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "2385: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "2386: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "2387: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "2388: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "2389: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "2390: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "2391: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "2392: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "2393: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "2394: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "2395: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "2396: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "2397: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "2398: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "2399: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "2400: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "2401: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "2402: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "2403: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "2404: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "2405: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "2406: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "2407: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "2408: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "2409: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "2410: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "2411: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "2412: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "2413: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "2414: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "2415: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "2416: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "2417: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "2418: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "2419: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "2420: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "2421: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "2422: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "2423: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "2424: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "2425: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "2426: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "2427: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "2428: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "2429: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "2430: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "2431: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "2432: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "2433: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "2434: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "2435: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "2436: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "2437: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "2438: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "2439: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "2440: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "2441: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "2442: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "2443: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "2444: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "2445: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "2446: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "2447: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "2448: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "2449: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "2450: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "2451: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "2452: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "2453: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "2454: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "2455: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "2456: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "2457: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "2458: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "2459: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "2460: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "2461: BertTokenizer.decode not understanding new vocabulary - https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "2462: Tokenizer tend to choose added tokens first rather than token in vocab - https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "2463: Special token printed out as output - https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948\n",
            "2464: [NER][Japanese] labeled segment shorter than token - https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "2465: T5Tokenizer add a whitespace token after added special tokens - https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "2466: Unable to register my own tokenizer - https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "2467: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab! - https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "2468: Use Unicode blocks in regex (in Replace normalizer) - https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904\n",
            "2469: How to handle translations one source language to many target sentences for the same language - https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "2470: NER Label tokenization with overflowing tokens - https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561\n",
            "2471: How to use a trained tokenizer for semantic search? - https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "2472: Unk_token not set after training a BPETokenizer tokenizer - https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733\n",
            "2473: AutoTokenizer is very slow when loading llama tokenizer - https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547\n",
            "2474: Offset mappings differ for tokenizers - https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424\n",
            "2475: The process for tokenizing concatenated dataset is slow st the end of tokenizing - https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "2476: Batch tokenize (split into tokens, without processing) - https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238\n",
            "2477: Does AutoTokenizer uploads data to HuggingFace - https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "2478: RobertaTokenizer decode and tokenize do not have the same output - https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "2479: Training tokenizers with padding in between tokens - https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "2480: Leaving unknown words untokenized like in OpenMNT - https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "2481: Keeping special chars in translations - https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270\n",
            "2482: Should cls_token be [CLS] or <cls>? - https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "2483: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False - https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "2484: I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error - https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742\n",
            "2485: OSError: Can’t load tokenizer for ‘facebook/xmod-base’ - https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471\n",
            "2486: `GPT2Tokenizer` Tokenizer handling `\\n\\n` differently in different settings - https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289\n",
            "2487: DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‘padding=True’ ‘truncation=True’ to have batched tensors with the same length - https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625\n",
            "2488: Decode token IDs into a list (not a single string) - https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991\n",
            "2489: Error training MLM with Roberta Tokenizer - https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878\n",
            "2490: Are the slow and fast tokenizer results the same output for the same input? - https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "2491: “Add_tokens” breaks words when encoding - https://discuss.huggingface.co/t/add-tokens-breaks-words-when-encoding/21154\n",
            "2492: 2 tokens for one character in T5 - https://discuss.huggingface.co/t/2-tokens-for-one-character-in-t5/14960\n",
            "2493: OSError: Model name ‘gpt2’ was not found in tokenizers model name list (gpt2,…) - https://discuss.huggingface.co/t/oserror-model-name-gpt2-was-not-found-in-tokenizers-model-name-list-gpt2/2164\n",
            "2494: DNA long sequence tokenization - https://discuss.huggingface.co/t/dna-long-sequence-tokenization/16154\n",
            "2495: SentencePiece tokenizer encodes to unknown token - https://discuss.huggingface.co/t/sentencepiece-tokenizer-encodes-to-unknown-token/49113\n",
            "2496: Tokenizer behaviour with pipeline - https://discuss.huggingface.co/t/tokenizer-behaviour-with-pipeline/48969\n",
            "2497: Load tokenizer from file : Exception: data did not match any variant of untagged enum ModelWrapper - https://discuss.huggingface.co/t/load-tokenizer-from-file-exception-data-did-not-match-any-variant-of-untagged-enum-modelwrapper/25325\n",
            "2498: ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076 - https://discuss.huggingface.co/t/arrowinvalid-column-3-named-attention-mask-expected-length-1000-but-got-length-1076/6904\n",
            "2499: Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model - https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829\n",
            "2500: Initialize Vocabulary for Unigram Tokenizer - https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371\n",
            "2501: Make correct padding for text generation with GPT-NEO - https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800\n",
            "2502: How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers? - https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639\n",
            "2503: Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF - https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602\n",
            "2504: Writing custom tokenizer and wrapping it in tokenizer object - https://discuss.huggingface.co/t/writing-custom-tokenizer-and-wrapping-it-in-tokenizer-object/39679\n",
            "2505: Tokenizer for German lang - https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222\n",
            "2506: Chunk tokens into desired chunk length without simply getting rid of rest of tokens - https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399\n",
            "2507: Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers - https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872\n",
            "2508: LlamaTokenizerFast returns token_type_ids but the forward pass of the LlamaModel does not receive token_type_ids - https://discuss.huggingface.co/t/llamatokenizerfast-returns-token-type-ids-but-the-forward-pass-of-the-llamamodel-does-not-receive-token-type-ids/42431\n",
            "2509: GPT2Tokenizer not working in Kaggle Notebook - https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508\n",
            "2510: Exploring the Majestic Temples in Karnataka - https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952\n",
            "2511: Tokenizer producing token index greater than size of the dictionary - https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989\n",
            "2512: How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer - https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843\n",
            "2513: How to create a HF tokenizer’s vocab file from a BPE model’s merges.txt file? - https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737\n",
            "2514: Scala/JVM Bindings for Tokenizers - https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393\n",
            "2515: Tokenizers Wheel Takes Forever to Build - https://discuss.huggingface.co/t/tokenizers-wheel-takes-forever-to-build/17114\n",
            "2516: Where the introduction of tokenizers.implementations? - https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959\n",
            "2517: How to return custom `token_type_ids` or other values from a tokenizer? - https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525\n",
            "2518: Easy way to compare tokenizers - https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347\n",
            "2519: Unable to load image using llama-index - https://discuss.huggingface.co/t/unable-to-load-image-using-llama-index/38304\n",
            "2520: Help defining tokenizer - https://discuss.huggingface.co/t/help-defining-tokenizer/38091\n",
            "2521: Token Offsets in Rust vs. Python - https://discuss.huggingface.co/t/token-offsets-in-rust-vs-python/37949\n",
            "2522: “OSError: Model name ‘./XX’ was not found in tokenizers model name list” - cannot load custom tokenizer in Transformers - https://discuss.huggingface.co/t/oserror-model-name-xx-was-not-found-in-tokenizers-model-name-list-cannot-load-custom-tokenizer-in-transformers/2714\n",
            "2523: Converting JSON/dict to flatten string with indicator tokens - https://discuss.huggingface.co/t/converting-json-dict-to-flatten-string-with-indicator-tokens/37387\n",
            "2524: Train Retry Tokenizer - https://discuss.huggingface.co/t/train-retry-tokenizer/37031\n",
            "2525: Pretokenise on punctuation except hyphens - https://discuss.huggingface.co/t/pretokenise-on-punctuation-except-hyphens/36691\n",
            "2526: Tokenizer Trainer Crashing - https://discuss.huggingface.co/t/tokenizer-trainer-crashing/36645\n",
            "2527: Tokenizer extremely slow when deployed to a container - https://discuss.huggingface.co/t/tokenizer-extremely-slow-when-deployed-to-a-container/36569\n",
            "2528: Dealing with Decimal and Fractions - https://discuss.huggingface.co/t/dealing-with-decimal-and-fractions/23377\n",
            "2529: `add_tokens` with argument `special_tokens=True` vs `add_special_tokens` - https://discuss.huggingface.co/t/add-tokens-with-argument-special-tokens-true-vs-add-special-tokens/35648\n",
            "2530: Unable to upload custom Pytorch model in huggingface - https://discuss.huggingface.co/t/unable-to-upload-custom-pytorch-model-in-huggingface/35545\n",
            "2531: RuntimeError: Cannot re-initialize CUDA in forked subprocess - https://discuss.huggingface.co/t/runtimeerror-cannot-re-initialize-cuda-in-forked-subprocess/28392\n",
            "2532: Overflowing Tokens in MarkupLM - https://discuss.huggingface.co/t/overflowing-tokens-in-markuplm/35217\n",
            "2533: I get the predicted token as ` े` . What am I doing wrong? - https://discuss.huggingface.co/t/i-get-the-predicted-token-as-what-am-i-doing-wrong/29038\n",
            "2534: <unk> token in the output instead curly braces - https://discuss.huggingface.co/t/unk-token-in-the-output-instead-curly-braces/34653\n",
            "2535: How to add a new token without expanding the vocabulary - https://discuss.huggingface.co/t/how-to-add-a-new-token-without-expanding-the-vocabulary/34536\n",
            "2536: Does the ByteLevelBPETokenizer need to be wrapped in a normal Tokenizer? - https://discuss.huggingface.co/t/does-the-bytelevelbpetokenizer-need-to-be-wrapped-in-a-normal-tokenizer/34121\n",
            "2537: What is required to create a fast tokenizer? For example for a Marian model - https://discuss.huggingface.co/t/what-is-required-to-create-a-fast-tokenizer-for-example-for-a-marian-model/33941\n",
            "2538: GPT2Tokenizer.decode maps unicode sequences to the same string ‘�’ - https://discuss.huggingface.co/t/gpt2tokenizer-decode-maps-unicode-sequences-to-the-same-string/32453\n",
            "2539: Issue with Tokenizer - https://discuss.huggingface.co/t/issue-with-tokenizer/33796\n",
            "2540: Tokenizing my novel for GPT model - https://discuss.huggingface.co/t/tokenizing-my-novel-for-gpt-model/33532\n",
            "2541: How to add additional custom pre-tokenization processing? - https://discuss.huggingface.co/t/how-to-add-additional-custom-pre-tokenization-processing/1637\n",
            "2542: Customize FlauBERT tokenizer to split line breaks - https://discuss.huggingface.co/t/customize-flaubert-tokenizer-to-split-line-breaks/33011\n",
            "2543: How to change the size of model_max_length? - https://discuss.huggingface.co/t/how-to-change-the-size-of-model-max-length/32942\n",
            "2544: Can’t get to the source code of `tokenizer.convert_tokens_to_string` - https://discuss.huggingface.co/t/cant-get-to-the-source-code-of-tokenizer-convert-tokens-to-string/32714\n",
            "2545: Why I’m getting same result with or without using Wav2Vec2Processor? - https://discuss.huggingface.co/t/why-im-getting-same-result-with-or-without-using-wav2vec2processor/32482\n",
            "2546: How does `tokenizer().input_ids` work and how different it is from tokenizer.encode() before `model.generate()` and decoding step? - https://discuss.huggingface.co/t/how-does-tokenizer-input-ids-work-and-how-different-it-is-from-tokenizer-encode-before-model-generate-and-decoding-step/30476\n",
            "2547: What file type should my training data be? - https://discuss.huggingface.co/t/what-file-type-should-my-training-data-be/32075\n",
            "2548: Best way to get the closest token indices of input of char_to_token is a whitespace - https://discuss.huggingface.co/t/best-way-to-get-the-closest-token-indices-of-input-of-char-to-token-is-a-whitespace/32004\n",
            "2549: Token indices sequence length is longer than the specified maximum sequence length - https://discuss.huggingface.co/t/token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length/25133\n",
            "2550: Create a simple tokenizer - https://discuss.huggingface.co/t/create-a-simple-tokenizer/31677\n",
            "2551: Sliding window for Long Documents - https://discuss.huggingface.co/t/sliding-window-for-long-documents/19367\n",
            "2552: Creating tokenizer from counts file? - https://discuss.huggingface.co/t/creating-tokenizer-from-counts-file/31399\n",
            "2553: Tokenizer.train() running out of memory - https://discuss.huggingface.co/t/tokenizer-train-running-out-of-memory/31355\n",
            "2554: Tokenizing Float Tensor? - https://discuss.huggingface.co/t/tokenizing-float-tensor/30604\n",
            "2555: Padding and truncation for custom tokenizer - https://discuss.huggingface.co/t/padding-and-truncation-for-custom-tokenizer/30180\n",
            "2556: Incorporate SARI score into run_summarization.py example script - https://discuss.huggingface.co/t/incorporate-sari-score-into-run-summarization-py-example-script/29511\n",
            "2557: Is that possible to embed the tokenizer into the model to have it running on GCP using TensorFlow Serving? - https://discuss.huggingface.co/t/is-that-possible-to-embed-the-tokenizer-into-the-model-to-have-it-running-on-gcp-using-tensorflow-serving/10532\n",
            "2558: Huggingface inference API issue - https://discuss.huggingface.co/t/huggingface-inference-api-issue/29307\n",
            "2559: Using Tokenizer for integer data - https://discuss.huggingface.co/t/using-tokenizer-for-integer-data/28835\n",
            "2560: GPT2 long text approach - https://discuss.huggingface.co/t/gpt2-long-text-approach/28165\n",
            "2561: Huggingface t5 models seem to not download a tokenizer file - https://discuss.huggingface.co/t/huggingface-t5-models-seem-to-not-download-a-tokenizer-file/27935\n",
            "2562: How to save a fast tokenizer using the transformer library and then load it using Tokenizers? - https://discuss.huggingface.co/t/how-to-save-a-fast-tokenizer-using-the-transformer-library-and-then-load-it-using-tokenizers/9567\n",
            "2563: Using a BertTokenizer when training a RobertaForMaskedLM - https://discuss.huggingface.co/t/using-a-berttokenizer-when-training-a-robertaformaskedlm/27456\n",
            "2564: Need clarity on “padding” parameter in Bert Tokenizer - https://discuss.huggingface.co/t/need-clarity-on-padding-parameter-in-bert-tokenizer/27444\n",
            "2565: How to convert HuggingFace tokenizers into ONNX format? - https://discuss.huggingface.co/t/how-to-convert-huggingface-tokenizers-into-onnx-format/27272\n",
            "2566: Can’t save ConvBert tokenizer - https://discuss.huggingface.co/t/cant-save-convbert-tokenizer/16006\n",
            "2567: RoBERTa Tokenizer Java Implementation - https://discuss.huggingface.co/t/roberta-tokenizer-java-implementation/16737\n",
            "2568: Unigram vocab_size doesn’t fit - https://discuss.huggingface.co/t/unigram-vocab-size-doesnt-fit/26856\n",
            "2569: Option to load only tokenizer and model configuration into “token-classification” pipeline - https://discuss.huggingface.co/t/option-to-load-only-tokenizer-and-model-configuration-into-token-classification-pipeline/26697\n",
            "2570: Encode_plus Pretokenized input seuqence must be Union - https://discuss.huggingface.co/t/encode-plus-pretokenized-input-seuqence-must-be-union/26449\n",
            "2571: Application of TFBertTokenizer - https://discuss.huggingface.co/t/application-of-tfberttokenizer/26437\n",
            "2572: TemplateProcessing for encoder-decoder - https://discuss.huggingface.co/t/templateprocessing-for-encoder-decoder/26135\n",
            "2573: Using `TFBertTokenizer` instead of `BertTokenizer` with `TFBertForQuestionAnswering` - https://discuss.huggingface.co/t/using-tfberttokenizer-instead-of-berttokenizer-with-tfbertforquestionanswering/26127\n",
            "2574: How to concatenate an answer to multiple choices after padded tokenization - https://discuss.huggingface.co/t/how-to-concatenate-an-answer-to-multiple-choices-after-padded-tokenization/26110\n",
            "2575: Maximum recursion depth exceeded when using DataCollator - https://discuss.huggingface.co/t/maximum-recursion-depth-exceeded-when-using-datacollator/25937\n",
            "2576: Adding a special language token to MBART - https://discuss.huggingface.co/t/adding-a-special-language-token-to-mbart/25967\n",
            "2577: Custom PostProcessor? - https://discuss.huggingface.co/t/custom-postprocessor/25847\n",
            "2578: Tokenizer.pad_token=what? - https://discuss.huggingface.co/t/tokenizer-pad-token-what/24367\n",
            "2579: Using HuggingFace Tokenizers Without Special Characters - https://discuss.huggingface.co/t/using-huggingface-tokenizers-without-special-characters/21962\n",
            "2580: How to get sp_model variable from T5Tokenizer? - https://discuss.huggingface.co/t/how-to-get-sp-model-variable-from-t5tokenizer/25171\n",
            "2581: Wav2vec2CTCTokenizer and vocab.json - https://discuss.huggingface.co/t/wav2vec2ctctokenizer-and-vocab-json/25146\n",
            "2582: Period ID in RobertaTokenizer with is_split_into_words - https://discuss.huggingface.co/t/period-id-in-robertatokenizer-with-is-split-into-words/24142\n",
            "2583: WordLevel error: Missing [UNK] token from the vocabulary - https://discuss.huggingface.co/t/wordlevel-error-missing-unk-token-from-the-vocabulary/5107\n",
            "2584: Tokenizer post_processor help - https://discuss.huggingface.co/t/tokenizer-post-processor-help/21926\n",
            "2585: Preprocessing raw text - https://discuss.huggingface.co/t/preprocessing-raw-text/24559\n",
            "2586: Save tokenizer with argument - https://discuss.huggingface.co/t/save-tokenizer-with-argument/17389\n",
            "2587: Trained tokenizer API as PretrainedTokenizer - https://discuss.huggingface.co/t/trained-tokenizer-api-as-pretrainedtokenizer/23639\n",
            "2588: Remove only certain special token id during tokenizer decode - https://discuss.huggingface.co/t/remove-only-certain-special-token-id-during-tokenizer-decode/20436\n",
            "2589: Convert_tokens_to_ids produces <unk> - https://discuss.huggingface.co/t/convert-tokens-to-ids-produces-unk/24771\n",
            "2590: Text preprocessing for fitting Tokenizer model - https://discuss.huggingface.co/t/text-preprocessing-for-fitting-tokenizer-model/24869\n",
            "2591: Special tokens warning - https://discuss.huggingface.co/t/special-tokens-warning/24939\n",
            "2592: Simple Transformers Multilabelclassification - https://discuss.huggingface.co/t/simple-transformers-multilabelclassification/24568\n",
            "2593: Cannot initialize deberta-v3-base tokenizer - https://discuss.huggingface.co/t/cannot-initialize-deberta-v3-base-tokenizer/15322\n",
            "2594: Getting Wholeword corresponding to a subword in a text? - https://discuss.huggingface.co/t/getting-wholeword-corresponding-to-a-subword-in-a-text/24152\n",
            "2595: Issue with pushing tokenizer to hub - https://discuss.huggingface.co/t/issue-with-pushing-tokenizer-to-hub/24120\n",
            "2596: How do we customize the number of entites for NER pretrained model? - https://discuss.huggingface.co/t/how-do-we-customize-the-number-of-entites-for-ner-pretrained-model/24085\n",
            "2597: Configure RobertaTokenizer - https://discuss.huggingface.co/t/configure-robertatokenizer/23969\n",
            "2598: How to properly clean vocabulary from BBPE tokenizer - https://discuss.huggingface.co/t/how-to-properly-clean-vocabulary-from-bbpe-tokenizer/22827\n",
            "2599: Map tokenization and posterior to smaller substrings - https://discuss.huggingface.co/t/map-tokenization-and-posterior-to-smaller-substrings/23792\n",
            "2600: T5 model tokenizer - https://discuss.huggingface.co/t/t5-model-tokenizer/23640\n",
            "2601: Fast tokenizer for marianMTModel - https://discuss.huggingface.co/t/fast-tokenizer-for-marianmtmodel/23638\n",
            "2602: Word tokenizers for text generators - https://discuss.huggingface.co/t/word-tokenizers-for-text-generators/23464\n",
            "2603: SentencePieceUnigramTokenizer - https://discuss.huggingface.co/t/sentencepieceunigramtokenizer/23509\n",
            "2604: Tokenizer is not being loaded on Huggingface Inference - https://discuss.huggingface.co/t/tokenizer-is-not-being-loaded-on-huggingface-inference/23505\n",
            "2605: Why is BertNormalizer not exposed on the tokenizers library? - https://discuss.huggingface.co/t/why-is-bertnormalizer-not-exposed-on-the-tokenizers-library/23340\n",
            "2606: Sentence splitting - https://discuss.huggingface.co/t/sentence-splitting/5393\n",
            "2607: Average time to train a SentencePieceBPETokenizer - https://discuss.huggingface.co/t/average-time-to-train-a-sentencepiecebpetokenizer/23081\n",
            "2608: 1 line code for NER data set preparation using tokenizer library! - https://discuss.huggingface.co/t/1-line-code-for-ner-data-set-preparation-using-tokenizer-library/22816\n",
            "2609: Microsoft/codebert-base produces two sep tokens - https://discuss.huggingface.co/t/microsoft-codebert-base-produces-two-sep-tokens/21366\n",
            "2610: Padding with sliding window - https://discuss.huggingface.co/t/padding-with-sliding-window/18921\n",
            "2611: Find which tokens are unknown in new data - https://discuss.huggingface.co/t/find-which-tokens-are-unknown-in-new-data/22466\n",
            "2612: How to train target tokenizer - https://discuss.huggingface.co/t/how-to-train-target-tokenizer/22291\n",
            "2613: How to know if a subtoken is a word or part of a word? - https://discuss.huggingface.co/t/how-to-know-if-a-subtoken-is-a-word-or-part-of-a-word/923\n",
            "2614: BART Tokenizer tokenises same word differently? - https://discuss.huggingface.co/t/bart-tokenizer-tokenises-same-word-differently/21835\n",
            "2615: Fine-tuned BERT tokenizer taking too long to load - https://discuss.huggingface.co/t/fine-tuned-bert-tokenizer-taking-too-long-to-load/9747\n",
            "2616: Add BOS and EOS when encoding a sentence - https://discuss.huggingface.co/t/add-bos-and-eos-when-encoding-a-sentence/21833\n",
            "2617: Customization of Wav2Vec2CTCTokenizer with rules - https://discuss.huggingface.co/t/customization-of-wav2vec2ctctokenizer-with-rules/21912\n",
            "2618: Customized tokenization files in run_clm script - https://discuss.huggingface.co/t/customized-tokenization-files-in-run-clm-script/21460\n",
            "2619: Using customized algorithm - https://discuss.huggingface.co/t/using-customized-algorithm/21753\n",
            "2620: Issue with Flaubert Tokenizer as word_ids() method is not available for NER Task - https://discuss.huggingface.co/t/issue-with-flaubert-tokenizer-as-word-ids-method-is-not-available-for-ner-task/20374\n",
            "2621: Word_ids not working with deberta_v2 - https://discuss.huggingface.co/t/word-ids-not-working-with-deberta-v2/21523\n",
            "2622: How to tokenize large contexts without running out of memory - https://discuss.huggingface.co/t/how-to-tokenize-large-contexts-without-running-out-of-memory/5882\n",
            "2623: Does Deberta tokenizer use wordpiece? - https://discuss.huggingface.co/t/does-deberta-tokenizer-use-wordpiece/21307\n",
            "2624: Get vocabulary tokens in order to exclude them from generate function - https://discuss.huggingface.co/t/get-vocabulary-tokens-in-order-to-exclude-them-from-generate-function/5192\n",
            "2625: Avoid creating certain tokens when training a tokenizer - https://discuss.huggingface.co/t/avoid-creating-certain-tokens-when-training-a-tokenizer/20864\n",
            "2626: Error finetuning XLM-RoBERTa-Large when training - https://discuss.huggingface.co/t/error-finetuning-xlm-roberta-large-when-training/20412\n",
            "2627: HuggingFace BPE Trainer Error - Training Tokenizer - https://discuss.huggingface.co/t/huggingface-bpe-trainer-error-training-tokenizer/10629\n",
            "2628: Word_to_tokens() and word_ids() —- microsoft/deberta-v2/v3 - https://discuss.huggingface.co/t/word-to-tokens-and-word-ids-microsoft-deberta-v2-v3/19984\n",
            "2629: No PreTrainedTokenizerFast for Deberta-V3, no doc_stride - https://discuss.huggingface.co/t/no-pretrainedtokenizerfast-for-deberta-v3-no-doc-stride/20345\n",
            "2630: Tokenizer from own vocab - https://discuss.huggingface.co/t/tokenizer-from-own-vocab/20245\n",
            "2631: No labels column for tokenized data - https://discuss.huggingface.co/t/no-labels-column-for-tokenized-data/19540\n",
            "2632: Programmatic way to Tokenization on Custom Text Columns - https://discuss.huggingface.co/t/programmatic-way-to-tokenization-on-custom-text-columns/19679\n",
            "2633: Bug in Offset generation for Rupee symbol - https://discuss.huggingface.co/t/bug-in-offset-generation-for-rupee-symbol/19655\n",
            "2634: How to handle parenthesis, quotation marks, \\n etc when creating tokenizer from scratch - https://discuss.huggingface.co/t/how-to-handle-parenthesis-quotation-marks-n-etc-when-creating-tokenizer-from-scratch/19628\n",
            "2635: EM training on unigram tokenizer taking way longer than predicted - https://discuss.huggingface.co/t/em-training-on-unigram-tokenizer-taking-way-longer-than-predicted/19502\n",
            "2636: Training unigram on long sequences - https://discuss.huggingface.co/t/training-unigram-on-long-sequences/19188\n",
            "2637: Issue with post-processing - https://discuss.huggingface.co/t/issue-with-post-processing/2703\n",
            "2638: FutureWarning about BertTokenizer.from_pretrained() at latest version - https://discuss.huggingface.co/t/futurewarning-about-berttokenizer-from-pretrained-at-latest-version/18750\n",
            "2639: Enhaced word_ids() API for Chinese or CJK languages? - https://discuss.huggingface.co/t/enhaced-word-ids-api-for-chinese-or-cjk-languages/18662\n",
            "2640: Importing tokenizers version >0.10.3 fails due to openssl - https://discuss.huggingface.co/t/importing-tokenizers-version-0-10-3-fails-due-to-openssl/17820\n",
            "2641: Lower case with input ids - https://discuss.huggingface.co/t/lower-case-with-input-ids/18490\n",
            "2642: Dialogue classification - https://discuss.huggingface.co/t/dialogue-classification/18466\n",
            "2643: Multilang bert vs translating to english - https://discuss.huggingface.co/t/multilang-bert-vs-translating-to-english/18454\n",
            "2644: pyo3_runtime.PanicException: likelihood is NAN. Input sentence may be too long - https://discuss.huggingface.co/t/pyo3-runtime-panicexception-likelihood-is-nan-input-sentence-may-be-too-long/18398\n",
            "2645: Pytorch_model.bin not working because of lfs - https://discuss.huggingface.co/t/pytorch-model-bin-not-working-because-of-lfs/18290\n",
            "2646: Tokenizer ignores repeated whitespaces - https://discuss.huggingface.co/t/tokenizer-ignores-repeated-whitespaces/17864\n",
            "2647: How truncation works when applying BERT tokenizer on the batch of sentence pairs in HuggingFace? - https://discuss.huggingface.co/t/how-truncation-works-when-applying-bert-tokenizer-on-the-batch-of-sentence-pairs-in-huggingface/17977\n",
            "2648: How to save a tokenizer only consisting of added tokens - https://discuss.huggingface.co/t/how-to-save-a-tokenizer-only-consisting-of-added-tokens/17825\n",
            "2649: Passing list of inputs to tokenize - https://discuss.huggingface.co/t/passing-list-of-inputs-to-tokenize/17499\n",
            "2650: Issues with Data Collator and Tokenizing with NER Datasets - https://discuss.huggingface.co/t/issues-with-data-collator-and-tokenizing-with-ner-datasets/17489\n",
            "2651: How to perform tokenization on an ONNX model in JS? - https://discuss.huggingface.co/t/how-to-perform-tokenization-on-an-onnx-model-in-js/17581\n",
            "2652: Using the Tokenizers library in a Unity project - https://discuss.huggingface.co/t/using-the-tokenizers-library-in-a-unity-project/17533\n",
            "2653: Further pre-training the tokenizer? - https://discuss.huggingface.co/t/further-pre-training-the-tokenizer/17360\n",
            "2654: Error when doing tokenization - https://discuss.huggingface.co/t/error-when-doing-tokenization/17344\n",
            "2655: How to decode with spaces? - https://discuss.huggingface.co/t/how-to-decode-with-spaces/17289\n",
            "2656: Show Submodels of PegasusTokenizer - https://discuss.huggingface.co/t/show-submodels-of-pegasustokenizer/17286\n",
            "2657: Load SentencePieceBPETokenizer in TF - https://discuss.huggingface.co/t/load-sentencepiecebpetokenizer-in-tf/17257\n",
            "2658: Best way to mask a multi-token word when using `.*ForMaskedLM` models - https://discuss.huggingface.co/t/best-way-to-mask-a-multi-token-word-when-using-formaskedlm-models/6428\n",
            "2659: Does a tokenizer keep the mapping between my labels to their encoding? - https://discuss.huggingface.co/t/does-a-tokenizer-keep-the-mapping-between-my-labels-to-their-encoding/16296\n",
            "2660: What is Wav2Vec2FeatureExtractor doing? - https://discuss.huggingface.co/t/what-is-wav2vec2featureextractor-doing/16423\n",
            "2661: What does this warning mean? -overflowing tokens are not returned for the setting you have chosen - https://discuss.huggingface.co/t/what-does-this-warning-mean-overflowing-tokens-are-not-returned-for-the-setting-you-have-chosen/11594\n",
            "2662: How can I make sure Tokenizer pads to a fixed length? - https://discuss.huggingface.co/t/how-can-i-make-sure-tokenizer-pads-to-a-fixed-length/16116\n",
            "2663: Issue with Decoding in HuggingFace - https://discuss.huggingface.co/t/issue-with-decoding-in-huggingface/15699\n",
            "2664: ValueError: Unable to create tensor for 1 dataset but not the other of same type - https://discuss.huggingface.co/t/valueerror-unable-to-create-tensor-for-1-dataset-but-not-the-other-of-same-type/15995\n",
            "2665: Disabling addition of CLS from BERT tokenizer - https://discuss.huggingface.co/t/disabling-addition-of-cls-from-bert-tokenizer/15538\n",
            "2666: Tokenized sequence lengths - https://discuss.huggingface.co/t/tokenized-sequence-lengths/15117\n",
            "2667: Finetuning GPT-J6B for custom dataset - https://discuss.huggingface.co/t/finetuning-gpt-j6b-for-custom-dataset/9924\n",
            "2668: Training tokenizer takes too much RAM - https://discuss.huggingface.co/t/training-tokenizer-takes-too-much-ram/14256\n",
            "2669: How to “further pretrain” a tokenizer (do I need to do so?) - https://discuss.huggingface.co/t/how-to-further-pretrain-a-tokenizer-do-i-need-to-do-so/14719\n",
            "2670: Run_seq2seq_qa.py: Column 3 named labels expected length 1007 but got length 1000 - https://discuss.huggingface.co/t/run-seq2seq-qa-py-column-3-named-labels-expected-length-1007-but-got-length-1000/13237\n",
            "2671: Issues with offset_mapping values - https://discuss.huggingface.co/t/issues-with-offset-mapping-values/4237\n",
            "2672: How would you train a sentencepiece BPE tokenizer on this language with 400 “characters”? - https://discuss.huggingface.co/t/how-would-you-train-a-sentencepiece-bpe-tokenizer-on-this-language-with-400-characters/14669\n",
            "2673: All my sequences get tokenized the same - https://discuss.huggingface.co/t/all-my-sequences-get-tokenized-the-same/14657\n",
            "2674: Combine multiple sentences together during tokenization - https://discuss.huggingface.co/t/combine-multiple-sentences-together-during-tokenization/3430\n",
            "2675: NER tag , aggregation stratergy - https://discuss.huggingface.co/t/ner-tag-aggregation-stratergy/14199\n",
            "2676: How to ensure that tokenizers never truncate partial words? - https://discuss.huggingface.co/t/how-to-ensure-that-tokenizers-never-truncate-partial-words/14024\n",
            "2677: How to ensure the `overflow` with `stride` always starts with a full word? - https://discuss.huggingface.co/t/how-to-ensure-the-overflow-with-stride-always-starts-with-a-full-word/14030\n",
            "2678: Adding new tokens to a BERT tokenizer - Getting ValueError - https://discuss.huggingface.co/t/adding-new-tokens-to-a-bert-tokenizer-getting-valueerror/9253\n",
            "2679: Adding token to t5-base vocab does not respect space - https://discuss.huggingface.co/t/adding-token-to-t5-base-vocab-does-not-respect-space/13662\n",
            "2680: How can I change the token id of a special token? - https://discuss.huggingface.co/t/how-can-i-change-the-token-id-of-a-special-token/13445\n",
            "2681: Import distilbert-base-uncased tokenizer to an android app along with the tflite model - https://discuss.huggingface.co/t/import-distilbert-base-uncased-tokenizer-to-an-android-app-along-with-the-tflite-model/3234\n",
            "2682: What are the equivalent manner for using texts_to_sequences? - https://discuss.huggingface.co/t/what-are-the-equivalent-manner-for-using-texts-to-sequences/13220\n",
            "2683: ERROR?why encoding [MASK] before ‘.’ would gain a idx 13? - https://discuss.huggingface.co/t/error-why-encoding-mask-before-would-gain-a-idx-13/2897\n",
            "2684: LongFormer tokenizer has the same token_type_ids for sequence pairs - https://discuss.huggingface.co/t/longformer-tokenizer-has-the-same-token-type-ids-for-sequence-pairs/12992\n",
            "2685: Batch encode plus in Rust Tokenizers - https://discuss.huggingface.co/t/batch-encode-plus-in-rust-tokenizers/12722\n",
            "2686: Best solution for train tokenizer and MLM from scratch - https://discuss.huggingface.co/t/best-solution-for-train-tokenizer-and-mlm-from-scratch/12579\n",
            "2687: Implementing custom tokenizer components (normalizers, processors) - https://discuss.huggingface.co/t/implementing-custom-tokenizer-components-normalizers-processors/12371\n",
            "2688: Does T5Tokenizer support the Greek language? - https://discuss.huggingface.co/t/does-t5tokenizer-support-the-greek-language/12224\n",
            "2689: How padding in huggingface tokenizer works? - https://discuss.huggingface.co/t/how-padding-in-huggingface-tokenizer-works/12161\n",
            "2690: Why we need to add special tokens to tasks other than classification? - https://discuss.huggingface.co/t/why-we-need-to-add-special-tokens-to-tasks-other-than-classification/11975\n",
            "2691: How to configure TokenizerFast for AutoTokenizer - https://discuss.huggingface.co/t/how-to-configure-tokenizerfast-for-autotokenizer/11353\n",
            "2692: How to employ different vocabs for encoder and decoder respectively? - https://discuss.huggingface.co/t/how-to-employ-different-vocabs-for-encoder-and-decoder-respectively/11497\n",
            "2693: How to use tokenizer.tokenize in Chinese data properly? - https://discuss.huggingface.co/t/how-to-use-tokenizer-tokenize-in-chinese-data-properly/11491\n",
            "2694: Mask only specific words - https://discuss.huggingface.co/t/mask-only-specific-words/173\n",
            "2695: Load custom pretrained tokenizer - https://discuss.huggingface.co/t/load-custom-pretrained-tokenizer/11148\n",
            "2696: Using Custom Vocab.txt - https://discuss.huggingface.co/t/using-custom-vocab-txt/10847\n",
            "2697: Tokenizer.encode not returning encodings - https://discuss.huggingface.co/t/tokenizer-encode-not-returning-encodings/10616\n",
            "2698: There is no 0.11.0 tokenizers in pip - https://discuss.huggingface.co/t/there-is-no-0-11-0-tokenizers-in-pip/10381\n",
            "2699: Performance difference between ByteLevelBPE and Wordpiece tokenizers - https://discuss.huggingface.co/t/performance-difference-between-bytelevelbpe-and-wordpiece-tokenizers/10203\n",
            "2700: Should have a `model_type` key in its config.json - https://discuss.huggingface.co/t/should-have-a-model-type-key-in-its-config-json/10144\n",
            "2701: Using a fixed vocab.txt with AutoTokenizer? - https://discuss.huggingface.co/t/using-a-fixed-vocab-txt-with-autotokenizer/9919\n",
            "2702: Train wordpiece from scratch - https://discuss.huggingface.co/t/train-wordpiece-from-scratch/9843\n",
            "2703: I set up a different batch_size, but the time of data processing has not changed - https://discuss.huggingface.co/t/i-set-up-a-different-batch-size-but-the-time-of-data-processing-has-not-changed/9668\n",
            "2704: Cannot create an identical PretrainedTokenizerFast object from a Tokenizer created by tokenizers library - https://discuss.huggingface.co/t/cannot-create-an-identical-pretrainedtokenizerfast-object-from-a-tokenizer-created-by-tokenizers-library/9317\n",
            "2705: Index of wordpieces (subwords) after tokenization by transformers - https://discuss.huggingface.co/t/index-of-wordpieces-subwords-after-tokenization-by-transformers/9552\n",
            "2706: A problem about FutureWarning？ - https://discuss.huggingface.co/t/a-problem-about-futurewarning/9323\n",
            "2707: Extracting embedding values of NLP pertained models from tokenized strings - https://discuss.huggingface.co/t/extracting-embedding-values-of-nlp-pertained-models-from-tokenized-strings/9287\n",
            "2708: Tokenization in a NER context - https://discuss.huggingface.co/t/tokenization-in-a-ner-context/5635\n",
            "2709: Unable to convert output to interpretable format - https://discuss.huggingface.co/t/unable-to-convert-output-to-interpretable-format/8882\n",
            "2710: BpeTrainer implementation in Python - https://discuss.huggingface.co/t/bpetrainer-implementation-in-python/8625\n",
            "2711: MBart50Tokenizer vs XLMRobertaTokenizer - https://discuss.huggingface.co/t/mbart50tokenizer-vs-xlmrobertatokenizer/8498\n",
            "2712: Why multilingual BERT tokenizer doesn’t remove accent markers? - https://discuss.huggingface.co/t/why-multilingual-bert-tokenizer-doesnt-remove-accent-markers/8468\n",
            "2713: TypeError when loading tokenizer with from_pretrained method for bart-large-mnli model - https://discuss.huggingface.co/t/typeerror-when-loading-tokenizer-with-from-pretrained-method-for-bart-large-mnli-model/3378\n",
            "2714: Is it okay to split ids sequence when it is encoded using Byte-level BPE - https://discuss.huggingface.co/t/is-it-okay-to-split-ids-sequence-when-it-is-encoded-using-byte-level-bpe/8129\n",
            "2715: Using truncated fragments as input samples in training - https://discuss.huggingface.co/t/using-truncated-fragments-as-input-samples-in-training/6978\n",
            "2716: Using whitespace tokenizer for training models - https://discuss.huggingface.co/t/using-whitespace-tokenizer-for-training-models/6591\n",
            "2717: Save custom components - https://discuss.huggingface.co/t/save-custom-components/6458\n",
            "2718: How to see contents of a normalizer - https://discuss.huggingface.co/t/how-to-see-contents-of-a-normalizer/6045\n",
            "2719: Newbie: Main difference between tokenizers? - https://discuss.huggingface.co/t/newbie-main-difference-between-tokenizers/6035\n",
            "2720: Can’t load tokenizer for ‘sshleifer/student_blarge_12_3’ - https://discuss.huggingface.co/t/cant-load-tokenizer-for-sshleifer-student-blarge-12-3/6027\n",
            "2721: How to create a Huggingface tokenizer from a non-Huggingface tokenizer? - https://discuss.huggingface.co/t/how-to-create-a-huggingface-tokenizer-from-a-non-huggingface-tokenizer/5983\n",
            "2722: Add new tokens and learn the embeddings of the new tokens and keeping all the other parametes frozen - https://discuss.huggingface.co/t/add-new-tokens-and-learn-the-embeddings-of-the-new-tokens-and-keeping-all-the-other-parametes-frozen/5896\n",
            "2723: How do you use SentencePiece for BPE of sequences with no whitespace - https://discuss.huggingface.co/t/how-do-you-use-sentencepiece-for-bpe-of-sequences-with-no-whitespace/1895\n",
            "2724: BOS tokens for mBERT tokenizer - https://discuss.huggingface.co/t/bos-tokens-for-mbert-tokenizer/5467\n",
            "2725: BertTokenizerFast for stsb-xlm-r-multilingual model - https://discuss.huggingface.co/t/berttokenizerfast-for-stsb-xlm-r-multilingual-model/4742\n",
            "2726: Skip-gram tokens - https://discuss.huggingface.co/t/skip-gram-tokens/5294\n",
            "2727: Using a BertWordPieceTokenizer trained from scratch from transformers - https://discuss.huggingface.co/t/using-a-bertwordpiecetokenizer-trained-from-scratch-from-transformers/4391\n",
            "2728: Questions on model’s tokens - https://discuss.huggingface.co/t/questions-on-models-tokens/5023\n",
            "2729: Space token ’ ’ cannot be add when is_split_into_words = True - https://discuss.huggingface.co/t/space-token-cannot-be-add-when-is-split-into-words-true/4305\n",
            "2730: Are special_tokens the only tokens guaranteed to be atomic? - https://discuss.huggingface.co/t/are-special-tokens-the-only-tokens-guaranteed-to-be-atomic/4124\n",
            "2731: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "2732: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "2733: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "2734: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "2735: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "2736: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "2737: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "2738: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "2739: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "2740: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "2741: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "2742: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "2743: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "2744: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "2745: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "2746: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "2747: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "2748: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "2749: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "2750: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "2751: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "2752: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "2753: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "2754: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "2755: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "2756: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "2757: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "2758: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "2759: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "2760: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "2761: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "2762: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "2763: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "2764: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "2765: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "2766: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "2767: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "2768: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "2769: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "2770: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "2771: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "2772: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "2773: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "2774: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "2775: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "2776: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "2777: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "2778: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "2779: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "2780: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "2781: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "2782: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "2783: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "2784: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "2785: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "2786: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "2787: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "2788: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "2789: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "2790: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "2791: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "2792: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "2793: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "2794: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "2795: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "2796: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "2797: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "2798: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "2799: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "2800: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "2801: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "2802: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "2803: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "2804: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "2805: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "2806: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "2807: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "2808: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "2809: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "2810: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "2811: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "2812: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "2813: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "2814: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "2815: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "2816: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "2817: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "2818: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "2819: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "2820: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "2821: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "2822: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "2823: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "2824: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "2825: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "2826: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "2827: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "2828: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "2829: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "2830: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "2831: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "2832: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "2833: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "2834: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "2835: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "2836: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "2837: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "2838: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "2839: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "2840: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "2841: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "2842: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "2843: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "2844: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "2845: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "2846: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "2847: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "2848: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "2849: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "2850: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "2851: BertTokenizer.decode not understanding new vocabulary - https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "2852: Tokenizer tend to choose added tokens first rather than token in vocab - https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "2853: Special token printed out as output - https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948\n",
            "2854: [NER][Japanese] labeled segment shorter than token - https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "2855: T5Tokenizer add a whitespace token after added special tokens - https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "2856: Unable to register my own tokenizer - https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "2857: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab! - https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "2858: Use Unicode blocks in regex (in Replace normalizer) - https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904\n",
            "2859: How to handle translations one source language to many target sentences for the same language - https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "2860: NER Label tokenization with overflowing tokens - https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561\n",
            "2861: How to use a trained tokenizer for semantic search? - https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "2862: Unk_token not set after training a BPETokenizer tokenizer - https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733\n",
            "2863: AutoTokenizer is very slow when loading llama tokenizer - https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547\n",
            "2864: Offset mappings differ for tokenizers - https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424\n",
            "2865: The process for tokenizing concatenated dataset is slow st the end of tokenizing - https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "2866: Batch tokenize (split into tokens, without processing) - https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238\n",
            "2867: Does AutoTokenizer uploads data to HuggingFace - https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "2868: RobertaTokenizer decode and tokenize do not have the same output - https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "2869: Training tokenizers with padding in between tokens - https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "2870: Leaving unknown words untokenized like in OpenMNT - https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "2871: Keeping special chars in translations - https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270\n",
            "2872: Should cls_token be [CLS] or <cls>? - https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "2873: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False - https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "2874: I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error - https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742\n",
            "2875: OSError: Can’t load tokenizer for ‘facebook/xmod-base’ - https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471\n",
            "2876: `GPT2Tokenizer` Tokenizer handling `\\n\\n` differently in different settings - https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289\n",
            "2877: DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‘padding=True’ ‘truncation=True’ to have batched tensors with the same length - https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625\n",
            "2878: Decode token IDs into a list (not a single string) - https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991\n",
            "2879: Error training MLM with Roberta Tokenizer - https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878\n",
            "2880: Are the slow and fast tokenizer results the same output for the same input? - https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "2881: “Add_tokens” breaks words when encoding - https://discuss.huggingface.co/t/add-tokens-breaks-words-when-encoding/21154\n",
            "2882: 2 tokens for one character in T5 - https://discuss.huggingface.co/t/2-tokens-for-one-character-in-t5/14960\n",
            "2883: OSError: Model name ‘gpt2’ was not found in tokenizers model name list (gpt2,…) - https://discuss.huggingface.co/t/oserror-model-name-gpt2-was-not-found-in-tokenizers-model-name-list-gpt2/2164\n",
            "2884: DNA long sequence tokenization - https://discuss.huggingface.co/t/dna-long-sequence-tokenization/16154\n",
            "2885: SentencePiece tokenizer encodes to unknown token - https://discuss.huggingface.co/t/sentencepiece-tokenizer-encodes-to-unknown-token/49113\n",
            "2886: Tokenizer behaviour with pipeline - https://discuss.huggingface.co/t/tokenizer-behaviour-with-pipeline/48969\n",
            "2887: Load tokenizer from file : Exception: data did not match any variant of untagged enum ModelWrapper - https://discuss.huggingface.co/t/load-tokenizer-from-file-exception-data-did-not-match-any-variant-of-untagged-enum-modelwrapper/25325\n",
            "2888: ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076 - https://discuss.huggingface.co/t/arrowinvalid-column-3-named-attention-mask-expected-length-1000-but-got-length-1076/6904\n",
            "2889: Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model - https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829\n",
            "2890: Initialize Vocabulary for Unigram Tokenizer - https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371\n",
            "2891: Make correct padding for text generation with GPT-NEO - https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800\n",
            "2892: How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers? - https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639\n",
            "2893: Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF - https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602\n",
            "2894: Writing custom tokenizer and wrapping it in tokenizer object - https://discuss.huggingface.co/t/writing-custom-tokenizer-and-wrapping-it-in-tokenizer-object/39679\n",
            "2895: Tokenizer for German lang - https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222\n",
            "2896: Chunk tokens into desired chunk length without simply getting rid of rest of tokens - https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399\n",
            "2897: Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers - https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872\n",
            "2898: LlamaTokenizerFast returns token_type_ids but the forward pass of the LlamaModel does not receive token_type_ids - https://discuss.huggingface.co/t/llamatokenizerfast-returns-token-type-ids-but-the-forward-pass-of-the-llamamodel-does-not-receive-token-type-ids/42431\n",
            "2899: GPT2Tokenizer not working in Kaggle Notebook - https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508\n",
            "2900: Exploring the Majestic Temples in Karnataka - https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952\n",
            "2901: Tokenizer producing token index greater than size of the dictionary - https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989\n",
            "2902: How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer - https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843\n",
            "2903: How to create a HF tokenizer’s vocab file from a BPE model’s merges.txt file? - https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737\n",
            "2904: Scala/JVM Bindings for Tokenizers - https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393\n",
            "2905: Tokenizers Wheel Takes Forever to Build - https://discuss.huggingface.co/t/tokenizers-wheel-takes-forever-to-build/17114\n",
            "2906: Where the introduction of tokenizers.implementations? - https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959\n",
            "2907: How to return custom `token_type_ids` or other values from a tokenizer? - https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525\n",
            "2908: Easy way to compare tokenizers - https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347\n",
            "2909: Unable to load image using llama-index - https://discuss.huggingface.co/t/unable-to-load-image-using-llama-index/38304\n",
            "2910: Help defining tokenizer - https://discuss.huggingface.co/t/help-defining-tokenizer/38091\n",
            "2911: Token Offsets in Rust vs. Python - https://discuss.huggingface.co/t/token-offsets-in-rust-vs-python/37949\n",
            "2912: “OSError: Model name ‘./XX’ was not found in tokenizers model name list” - cannot load custom tokenizer in Transformers - https://discuss.huggingface.co/t/oserror-model-name-xx-was-not-found-in-tokenizers-model-name-list-cannot-load-custom-tokenizer-in-transformers/2714\n",
            "2913: Converting JSON/dict to flatten string with indicator tokens - https://discuss.huggingface.co/t/converting-json-dict-to-flatten-string-with-indicator-tokens/37387\n",
            "2914: Train Retry Tokenizer - https://discuss.huggingface.co/t/train-retry-tokenizer/37031\n",
            "2915: Pretokenise on punctuation except hyphens - https://discuss.huggingface.co/t/pretokenise-on-punctuation-except-hyphens/36691\n",
            "2916: Tokenizer Trainer Crashing - https://discuss.huggingface.co/t/tokenizer-trainer-crashing/36645\n",
            "2917: Tokenizer extremely slow when deployed to a container - https://discuss.huggingface.co/t/tokenizer-extremely-slow-when-deployed-to-a-container/36569\n",
            "2918: Dealing with Decimal and Fractions - https://discuss.huggingface.co/t/dealing-with-decimal-and-fractions/23377\n",
            "2919: `add_tokens` with argument `special_tokens=True` vs `add_special_tokens` - https://discuss.huggingface.co/t/add-tokens-with-argument-special-tokens-true-vs-add-special-tokens/35648\n",
            "2920: Unable to upload custom Pytorch model in huggingface - https://discuss.huggingface.co/t/unable-to-upload-custom-pytorch-model-in-huggingface/35545\n",
            "2921: RuntimeError: Cannot re-initialize CUDA in forked subprocess - https://discuss.huggingface.co/t/runtimeerror-cannot-re-initialize-cuda-in-forked-subprocess/28392\n",
            "2922: Overflowing Tokens in MarkupLM - https://discuss.huggingface.co/t/overflowing-tokens-in-markuplm/35217\n",
            "2923: I get the predicted token as ` े` . What am I doing wrong? - https://discuss.huggingface.co/t/i-get-the-predicted-token-as-what-am-i-doing-wrong/29038\n",
            "2924: <unk> token in the output instead curly braces - https://discuss.huggingface.co/t/unk-token-in-the-output-instead-curly-braces/34653\n",
            "2925: How to add a new token without expanding the vocabulary - https://discuss.huggingface.co/t/how-to-add-a-new-token-without-expanding-the-vocabulary/34536\n",
            "2926: Does the ByteLevelBPETokenizer need to be wrapped in a normal Tokenizer? - https://discuss.huggingface.co/t/does-the-bytelevelbpetokenizer-need-to-be-wrapped-in-a-normal-tokenizer/34121\n",
            "2927: What is required to create a fast tokenizer? For example for a Marian model - https://discuss.huggingface.co/t/what-is-required-to-create-a-fast-tokenizer-for-example-for-a-marian-model/33941\n",
            "2928: GPT2Tokenizer.decode maps unicode sequences to the same string ‘�’ - https://discuss.huggingface.co/t/gpt2tokenizer-decode-maps-unicode-sequences-to-the-same-string/32453\n",
            "2929: Issue with Tokenizer - https://discuss.huggingface.co/t/issue-with-tokenizer/33796\n",
            "2930: Tokenizing my novel for GPT model - https://discuss.huggingface.co/t/tokenizing-my-novel-for-gpt-model/33532\n",
            "2931: How to add additional custom pre-tokenization processing? - https://discuss.huggingface.co/t/how-to-add-additional-custom-pre-tokenization-processing/1637\n",
            "2932: Customize FlauBERT tokenizer to split line breaks - https://discuss.huggingface.co/t/customize-flaubert-tokenizer-to-split-line-breaks/33011\n",
            "2933: How to change the size of model_max_length? - https://discuss.huggingface.co/t/how-to-change-the-size-of-model-max-length/32942\n",
            "2934: Can’t get to the source code of `tokenizer.convert_tokens_to_string` - https://discuss.huggingface.co/t/cant-get-to-the-source-code-of-tokenizer-convert-tokens-to-string/32714\n",
            "2935: Why I’m getting same result with or without using Wav2Vec2Processor? - https://discuss.huggingface.co/t/why-im-getting-same-result-with-or-without-using-wav2vec2processor/32482\n",
            "2936: How does `tokenizer().input_ids` work and how different it is from tokenizer.encode() before `model.generate()` and decoding step? - https://discuss.huggingface.co/t/how-does-tokenizer-input-ids-work-and-how-different-it-is-from-tokenizer-encode-before-model-generate-and-decoding-step/30476\n",
            "2937: What file type should my training data be? - https://discuss.huggingface.co/t/what-file-type-should-my-training-data-be/32075\n",
            "2938: Best way to get the closest token indices of input of char_to_token is a whitespace - https://discuss.huggingface.co/t/best-way-to-get-the-closest-token-indices-of-input-of-char-to-token-is-a-whitespace/32004\n",
            "2939: Token indices sequence length is longer than the specified maximum sequence length - https://discuss.huggingface.co/t/token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length/25133\n",
            "2940: Create a simple tokenizer - https://discuss.huggingface.co/t/create-a-simple-tokenizer/31677\n",
            "2941: Sliding window for Long Documents - https://discuss.huggingface.co/t/sliding-window-for-long-documents/19367\n",
            "2942: Creating tokenizer from counts file? - https://discuss.huggingface.co/t/creating-tokenizer-from-counts-file/31399\n",
            "2943: Tokenizer.train() running out of memory - https://discuss.huggingface.co/t/tokenizer-train-running-out-of-memory/31355\n",
            "2944: Tokenizing Float Tensor? - https://discuss.huggingface.co/t/tokenizing-float-tensor/30604\n",
            "2945: Padding and truncation for custom tokenizer - https://discuss.huggingface.co/t/padding-and-truncation-for-custom-tokenizer/30180\n",
            "2946: Incorporate SARI score into run_summarization.py example script - https://discuss.huggingface.co/t/incorporate-sari-score-into-run-summarization-py-example-script/29511\n",
            "2947: Is that possible to embed the tokenizer into the model to have it running on GCP using TensorFlow Serving? - https://discuss.huggingface.co/t/is-that-possible-to-embed-the-tokenizer-into-the-model-to-have-it-running-on-gcp-using-tensorflow-serving/10532\n",
            "2948: Huggingface inference API issue - https://discuss.huggingface.co/t/huggingface-inference-api-issue/29307\n",
            "2949: Using Tokenizer for integer data - https://discuss.huggingface.co/t/using-tokenizer-for-integer-data/28835\n",
            "2950: GPT2 long text approach - https://discuss.huggingface.co/t/gpt2-long-text-approach/28165\n",
            "2951: Huggingface t5 models seem to not download a tokenizer file - https://discuss.huggingface.co/t/huggingface-t5-models-seem-to-not-download-a-tokenizer-file/27935\n",
            "2952: How to save a fast tokenizer using the transformer library and then load it using Tokenizers? - https://discuss.huggingface.co/t/how-to-save-a-fast-tokenizer-using-the-transformer-library-and-then-load-it-using-tokenizers/9567\n",
            "2953: Using a BertTokenizer when training a RobertaForMaskedLM - https://discuss.huggingface.co/t/using-a-berttokenizer-when-training-a-robertaformaskedlm/27456\n",
            "2954: Need clarity on “padding” parameter in Bert Tokenizer - https://discuss.huggingface.co/t/need-clarity-on-padding-parameter-in-bert-tokenizer/27444\n",
            "2955: How to convert HuggingFace tokenizers into ONNX format? - https://discuss.huggingface.co/t/how-to-convert-huggingface-tokenizers-into-onnx-format/27272\n",
            "2956: Can’t save ConvBert tokenizer - https://discuss.huggingface.co/t/cant-save-convbert-tokenizer/16006\n",
            "2957: RoBERTa Tokenizer Java Implementation - https://discuss.huggingface.co/t/roberta-tokenizer-java-implementation/16737\n",
            "2958: Unigram vocab_size doesn’t fit - https://discuss.huggingface.co/t/unigram-vocab-size-doesnt-fit/26856\n",
            "2959: Option to load only tokenizer and model configuration into “token-classification” pipeline - https://discuss.huggingface.co/t/option-to-load-only-tokenizer-and-model-configuration-into-token-classification-pipeline/26697\n",
            "2960: Encode_plus Pretokenized input seuqence must be Union - https://discuss.huggingface.co/t/encode-plus-pretokenized-input-seuqence-must-be-union/26449\n",
            "2961: Application of TFBertTokenizer - https://discuss.huggingface.co/t/application-of-tfberttokenizer/26437\n",
            "2962: TemplateProcessing for encoder-decoder - https://discuss.huggingface.co/t/templateprocessing-for-encoder-decoder/26135\n",
            "2963: Using `TFBertTokenizer` instead of `BertTokenizer` with `TFBertForQuestionAnswering` - https://discuss.huggingface.co/t/using-tfberttokenizer-instead-of-berttokenizer-with-tfbertforquestionanswering/26127\n",
            "2964: How to concatenate an answer to multiple choices after padded tokenization - https://discuss.huggingface.co/t/how-to-concatenate-an-answer-to-multiple-choices-after-padded-tokenization/26110\n",
            "2965: Maximum recursion depth exceeded when using DataCollator - https://discuss.huggingface.co/t/maximum-recursion-depth-exceeded-when-using-datacollator/25937\n",
            "2966: Adding a special language token to MBART - https://discuss.huggingface.co/t/adding-a-special-language-token-to-mbart/25967\n",
            "2967: Custom PostProcessor? - https://discuss.huggingface.co/t/custom-postprocessor/25847\n",
            "2968: Tokenizer.pad_token=what? - https://discuss.huggingface.co/t/tokenizer-pad-token-what/24367\n",
            "2969: Using HuggingFace Tokenizers Without Special Characters - https://discuss.huggingface.co/t/using-huggingface-tokenizers-without-special-characters/21962\n",
            "2970: How to get sp_model variable from T5Tokenizer? - https://discuss.huggingface.co/t/how-to-get-sp-model-variable-from-t5tokenizer/25171\n",
            "2971: Wav2vec2CTCTokenizer and vocab.json - https://discuss.huggingface.co/t/wav2vec2ctctokenizer-and-vocab-json/25146\n",
            "2972: Period ID in RobertaTokenizer with is_split_into_words - https://discuss.huggingface.co/t/period-id-in-robertatokenizer-with-is-split-into-words/24142\n",
            "2973: WordLevel error: Missing [UNK] token from the vocabulary - https://discuss.huggingface.co/t/wordlevel-error-missing-unk-token-from-the-vocabulary/5107\n",
            "2974: Tokenizer post_processor help - https://discuss.huggingface.co/t/tokenizer-post-processor-help/21926\n",
            "2975: Preprocessing raw text - https://discuss.huggingface.co/t/preprocessing-raw-text/24559\n",
            "2976: Save tokenizer with argument - https://discuss.huggingface.co/t/save-tokenizer-with-argument/17389\n",
            "2977: Trained tokenizer API as PretrainedTokenizer - https://discuss.huggingface.co/t/trained-tokenizer-api-as-pretrainedtokenizer/23639\n",
            "2978: Remove only certain special token id during tokenizer decode - https://discuss.huggingface.co/t/remove-only-certain-special-token-id-during-tokenizer-decode/20436\n",
            "2979: Convert_tokens_to_ids produces <unk> - https://discuss.huggingface.co/t/convert-tokens-to-ids-produces-unk/24771\n",
            "2980: Text preprocessing for fitting Tokenizer model - https://discuss.huggingface.co/t/text-preprocessing-for-fitting-tokenizer-model/24869\n",
            "2981: Special tokens warning - https://discuss.huggingface.co/t/special-tokens-warning/24939\n",
            "2982: Simple Transformers Multilabelclassification - https://discuss.huggingface.co/t/simple-transformers-multilabelclassification/24568\n",
            "2983: Cannot initialize deberta-v3-base tokenizer - https://discuss.huggingface.co/t/cannot-initialize-deberta-v3-base-tokenizer/15322\n",
            "2984: Getting Wholeword corresponding to a subword in a text? - https://discuss.huggingface.co/t/getting-wholeword-corresponding-to-a-subword-in-a-text/24152\n",
            "2985: Issue with pushing tokenizer to hub - https://discuss.huggingface.co/t/issue-with-pushing-tokenizer-to-hub/24120\n",
            "2986: How do we customize the number of entites for NER pretrained model? - https://discuss.huggingface.co/t/how-do-we-customize-the-number-of-entites-for-ner-pretrained-model/24085\n",
            "2987: Configure RobertaTokenizer - https://discuss.huggingface.co/t/configure-robertatokenizer/23969\n",
            "2988: How to properly clean vocabulary from BBPE tokenizer - https://discuss.huggingface.co/t/how-to-properly-clean-vocabulary-from-bbpe-tokenizer/22827\n",
            "2989: Map tokenization and posterior to smaller substrings - https://discuss.huggingface.co/t/map-tokenization-and-posterior-to-smaller-substrings/23792\n",
            "2990: T5 model tokenizer - https://discuss.huggingface.co/t/t5-model-tokenizer/23640\n",
            "2991: Fast tokenizer for marianMTModel - https://discuss.huggingface.co/t/fast-tokenizer-for-marianmtmodel/23638\n",
            "2992: Word tokenizers for text generators - https://discuss.huggingface.co/t/word-tokenizers-for-text-generators/23464\n",
            "2993: SentencePieceUnigramTokenizer - https://discuss.huggingface.co/t/sentencepieceunigramtokenizer/23509\n",
            "2994: Tokenizer is not being loaded on Huggingface Inference - https://discuss.huggingface.co/t/tokenizer-is-not-being-loaded-on-huggingface-inference/23505\n",
            "2995: Why is BertNormalizer not exposed on the tokenizers library? - https://discuss.huggingface.co/t/why-is-bertnormalizer-not-exposed-on-the-tokenizers-library/23340\n",
            "2996: Sentence splitting - https://discuss.huggingface.co/t/sentence-splitting/5393\n",
            "2997: Average time to train a SentencePieceBPETokenizer - https://discuss.huggingface.co/t/average-time-to-train-a-sentencepiecebpetokenizer/23081\n",
            "2998: 1 line code for NER data set preparation using tokenizer library! - https://discuss.huggingface.co/t/1-line-code-for-ner-data-set-preparation-using-tokenizer-library/22816\n",
            "2999: Microsoft/codebert-base produces two sep tokens - https://discuss.huggingface.co/t/microsoft-codebert-base-produces-two-sep-tokens/21366\n",
            "3000: Padding with sliding window - https://discuss.huggingface.co/t/padding-with-sliding-window/18921\n",
            "3001: Find which tokens are unknown in new data - https://discuss.huggingface.co/t/find-which-tokens-are-unknown-in-new-data/22466\n",
            "3002: How to train target tokenizer - https://discuss.huggingface.co/t/how-to-train-target-tokenizer/22291\n",
            "3003: How to know if a subtoken is a word or part of a word? - https://discuss.huggingface.co/t/how-to-know-if-a-subtoken-is-a-word-or-part-of-a-word/923\n",
            "3004: BART Tokenizer tokenises same word differently? - https://discuss.huggingface.co/t/bart-tokenizer-tokenises-same-word-differently/21835\n",
            "3005: Fine-tuned BERT tokenizer taking too long to load - https://discuss.huggingface.co/t/fine-tuned-bert-tokenizer-taking-too-long-to-load/9747\n",
            "3006: Add BOS and EOS when encoding a sentence - https://discuss.huggingface.co/t/add-bos-and-eos-when-encoding-a-sentence/21833\n",
            "3007: Customization of Wav2Vec2CTCTokenizer with rules - https://discuss.huggingface.co/t/customization-of-wav2vec2ctctokenizer-with-rules/21912\n",
            "3008: Customized tokenization files in run_clm script - https://discuss.huggingface.co/t/customized-tokenization-files-in-run-clm-script/21460\n",
            "3009: Using customized algorithm - https://discuss.huggingface.co/t/using-customized-algorithm/21753\n",
            "3010: Issue with Flaubert Tokenizer as word_ids() method is not available for NER Task - https://discuss.huggingface.co/t/issue-with-flaubert-tokenizer-as-word-ids-method-is-not-available-for-ner-task/20374\n",
            "3011: Word_ids not working with deberta_v2 - https://discuss.huggingface.co/t/word-ids-not-working-with-deberta-v2/21523\n",
            "3012: How to tokenize large contexts without running out of memory - https://discuss.huggingface.co/t/how-to-tokenize-large-contexts-without-running-out-of-memory/5882\n",
            "3013: Does Deberta tokenizer use wordpiece? - https://discuss.huggingface.co/t/does-deberta-tokenizer-use-wordpiece/21307\n",
            "3014: Get vocabulary tokens in order to exclude them from generate function - https://discuss.huggingface.co/t/get-vocabulary-tokens-in-order-to-exclude-them-from-generate-function/5192\n",
            "3015: Avoid creating certain tokens when training a tokenizer - https://discuss.huggingface.co/t/avoid-creating-certain-tokens-when-training-a-tokenizer/20864\n",
            "3016: Error finetuning XLM-RoBERTa-Large when training - https://discuss.huggingface.co/t/error-finetuning-xlm-roberta-large-when-training/20412\n",
            "3017: HuggingFace BPE Trainer Error - Training Tokenizer - https://discuss.huggingface.co/t/huggingface-bpe-trainer-error-training-tokenizer/10629\n",
            "3018: Word_to_tokens() and word_ids() —- microsoft/deberta-v2/v3 - https://discuss.huggingface.co/t/word-to-tokens-and-word-ids-microsoft-deberta-v2-v3/19984\n",
            "3019: No PreTrainedTokenizerFast for Deberta-V3, no doc_stride - https://discuss.huggingface.co/t/no-pretrainedtokenizerfast-for-deberta-v3-no-doc-stride/20345\n",
            "3020: Tokenizer from own vocab - https://discuss.huggingface.co/t/tokenizer-from-own-vocab/20245\n",
            "3021: No labels column for tokenized data - https://discuss.huggingface.co/t/no-labels-column-for-tokenized-data/19540\n",
            "3022: Programmatic way to Tokenization on Custom Text Columns - https://discuss.huggingface.co/t/programmatic-way-to-tokenization-on-custom-text-columns/19679\n",
            "3023: Bug in Offset generation for Rupee symbol - https://discuss.huggingface.co/t/bug-in-offset-generation-for-rupee-symbol/19655\n",
            "3024: How to handle parenthesis, quotation marks, \\n etc when creating tokenizer from scratch - https://discuss.huggingface.co/t/how-to-handle-parenthesis-quotation-marks-n-etc-when-creating-tokenizer-from-scratch/19628\n",
            "3025: EM training on unigram tokenizer taking way longer than predicted - https://discuss.huggingface.co/t/em-training-on-unigram-tokenizer-taking-way-longer-than-predicted/19502\n",
            "3026: Training unigram on long sequences - https://discuss.huggingface.co/t/training-unigram-on-long-sequences/19188\n",
            "3027: Issue with post-processing - https://discuss.huggingface.co/t/issue-with-post-processing/2703\n",
            "3028: FutureWarning about BertTokenizer.from_pretrained() at latest version - https://discuss.huggingface.co/t/futurewarning-about-berttokenizer-from-pretrained-at-latest-version/18750\n",
            "3029: Enhaced word_ids() API for Chinese or CJK languages? - https://discuss.huggingface.co/t/enhaced-word-ids-api-for-chinese-or-cjk-languages/18662\n",
            "3030: Importing tokenizers version >0.10.3 fails due to openssl - https://discuss.huggingface.co/t/importing-tokenizers-version-0-10-3-fails-due-to-openssl/17820\n",
            "3031: Lower case with input ids - https://discuss.huggingface.co/t/lower-case-with-input-ids/18490\n",
            "3032: Dialogue classification - https://discuss.huggingface.co/t/dialogue-classification/18466\n",
            "3033: Multilang bert vs translating to english - https://discuss.huggingface.co/t/multilang-bert-vs-translating-to-english/18454\n",
            "3034: pyo3_runtime.PanicException: likelihood is NAN. Input sentence may be too long - https://discuss.huggingface.co/t/pyo3-runtime-panicexception-likelihood-is-nan-input-sentence-may-be-too-long/18398\n",
            "3035: Pytorch_model.bin not working because of lfs - https://discuss.huggingface.co/t/pytorch-model-bin-not-working-because-of-lfs/18290\n",
            "3036: Tokenizer ignores repeated whitespaces - https://discuss.huggingface.co/t/tokenizer-ignores-repeated-whitespaces/17864\n",
            "3037: How truncation works when applying BERT tokenizer on the batch of sentence pairs in HuggingFace? - https://discuss.huggingface.co/t/how-truncation-works-when-applying-bert-tokenizer-on-the-batch-of-sentence-pairs-in-huggingface/17977\n",
            "3038: How to save a tokenizer only consisting of added tokens - https://discuss.huggingface.co/t/how-to-save-a-tokenizer-only-consisting-of-added-tokens/17825\n",
            "3039: Passing list of inputs to tokenize - https://discuss.huggingface.co/t/passing-list-of-inputs-to-tokenize/17499\n",
            "3040: Issues with Data Collator and Tokenizing with NER Datasets - https://discuss.huggingface.co/t/issues-with-data-collator-and-tokenizing-with-ner-datasets/17489\n",
            "3041: How to perform tokenization on an ONNX model in JS? - https://discuss.huggingface.co/t/how-to-perform-tokenization-on-an-onnx-model-in-js/17581\n",
            "3042: Using the Tokenizers library in a Unity project - https://discuss.huggingface.co/t/using-the-tokenizers-library-in-a-unity-project/17533\n",
            "3043: Further pre-training the tokenizer? - https://discuss.huggingface.co/t/further-pre-training-the-tokenizer/17360\n",
            "3044: Error when doing tokenization - https://discuss.huggingface.co/t/error-when-doing-tokenization/17344\n",
            "3045: How to decode with spaces? - https://discuss.huggingface.co/t/how-to-decode-with-spaces/17289\n",
            "3046: Show Submodels of PegasusTokenizer - https://discuss.huggingface.co/t/show-submodels-of-pegasustokenizer/17286\n",
            "3047: Load SentencePieceBPETokenizer in TF - https://discuss.huggingface.co/t/load-sentencepiecebpetokenizer-in-tf/17257\n",
            "3048: Best way to mask a multi-token word when using `.*ForMaskedLM` models - https://discuss.huggingface.co/t/best-way-to-mask-a-multi-token-word-when-using-formaskedlm-models/6428\n",
            "3049: Does a tokenizer keep the mapping between my labels to their encoding? - https://discuss.huggingface.co/t/does-a-tokenizer-keep-the-mapping-between-my-labels-to-their-encoding/16296\n",
            "3050: What is Wav2Vec2FeatureExtractor doing? - https://discuss.huggingface.co/t/what-is-wav2vec2featureextractor-doing/16423\n",
            "3051: What does this warning mean? -overflowing tokens are not returned for the setting you have chosen - https://discuss.huggingface.co/t/what-does-this-warning-mean-overflowing-tokens-are-not-returned-for-the-setting-you-have-chosen/11594\n",
            "3052: How can I make sure Tokenizer pads to a fixed length? - https://discuss.huggingface.co/t/how-can-i-make-sure-tokenizer-pads-to-a-fixed-length/16116\n",
            "3053: Issue with Decoding in HuggingFace - https://discuss.huggingface.co/t/issue-with-decoding-in-huggingface/15699\n",
            "3054: ValueError: Unable to create tensor for 1 dataset but not the other of same type - https://discuss.huggingface.co/t/valueerror-unable-to-create-tensor-for-1-dataset-but-not-the-other-of-same-type/15995\n",
            "3055: Disabling addition of CLS from BERT tokenizer - https://discuss.huggingface.co/t/disabling-addition-of-cls-from-bert-tokenizer/15538\n",
            "3056: Tokenized sequence lengths - https://discuss.huggingface.co/t/tokenized-sequence-lengths/15117\n",
            "3057: Finetuning GPT-J6B for custom dataset - https://discuss.huggingface.co/t/finetuning-gpt-j6b-for-custom-dataset/9924\n",
            "3058: Training tokenizer takes too much RAM - https://discuss.huggingface.co/t/training-tokenizer-takes-too-much-ram/14256\n",
            "3059: How to “further pretrain” a tokenizer (do I need to do so?) - https://discuss.huggingface.co/t/how-to-further-pretrain-a-tokenizer-do-i-need-to-do-so/14719\n",
            "3060: Run_seq2seq_qa.py: Column 3 named labels expected length 1007 but got length 1000 - https://discuss.huggingface.co/t/run-seq2seq-qa-py-column-3-named-labels-expected-length-1007-but-got-length-1000/13237\n",
            "3061: Issues with offset_mapping values - https://discuss.huggingface.co/t/issues-with-offset-mapping-values/4237\n",
            "3062: How would you train a sentencepiece BPE tokenizer on this language with 400 “characters”? - https://discuss.huggingface.co/t/how-would-you-train-a-sentencepiece-bpe-tokenizer-on-this-language-with-400-characters/14669\n",
            "3063: All my sequences get tokenized the same - https://discuss.huggingface.co/t/all-my-sequences-get-tokenized-the-same/14657\n",
            "3064: Combine multiple sentences together during tokenization - https://discuss.huggingface.co/t/combine-multiple-sentences-together-during-tokenization/3430\n",
            "3065: NER tag , aggregation stratergy - https://discuss.huggingface.co/t/ner-tag-aggregation-stratergy/14199\n",
            "3066: How to ensure that tokenizers never truncate partial words? - https://discuss.huggingface.co/t/how-to-ensure-that-tokenizers-never-truncate-partial-words/14024\n",
            "3067: How to ensure the `overflow` with `stride` always starts with a full word? - https://discuss.huggingface.co/t/how-to-ensure-the-overflow-with-stride-always-starts-with-a-full-word/14030\n",
            "3068: Adding new tokens to a BERT tokenizer - Getting ValueError - https://discuss.huggingface.co/t/adding-new-tokens-to-a-bert-tokenizer-getting-valueerror/9253\n",
            "3069: Adding token to t5-base vocab does not respect space - https://discuss.huggingface.co/t/adding-token-to-t5-base-vocab-does-not-respect-space/13662\n",
            "3070: How can I change the token id of a special token? - https://discuss.huggingface.co/t/how-can-i-change-the-token-id-of-a-special-token/13445\n",
            "3071: Import distilbert-base-uncased tokenizer to an android app along with the tflite model - https://discuss.huggingface.co/t/import-distilbert-base-uncased-tokenizer-to-an-android-app-along-with-the-tflite-model/3234\n",
            "3072: What are the equivalent manner for using texts_to_sequences? - https://discuss.huggingface.co/t/what-are-the-equivalent-manner-for-using-texts-to-sequences/13220\n",
            "3073: ERROR?why encoding [MASK] before ‘.’ would gain a idx 13? - https://discuss.huggingface.co/t/error-why-encoding-mask-before-would-gain-a-idx-13/2897\n",
            "3074: LongFormer tokenizer has the same token_type_ids for sequence pairs - https://discuss.huggingface.co/t/longformer-tokenizer-has-the-same-token-type-ids-for-sequence-pairs/12992\n",
            "3075: Batch encode plus in Rust Tokenizers - https://discuss.huggingface.co/t/batch-encode-plus-in-rust-tokenizers/12722\n",
            "3076: Best solution for train tokenizer and MLM from scratch - https://discuss.huggingface.co/t/best-solution-for-train-tokenizer-and-mlm-from-scratch/12579\n",
            "3077: Implementing custom tokenizer components (normalizers, processors) - https://discuss.huggingface.co/t/implementing-custom-tokenizer-components-normalizers-processors/12371\n",
            "3078: Does T5Tokenizer support the Greek language? - https://discuss.huggingface.co/t/does-t5tokenizer-support-the-greek-language/12224\n",
            "3079: How padding in huggingface tokenizer works? - https://discuss.huggingface.co/t/how-padding-in-huggingface-tokenizer-works/12161\n",
            "3080: Why we need to add special tokens to tasks other than classification? - https://discuss.huggingface.co/t/why-we-need-to-add-special-tokens-to-tasks-other-than-classification/11975\n",
            "3081: How to configure TokenizerFast for AutoTokenizer - https://discuss.huggingface.co/t/how-to-configure-tokenizerfast-for-autotokenizer/11353\n",
            "3082: How to employ different vocabs for encoder and decoder respectively? - https://discuss.huggingface.co/t/how-to-employ-different-vocabs-for-encoder-and-decoder-respectively/11497\n",
            "3083: How to use tokenizer.tokenize in Chinese data properly? - https://discuss.huggingface.co/t/how-to-use-tokenizer-tokenize-in-chinese-data-properly/11491\n",
            "3084: Mask only specific words - https://discuss.huggingface.co/t/mask-only-specific-words/173\n",
            "3085: Load custom pretrained tokenizer - https://discuss.huggingface.co/t/load-custom-pretrained-tokenizer/11148\n",
            "3086: Using Custom Vocab.txt - https://discuss.huggingface.co/t/using-custom-vocab-txt/10847\n",
            "3087: Tokenizer.encode not returning encodings - https://discuss.huggingface.co/t/tokenizer-encode-not-returning-encodings/10616\n",
            "3088: There is no 0.11.0 tokenizers in pip - https://discuss.huggingface.co/t/there-is-no-0-11-0-tokenizers-in-pip/10381\n",
            "3089: Performance difference between ByteLevelBPE and Wordpiece tokenizers - https://discuss.huggingface.co/t/performance-difference-between-bytelevelbpe-and-wordpiece-tokenizers/10203\n",
            "3090: Should have a `model_type` key in its config.json - https://discuss.huggingface.co/t/should-have-a-model-type-key-in-its-config-json/10144\n",
            "3091: Using a fixed vocab.txt with AutoTokenizer? - https://discuss.huggingface.co/t/using-a-fixed-vocab-txt-with-autotokenizer/9919\n",
            "3092: Train wordpiece from scratch - https://discuss.huggingface.co/t/train-wordpiece-from-scratch/9843\n",
            "3093: I set up a different batch_size, but the time of data processing has not changed - https://discuss.huggingface.co/t/i-set-up-a-different-batch-size-but-the-time-of-data-processing-has-not-changed/9668\n",
            "3094: Cannot create an identical PretrainedTokenizerFast object from a Tokenizer created by tokenizers library - https://discuss.huggingface.co/t/cannot-create-an-identical-pretrainedtokenizerfast-object-from-a-tokenizer-created-by-tokenizers-library/9317\n",
            "3095: Index of wordpieces (subwords) after tokenization by transformers - https://discuss.huggingface.co/t/index-of-wordpieces-subwords-after-tokenization-by-transformers/9552\n",
            "3096: A problem about FutureWarning？ - https://discuss.huggingface.co/t/a-problem-about-futurewarning/9323\n",
            "3097: Extracting embedding values of NLP pertained models from tokenized strings - https://discuss.huggingface.co/t/extracting-embedding-values-of-nlp-pertained-models-from-tokenized-strings/9287\n",
            "3098: Tokenization in a NER context - https://discuss.huggingface.co/t/tokenization-in-a-ner-context/5635\n",
            "3099: Unable to convert output to interpretable format - https://discuss.huggingface.co/t/unable-to-convert-output-to-interpretable-format/8882\n",
            "3100: BpeTrainer implementation in Python - https://discuss.huggingface.co/t/bpetrainer-implementation-in-python/8625\n",
            "3101: MBart50Tokenizer vs XLMRobertaTokenizer - https://discuss.huggingface.co/t/mbart50tokenizer-vs-xlmrobertatokenizer/8498\n",
            "3102: Why multilingual BERT tokenizer doesn’t remove accent markers? - https://discuss.huggingface.co/t/why-multilingual-bert-tokenizer-doesnt-remove-accent-markers/8468\n",
            "3103: TypeError when loading tokenizer with from_pretrained method for bart-large-mnli model - https://discuss.huggingface.co/t/typeerror-when-loading-tokenizer-with-from-pretrained-method-for-bart-large-mnli-model/3378\n",
            "3104: Is it okay to split ids sequence when it is encoded using Byte-level BPE - https://discuss.huggingface.co/t/is-it-okay-to-split-ids-sequence-when-it-is-encoded-using-byte-level-bpe/8129\n",
            "3105: Using truncated fragments as input samples in training - https://discuss.huggingface.co/t/using-truncated-fragments-as-input-samples-in-training/6978\n",
            "3106: Using whitespace tokenizer for training models - https://discuss.huggingface.co/t/using-whitespace-tokenizer-for-training-models/6591\n",
            "3107: Save custom components - https://discuss.huggingface.co/t/save-custom-components/6458\n",
            "3108: How to see contents of a normalizer - https://discuss.huggingface.co/t/how-to-see-contents-of-a-normalizer/6045\n",
            "3109: Newbie: Main difference between tokenizers? - https://discuss.huggingface.co/t/newbie-main-difference-between-tokenizers/6035\n",
            "3110: Can’t load tokenizer for ‘sshleifer/student_blarge_12_3’ - https://discuss.huggingface.co/t/cant-load-tokenizer-for-sshleifer-student-blarge-12-3/6027\n",
            "3111: How to create a Huggingface tokenizer from a non-Huggingface tokenizer? - https://discuss.huggingface.co/t/how-to-create-a-huggingface-tokenizer-from-a-non-huggingface-tokenizer/5983\n",
            "3112: Add new tokens and learn the embeddings of the new tokens and keeping all the other parametes frozen - https://discuss.huggingface.co/t/add-new-tokens-and-learn-the-embeddings-of-the-new-tokens-and-keeping-all-the-other-parametes-frozen/5896\n",
            "3113: How do you use SentencePiece for BPE of sequences with no whitespace - https://discuss.huggingface.co/t/how-do-you-use-sentencepiece-for-bpe-of-sequences-with-no-whitespace/1895\n",
            "3114: BOS tokens for mBERT tokenizer - https://discuss.huggingface.co/t/bos-tokens-for-mbert-tokenizer/5467\n",
            "3115: BertTokenizerFast for stsb-xlm-r-multilingual model - https://discuss.huggingface.co/t/berttokenizerfast-for-stsb-xlm-r-multilingual-model/4742\n",
            "3116: Skip-gram tokens - https://discuss.huggingface.co/t/skip-gram-tokens/5294\n",
            "3117: Using a BertWordPieceTokenizer trained from scratch from transformers - https://discuss.huggingface.co/t/using-a-bertwordpiecetokenizer-trained-from-scratch-from-transformers/4391\n",
            "3118: Questions on model’s tokens - https://discuss.huggingface.co/t/questions-on-models-tokens/5023\n",
            "3119: Space token ’ ’ cannot be add when is_split_into_words = True - https://discuss.huggingface.co/t/space-token-cannot-be-add-when-is-split-into-words-true/4305\n",
            "3120: Are special_tokens the only tokens guaranteed to be atomic? - https://discuss.huggingface.co/t/are-special-tokens-the-only-tokens-guaranteed-to-be-atomic/4124\n",
            "3121: Does AutoTokenizer.from_pretrained add [cls] tokens? - https://discuss.huggingface.co/t/does-autotokenizer-from-pretrained-add-cls-tokens/4056\n",
            "3122: BertTokenizer’s encode_plus returns 2d tensor when printing ‘input_ids’/ ‘attention_mask’ - https://discuss.huggingface.co/t/berttokenizers-encode-plus-returns-2d-tensor-when-printing-input-ids-attention-mask/3530\n",
            "3123: Tunning tokenizer on my own dataset - https://discuss.huggingface.co/t/tunning-tokenizer-on-my-own-dataset/3367\n",
            "3124: Why Bert-chinese use do_lower_case=False? - https://discuss.huggingface.co/t/why-bert-chinese-use-do-lower-case-false/2952\n",
            "3125: Bug with tokernizer’s offset mapping for NER problems? - https://discuss.huggingface.co/t/bug-with-tokernizers-offset-mapping-for-ner-problems/2928\n",
            "3126: BERT WordPiece Tokenizer: some matras missing after tokenization for Hindi Language #572 - https://discuss.huggingface.co/t/bert-wordpiece-tokenizer-some-matras-missing-after-tokenization-for-hindi-language-572/2936\n",
            "3127: Error with <|endoftext|> in Tokenizer GPT2 - https://discuss.huggingface.co/t/error-with-endoftext-in-tokenizer-gpt2/2838\n",
            "3128: Build a RoBERTa tokenizer from scratch - https://discuss.huggingface.co/t/build-a-roberta-tokenizer-from-scratch/2758\n",
            "3129: Couldn’t instantiate the backend tokenizer - https://discuss.huggingface.co/t/couldnt-instantiate-the-backend-tokenizer/2662\n",
            "3130: Bypassing tokenizers - https://discuss.huggingface.co/t/bypassing-tokenizers/2162\n",
            "3131: Tokenizing Domain Specific Text - https://discuss.huggingface.co/t/tokenizing-domain-specific-text/1978\n",
            "3132: Issue with tokenizer.tokenize - https://discuss.huggingface.co/t/issue-with-tokenizer-tokenize/1891\n",
            "3133: Tokenizer taking extremely long time to train - https://discuss.huggingface.co/t/tokenizer-taking-extremely-long-time-to-train/1875\n",
            "3134: Where to find the “wiki-big.train.raw” data as mentioned in the snippet for tokenizers 0.9? - https://discuss.huggingface.co/t/where-to-find-the-wiki-big-train-raw-data-as-mentioned-in-the-snippet-for-tokenizers-0-9/1796\n",
            "3135: Change bpe-dropout value on the fly? - https://discuss.huggingface.co/t/change-bpe-dropout-value-on-the-fly/1721\n",
            "3136: Loading pretrained SentencePiece tokenizer from Fairseq - https://discuss.huggingface.co/t/loading-pretrained-sentencepiece-tokenizer-from-fairseq/1326\n",
            "3137: What does `tokenizers.normalizer.normalize` do? - https://discuss.huggingface.co/t/what-does-tokenizers-normalizer-normalize-do/1463\n",
            "3138: Automatic sentence segmentation and encoding - https://discuss.huggingface.co/t/automatic-sentence-segmentation-and-encoding/1479\n",
            "3139: How to truncate from the head in AutoTokenizer? - https://discuss.huggingface.co/t/how-to-truncate-from-the-head-in-autotokenizer/676\n",
            "3140: How much memory is needed for training ByteLevelBPETokenizer? - https://discuss.huggingface.co/t/how-much-memory-is-needed-for-training-bytelevelbpetokenizer/1165\n",
            "3141: How to make tokenizer convert subword token to an independent token? - https://discuss.huggingface.co/t/how-to-make-tokenizer-convert-subword-token-to-an-independent-token/1015\n",
            "3142: Using a pretrained tokenizer vs training a one from scratch - https://discuss.huggingface.co/t/using-a-pretrained-tokenizer-vs-training-a-one-from-scratch/783\n",
            "3143: Masking Probability - https://discuss.huggingface.co/t/masking-probability/746\n",
            "3144: Tokenizer not found - https://discuss.huggingface.co/t/tokenizer-not-found/757\n",
            "3145: Add new tokens for subwords - https://discuss.huggingface.co/t/add-new-tokens-for-subwords/489\n",
            "3146: Token alignment for word-level tasks - https://discuss.huggingface.co/t/token-alignment-for-word-level-tasks/577\n",
            "3147: ByteLevelBPETokenizer inconsistent behavior - https://discuss.huggingface.co/t/bytelevelbpetokenizer-inconsistent-behavior/442\n",
            "3148: Use a pretrained ByteLevelBPETokenizer on text - https://discuss.huggingface.co/t/use-a-pretrained-bytelevelbpetokenizer-on-text/348\n",
            "3149: Continuation token in pertained tokenizer bert-base-chinese - https://discuss.huggingface.co/t/continuation-token-in-pertained-tokenizer-bert-base-chinese/218\n",
            "3150: Tokenizers v0.8.0 is out! - https://discuss.huggingface.co/t/tokenizers-v0-8-0-is-out/51\n",
            "3151: Byte Level Tokenizer While Training - https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "3152: Tokenizer: what function removes spaces between ‘<’ and ‘>’? - https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "3153: Convert huggingface tokenizer into sentencepiece format - https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682\n",
            "3154: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error - https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "3155: Generate tokenizer.json for Marian(Opus) MT - https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157\n",
            "3156: Tokenizer method inference - https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "3157: How to skip tokens from translation? - https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171\n",
            "3158: Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 - https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124\n",
            "3159: Authorization header is correct, but the token seems invalid - https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "3160: AutoTokenizer.encode with multiThread and mutliProcess - https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "3161: Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).` - https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269\n",
            "3162: Help to choose decoder for devnagari ocr - https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "3163: Speed up tokenizer training - https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417\n",
            "3164: Cannot load tokenizer for llama2 - https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046\n",
            "3165: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer - https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "3166: Tokenization compared to sentencepiece - https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "3167: Tokenizer Error [AGAIN!] - https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "3168: Decoding sequence of tokens produces question marks instead of actual tokens - https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "3169: Chat_template is not set & throwing error - https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095\n",
            "3170: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer - https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "3171: What does “trim_offsets” do in tokenizer post-processor? - https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "3172: Call rust function in python - https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "3173: How to train a LlamaTokenizer? - https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835\n",
            "3174: Issue with XLM-RoBERTa tokenizer - https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "3175: Adding tokens, but tokenizer doesn’t use them - https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "3176: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model? - https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "3177: Why does tokenization take so long? - https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "3178: Encoding and then decodeing text is not equal - https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "3179: HugginChat (Android App) - https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "3180: Token Classification: How to tokenize and align labels with overflow and stride? - https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353\n",
            "3181: Error with new tokenizers (URGENT!) - https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847\n",
            "3182: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences? - https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "3183: NLLB tokenizer multiple target/source languages within a training batch - https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307\n",
            "3184: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing - https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "3185: Update encode function slowTokenizer vs FastTokenizer - https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "3186: How do I remove tokens from a BPE Tokenizer’s vocabulary? - https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "3187: Problem with AutoTokenizer - https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "3188: Tokenizer vs Model - https://discuss.huggingface.co/t/tokenizer-vs-model/93689\n",
            "3189: Exporting tokenizer to an onnx model - https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467\n",
            "3190: `additional_special_tokens` are not added - https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "3191: Tokenizer splits words with accents into separate subwords - https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "3192: Emojis poisoning tokenizer - https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "3193: Modifying normalizer for pretrained tokenizers don’t consistently work - https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "3194: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer - https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "3195: Use sentence-transformers/all-MiniLM-L6-v2 fully local - https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "3196: Get “using the `__call__` method is faster” warning with DataCollatorWithPadding - https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924\n",
            "3197: Create entirely new vocabulary for tokenizer - https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "3198: Paligemma model Forward Method Not Returning Loss in Trainer #31045 - https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "3199: BUGs on offset-mapping - https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "3200: How long to expect training to take, and guidance on subset size? - https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380\n",
            "3201: Doubts about the tokenization strategy and the explanation of models through SHAP - https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "3202: Version incompatibility between transformers and tokenizers - https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "3203: Can’t load tokenizer using from_pretrained, Interface API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "3204: Unusual input_id size for distilBERT tokenizer - https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "3205: Unable to load saved tokenizer - https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "3206: Error loading tokenizer from local checkpoint directory - https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428\n",
            "3207: Difference between tokenizer and convert_tokens_to_ids - https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361\n",
            "3208: Encode token without spaced between them - https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "3209: ONNX T5 - Decoding seq2seq tokens - https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321\n",
            "3210: Construct a Marian tokenizer. Based on huggingface tokenizers - https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "3211: Can’t load tokenizer using from_pretrained, Inference API - https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235\n",
            "3212: A question about the DataCollator for LM - https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "3213: Asking to pad but the tokenizer does not have a padding token - https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344\n",
            "3214: Which file stores token frequency in SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "3215: Documentation of SentencePieceBPETokenizer? - https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777\n",
            "3216: Converting TikToken to Huggingface Tokenizer - https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535\n",
            "3217: Tokenizer mapping the same token to multiple token_ids - https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328\n",
            "3218: Treat Hawaiian Glottal stop as consonant, not punctuation - https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "3219: Train tokenizer for seq2seq model - https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "3220: ViTImageProcessor output visualization - https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335\n",
            "3221: Escape symbol appearance - https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "3222: Loading BPE modeled Tokenizer results in empty tokenizer - https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "3223: Translate from one tokenizer to another - https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "3224: Custom training - tokenization via collate fn or __getitem__? - https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "3225: Running train_new_from_iterator to train a tokenizer is very slow - https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "3226: Printing tokens array - https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "3227: Preprocessing of dataset - https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "3228: Is it safe to assume tokenizer does not change after initialization? - https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "3229: WordPiece tokenizer doesn’t work for long sequences - https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391\n",
            "3230: OPT special tokens - https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "3231: Inputs.word_ids() length not matching word label length - https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976\n",
            "3232: How does `byte_fallback` work and affect vocab size in BPE? - https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634\n",
            "3233: Custom Tokenizing? - https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "3234: Reused tokenizer returns unk - https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358\n",
            "3235: Adding too many tokens breaks tokenizer - https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005\n",
            "3236: Fastest way to tokenize millions of examples? - https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741\n",
            "3237: Run Mistral model only on CPU - https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136\n",
            "3238: Tokenizer not recognising words in vocabulary - https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140\n",
            "3239: Tokenizer dataset is very slow - https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722\n",
            "3240: T5 tokenizer vs t51.1 tokenizer - https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "3241: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType - https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "3242: I/O error calling ToenizersLibrary.createTokenizer in container - https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "3243: T5v1.1 tokenizer legacy=False - https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "3244: How to deal SQL query in tabular dataset? - https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "3245: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct - https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "3246: Incorporating my tokenizer into huggingface - https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "3247: Tokenizer splits up pre-split tokens - https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078\n",
            "3248: Building a custom Java tokenizer - https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "3249: Adding New Tokens to MarianMT Model - https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547\n",
            "3250: Issue with KOSMOS-2 encoding and decoding - https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019\n",
            "3251: Adding new tokens while preserving tokenization of adjacent tokens - https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604\n",
            "3252: Is there a way to save a pre-compiled AutoTokenizer? - https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "3253: Issues with BPE tokenizer - https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "3254: FastTokenizer add 10 more tokens in Avg - https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "3255: Added Tokens Not Decoding with Spaces - https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883\n",
            "3256: SOLVED: Module ‘numpy’ has no attribute ‘object’. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook - https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669\n",
            "3257: Special_tokens_mask - https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "3258: Unmasking adds an extra whitespace for BPE tokenizer - https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "3259: Caching tokenization - https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "3260: Right choice of padding side for Mistral - https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401\n",
            "3261: Regular tokens vs special tokens - https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187\n",
            "3262: Issue in loading the saved tokenizer - https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "3263: SentencePiece user_defined_symbols and fast tokenizers - https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208\n",
            "3264: Many ambiguous unicode characters for trained tokenizer - https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "3265: Skew between mistral prompt in docs vs. chat template - https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674\n",
            "3266: Tokenizer shrinking recipes - https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564\n",
            "3267: How to decode with custom pad tokens - https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504\n",
            "3268: Training sentencePiece from scratch? - https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477\n",
            "3269: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem - https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "3270: Get intermediate tokens and merges used in tokenization - https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "3271: BertTokenizer.decode not understanding new vocabulary - https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "3272: Tokenizer tend to choose added tokens first rather than token in vocab - https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "3273: Special token printed out as output - https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948\n",
            "3274: [NER][Japanese] labeled segment shorter than token - https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "3275: T5Tokenizer add a whitespace token after added special tokens - https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "3276: Unable to register my own tokenizer - https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "3277: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab! - https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "3278: Use Unicode blocks in regex (in Replace normalizer) - https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904\n",
            "3279: How to handle translations one source language to many target sentences for the same language - https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "3280: NER Label tokenization with overflowing tokens - https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561\n",
            "3281: How to use a trained tokenizer for semantic search? - https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "3282: Unk_token not set after training a BPETokenizer tokenizer - https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733\n",
            "3283: AutoTokenizer is very slow when loading llama tokenizer - https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547\n",
            "3284: Offset mappings differ for tokenizers - https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424\n",
            "3285: The process for tokenizing concatenated dataset is slow st the end of tokenizing - https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "3286: Batch tokenize (split into tokens, without processing) - https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238\n",
            "3287: Does AutoTokenizer uploads data to HuggingFace - https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "3288: RobertaTokenizer decode and tokenize do not have the same output - https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "3289: Training tokenizers with padding in between tokens - https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "3290: Leaving unknown words untokenized like in OpenMNT - https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "3291: Keeping special chars in translations - https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270\n",
            "3292: Should cls_token be [CLS] or <cls>? - https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "3293: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False - https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "3294: I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error - https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742\n",
            "3295: OSError: Can’t load tokenizer for ‘facebook/xmod-base’ - https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471\n",
            "3296: `GPT2Tokenizer` Tokenizer handling `\\n\\n` differently in different settings - https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289\n",
            "3297: DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‘padding=True’ ‘truncation=True’ to have batched tensors with the same length - https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625\n",
            "3298: Decode token IDs into a list (not a single string) - https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991\n",
            "3299: Error training MLM with Roberta Tokenizer - https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878\n",
            "3300: Are the slow and fast tokenizer results the same output for the same input? - https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "3301: “Add_tokens” breaks words when encoding - https://discuss.huggingface.co/t/add-tokens-breaks-words-when-encoding/21154\n",
            "3302: 2 tokens for one character in T5 - https://discuss.huggingface.co/t/2-tokens-for-one-character-in-t5/14960\n",
            "3303: OSError: Model name ‘gpt2’ was not found in tokenizers model name list (gpt2,…) - https://discuss.huggingface.co/t/oserror-model-name-gpt2-was-not-found-in-tokenizers-model-name-list-gpt2/2164\n",
            "3304: DNA long sequence tokenization - https://discuss.huggingface.co/t/dna-long-sequence-tokenization/16154\n",
            "3305: SentencePiece tokenizer encodes to unknown token - https://discuss.huggingface.co/t/sentencepiece-tokenizer-encodes-to-unknown-token/49113\n",
            "3306: Tokenizer behaviour with pipeline - https://discuss.huggingface.co/t/tokenizer-behaviour-with-pipeline/48969\n",
            "3307: Load tokenizer from file : Exception: data did not match any variant of untagged enum ModelWrapper - https://discuss.huggingface.co/t/load-tokenizer-from-file-exception-data-did-not-match-any-variant-of-untagged-enum-modelwrapper/25325\n",
            "3308: ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076 - https://discuss.huggingface.co/t/arrowinvalid-column-3-named-attention-mask-expected-length-1000-but-got-length-1076/6904\n",
            "3309: Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model - https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829\n",
            "3310: Initialize Vocabulary for Unigram Tokenizer - https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371\n",
            "3311: Make correct padding for text generation with GPT-NEO - https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800\n",
            "3312: How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers? - https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639\n",
            "3313: Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF - https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602\n",
            "3314: Writing custom tokenizer and wrapping it in tokenizer object - https://discuss.huggingface.co/t/writing-custom-tokenizer-and-wrapping-it-in-tokenizer-object/39679\n",
            "3315: Tokenizer for German lang - https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222\n",
            "3316: Chunk tokens into desired chunk length without simply getting rid of rest of tokens - https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399\n",
            "3317: Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers - https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872\n",
            "3318: LlamaTokenizerFast returns token_type_ids but the forward pass of the LlamaModel does not receive token_type_ids - https://discuss.huggingface.co/t/llamatokenizerfast-returns-token-type-ids-but-the-forward-pass-of-the-llamamodel-does-not-receive-token-type-ids/42431\n",
            "3319: GPT2Tokenizer not working in Kaggle Notebook - https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508\n",
            "3320: Exploring the Majestic Temples in Karnataka - https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952\n",
            "3321: Tokenizer producing token index greater than size of the dictionary - https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989\n",
            "3322: How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer - https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843\n",
            "3323: How to create a HF tokenizer’s vocab file from a BPE model’s merges.txt file? - https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737\n",
            "3324: Scala/JVM Bindings for Tokenizers - https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393\n",
            "3325: Tokenizers Wheel Takes Forever to Build - https://discuss.huggingface.co/t/tokenizers-wheel-takes-forever-to-build/17114\n",
            "3326: Where the introduction of tokenizers.implementations? - https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959\n",
            "3327: How to return custom `token_type_ids` or other values from a tokenizer? - https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525\n",
            "3328: Easy way to compare tokenizers - https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347\n",
            "3329: Unable to load image using llama-index - https://discuss.huggingface.co/t/unable-to-load-image-using-llama-index/38304\n",
            "3330: Help defining tokenizer - https://discuss.huggingface.co/t/help-defining-tokenizer/38091\n",
            "3331: Token Offsets in Rust vs. Python - https://discuss.huggingface.co/t/token-offsets-in-rust-vs-python/37949\n",
            "3332: “OSError: Model name ‘./XX’ was not found in tokenizers model name list” - cannot load custom tokenizer in Transformers - https://discuss.huggingface.co/t/oserror-model-name-xx-was-not-found-in-tokenizers-model-name-list-cannot-load-custom-tokenizer-in-transformers/2714\n",
            "3333: Converting JSON/dict to flatten string with indicator tokens - https://discuss.huggingface.co/t/converting-json-dict-to-flatten-string-with-indicator-tokens/37387\n",
            "3334: Train Retry Tokenizer - https://discuss.huggingface.co/t/train-retry-tokenizer/37031\n",
            "3335: Pretokenise on punctuation except hyphens - https://discuss.huggingface.co/t/pretokenise-on-punctuation-except-hyphens/36691\n",
            "3336: Tokenizer Trainer Crashing - https://discuss.huggingface.co/t/tokenizer-trainer-crashing/36645\n",
            "3337: Tokenizer extremely slow when deployed to a container - https://discuss.huggingface.co/t/tokenizer-extremely-slow-when-deployed-to-a-container/36569\n",
            "3338: Dealing with Decimal and Fractions - https://discuss.huggingface.co/t/dealing-with-decimal-and-fractions/23377\n",
            "3339: `add_tokens` with argument `special_tokens=True` vs `add_special_tokens` - https://discuss.huggingface.co/t/add-tokens-with-argument-special-tokens-true-vs-add-special-tokens/35648\n",
            "3340: Unable to upload custom Pytorch model in huggingface - https://discuss.huggingface.co/t/unable-to-upload-custom-pytorch-model-in-huggingface/35545\n",
            "3341: RuntimeError: Cannot re-initialize CUDA in forked subprocess - https://discuss.huggingface.co/t/runtimeerror-cannot-re-initialize-cuda-in-forked-subprocess/28392\n",
            "3342: Overflowing Tokens in MarkupLM - https://discuss.huggingface.co/t/overflowing-tokens-in-markuplm/35217\n",
            "3343: I get the predicted token as ` े` . What am I doing wrong? - https://discuss.huggingface.co/t/i-get-the-predicted-token-as-what-am-i-doing-wrong/29038\n",
            "3344: <unk> token in the output instead curly braces - https://discuss.huggingface.co/t/unk-token-in-the-output-instead-curly-braces/34653\n",
            "3345: How to add a new token without expanding the vocabulary - https://discuss.huggingface.co/t/how-to-add-a-new-token-without-expanding-the-vocabulary/34536\n",
            "3346: Does the ByteLevelBPETokenizer need to be wrapped in a normal Tokenizer? - https://discuss.huggingface.co/t/does-the-bytelevelbpetokenizer-need-to-be-wrapped-in-a-normal-tokenizer/34121\n",
            "3347: What is required to create a fast tokenizer? For example for a Marian model - https://discuss.huggingface.co/t/what-is-required-to-create-a-fast-tokenizer-for-example-for-a-marian-model/33941\n",
            "3348: GPT2Tokenizer.decode maps unicode sequences to the same string ‘�’ - https://discuss.huggingface.co/t/gpt2tokenizer-decode-maps-unicode-sequences-to-the-same-string/32453\n",
            "3349: Issue with Tokenizer - https://discuss.huggingface.co/t/issue-with-tokenizer/33796\n",
            "3350: Tokenizing my novel for GPT model - https://discuss.huggingface.co/t/tokenizing-my-novel-for-gpt-model/33532\n",
            "3351: How to add additional custom pre-tokenization processing? - https://discuss.huggingface.co/t/how-to-add-additional-custom-pre-tokenization-processing/1637\n",
            "3352: Customize FlauBERT tokenizer to split line breaks - https://discuss.huggingface.co/t/customize-flaubert-tokenizer-to-split-line-breaks/33011\n",
            "3353: How to change the size of model_max_length? - https://discuss.huggingface.co/t/how-to-change-the-size-of-model-max-length/32942\n",
            "3354: Can’t get to the source code of `tokenizer.convert_tokens_to_string` - https://discuss.huggingface.co/t/cant-get-to-the-source-code-of-tokenizer-convert-tokens-to-string/32714\n",
            "3355: Why I’m getting same result with or without using Wav2Vec2Processor? - https://discuss.huggingface.co/t/why-im-getting-same-result-with-or-without-using-wav2vec2processor/32482\n",
            "3356: How does `tokenizer().input_ids` work and how different it is from tokenizer.encode() before `model.generate()` and decoding step? - https://discuss.huggingface.co/t/how-does-tokenizer-input-ids-work-and-how-different-it-is-from-tokenizer-encode-before-model-generate-and-decoding-step/30476\n",
            "3357: What file type should my training data be? - https://discuss.huggingface.co/t/what-file-type-should-my-training-data-be/32075\n",
            "3358: Best way to get the closest token indices of input of char_to_token is a whitespace - https://discuss.huggingface.co/t/best-way-to-get-the-closest-token-indices-of-input-of-char-to-token-is-a-whitespace/32004\n",
            "3359: Token indices sequence length is longer than the specified maximum sequence length - https://discuss.huggingface.co/t/token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length/25133\n",
            "3360: Create a simple tokenizer - https://discuss.huggingface.co/t/create-a-simple-tokenizer/31677\n",
            "3361: Sliding window for Long Documents - https://discuss.huggingface.co/t/sliding-window-for-long-documents/19367\n",
            "3362: Creating tokenizer from counts file? - https://discuss.huggingface.co/t/creating-tokenizer-from-counts-file/31399\n",
            "3363: Tokenizer.train() running out of memory - https://discuss.huggingface.co/t/tokenizer-train-running-out-of-memory/31355\n",
            "3364: Tokenizing Float Tensor? - https://discuss.huggingface.co/t/tokenizing-float-tensor/30604\n",
            "3365: Padding and truncation for custom tokenizer - https://discuss.huggingface.co/t/padding-and-truncation-for-custom-tokenizer/30180\n",
            "3366: Incorporate SARI score into run_summarization.py example script - https://discuss.huggingface.co/t/incorporate-sari-score-into-run-summarization-py-example-script/29511\n",
            "3367: Is that possible to embed the tokenizer into the model to have it running on GCP using TensorFlow Serving? - https://discuss.huggingface.co/t/is-that-possible-to-embed-the-tokenizer-into-the-model-to-have-it-running-on-gcp-using-tensorflow-serving/10532\n",
            "3368: Huggingface inference API issue - https://discuss.huggingface.co/t/huggingface-inference-api-issue/29307\n",
            "3369: Using Tokenizer for integer data - https://discuss.huggingface.co/t/using-tokenizer-for-integer-data/28835\n",
            "3370: GPT2 long text approach - https://discuss.huggingface.co/t/gpt2-long-text-approach/28165\n",
            "3371: Huggingface t5 models seem to not download a tokenizer file - https://discuss.huggingface.co/t/huggingface-t5-models-seem-to-not-download-a-tokenizer-file/27935\n",
            "3372: How to save a fast tokenizer using the transformer library and then load it using Tokenizers? - https://discuss.huggingface.co/t/how-to-save-a-fast-tokenizer-using-the-transformer-library-and-then-load-it-using-tokenizers/9567\n",
            "3373: Using a BertTokenizer when training a RobertaForMaskedLM - https://discuss.huggingface.co/t/using-a-berttokenizer-when-training-a-robertaformaskedlm/27456\n",
            "3374: Need clarity on “padding” parameter in Bert Tokenizer - https://discuss.huggingface.co/t/need-clarity-on-padding-parameter-in-bert-tokenizer/27444\n",
            "3375: How to convert HuggingFace tokenizers into ONNX format? - https://discuss.huggingface.co/t/how-to-convert-huggingface-tokenizers-into-onnx-format/27272\n",
            "3376: Can’t save ConvBert tokenizer - https://discuss.huggingface.co/t/cant-save-convbert-tokenizer/16006\n",
            "3377: RoBERTa Tokenizer Java Implementation - https://discuss.huggingface.co/t/roberta-tokenizer-java-implementation/16737\n",
            "3378: Unigram vocab_size doesn’t fit - https://discuss.huggingface.co/t/unigram-vocab-size-doesnt-fit/26856\n",
            "3379: Option to load only tokenizer and model configuration into “token-classification” pipeline - https://discuss.huggingface.co/t/option-to-load-only-tokenizer-and-model-configuration-into-token-classification-pipeline/26697\n",
            "3380: Encode_plus Pretokenized input seuqence must be Union - https://discuss.huggingface.co/t/encode-plus-pretokenized-input-seuqence-must-be-union/26449\n",
            "3381: Application of TFBertTokenizer - https://discuss.huggingface.co/t/application-of-tfberttokenizer/26437\n",
            "3382: TemplateProcessing for encoder-decoder - https://discuss.huggingface.co/t/templateprocessing-for-encoder-decoder/26135\n",
            "3383: Using `TFBertTokenizer` instead of `BertTokenizer` with `TFBertForQuestionAnswering` - https://discuss.huggingface.co/t/using-tfberttokenizer-instead-of-berttokenizer-with-tfbertforquestionanswering/26127\n",
            "3384: How to concatenate an answer to multiple choices after padded tokenization - https://discuss.huggingface.co/t/how-to-concatenate-an-answer-to-multiple-choices-after-padded-tokenization/26110\n",
            "3385: Maximum recursion depth exceeded when using DataCollator - https://discuss.huggingface.co/t/maximum-recursion-depth-exceeded-when-using-datacollator/25937\n",
            "3386: Adding a special language token to MBART - https://discuss.huggingface.co/t/adding-a-special-language-token-to-mbart/25967\n",
            "3387: Custom PostProcessor? - https://discuss.huggingface.co/t/custom-postprocessor/25847\n",
            "3388: Tokenizer.pad_token=what? - https://discuss.huggingface.co/t/tokenizer-pad-token-what/24367\n",
            "3389: Using HuggingFace Tokenizers Without Special Characters - https://discuss.huggingface.co/t/using-huggingface-tokenizers-without-special-characters/21962\n",
            "3390: How to get sp_model variable from T5Tokenizer? - https://discuss.huggingface.co/t/how-to-get-sp-model-variable-from-t5tokenizer/25171\n",
            "3391: Wav2vec2CTCTokenizer and vocab.json - https://discuss.huggingface.co/t/wav2vec2ctctokenizer-and-vocab-json/25146\n",
            "3392: Period ID in RobertaTokenizer with is_split_into_words - https://discuss.huggingface.co/t/period-id-in-robertatokenizer-with-is-split-into-words/24142\n",
            "3393: WordLevel error: Missing [UNK] token from the vocabulary - https://discuss.huggingface.co/t/wordlevel-error-missing-unk-token-from-the-vocabulary/5107\n",
            "3394: Tokenizer post_processor help - https://discuss.huggingface.co/t/tokenizer-post-processor-help/21926\n",
            "3395: Preprocessing raw text - https://discuss.huggingface.co/t/preprocessing-raw-text/24559\n",
            "3396: Save tokenizer with argument - https://discuss.huggingface.co/t/save-tokenizer-with-argument/17389\n",
            "3397: Trained tokenizer API as PretrainedTokenizer - https://discuss.huggingface.co/t/trained-tokenizer-api-as-pretrainedtokenizer/23639\n",
            "3398: Remove only certain special token id during tokenizer decode - https://discuss.huggingface.co/t/remove-only-certain-special-token-id-during-tokenizer-decode/20436\n",
            "3399: Convert_tokens_to_ids produces <unk> - https://discuss.huggingface.co/t/convert-tokens-to-ids-produces-unk/24771\n",
            "3400: Text preprocessing for fitting Tokenizer model - https://discuss.huggingface.co/t/text-preprocessing-for-fitting-tokenizer-model/24869\n",
            "3401: Special tokens warning - https://discuss.huggingface.co/t/special-tokens-warning/24939\n",
            "3402: Simple Transformers Multilabelclassification - https://discuss.huggingface.co/t/simple-transformers-multilabelclassification/24568\n",
            "3403: Cannot initialize deberta-v3-base tokenizer - https://discuss.huggingface.co/t/cannot-initialize-deberta-v3-base-tokenizer/15322\n",
            "3404: Getting Wholeword corresponding to a subword in a text? - https://discuss.huggingface.co/t/getting-wholeword-corresponding-to-a-subword-in-a-text/24152\n",
            "3405: Issue with pushing tokenizer to hub - https://discuss.huggingface.co/t/issue-with-pushing-tokenizer-to-hub/24120\n",
            "3406: How do we customize the number of entites for NER pretrained model? - https://discuss.huggingface.co/t/how-do-we-customize-the-number-of-entites-for-ner-pretrained-model/24085\n",
            "3407: Configure RobertaTokenizer - https://discuss.huggingface.co/t/configure-robertatokenizer/23969\n",
            "3408: How to properly clean vocabulary from BBPE tokenizer - https://discuss.huggingface.co/t/how-to-properly-clean-vocabulary-from-bbpe-tokenizer/22827\n",
            "3409: Map tokenization and posterior to smaller substrings - https://discuss.huggingface.co/t/map-tokenization-and-posterior-to-smaller-substrings/23792\n",
            "3410: T5 model tokenizer - https://discuss.huggingface.co/t/t5-model-tokenizer/23640\n",
            "3411: Fast tokenizer for marianMTModel - https://discuss.huggingface.co/t/fast-tokenizer-for-marianmtmodel/23638\n",
            "3412: Word tokenizers for text generators - https://discuss.huggingface.co/t/word-tokenizers-for-text-generators/23464\n",
            "3413: SentencePieceUnigramTokenizer - https://discuss.huggingface.co/t/sentencepieceunigramtokenizer/23509\n",
            "3414: Tokenizer is not being loaded on Huggingface Inference - https://discuss.huggingface.co/t/tokenizer-is-not-being-loaded-on-huggingface-inference/23505\n",
            "3415: Why is BertNormalizer not exposed on the tokenizers library? - https://discuss.huggingface.co/t/why-is-bertnormalizer-not-exposed-on-the-tokenizers-library/23340\n",
            "3416: Sentence splitting - https://discuss.huggingface.co/t/sentence-splitting/5393\n",
            "3417: Average time to train a SentencePieceBPETokenizer - https://discuss.huggingface.co/t/average-time-to-train-a-sentencepiecebpetokenizer/23081\n",
            "3418: 1 line code for NER data set preparation using tokenizer library! - https://discuss.huggingface.co/t/1-line-code-for-ner-data-set-preparation-using-tokenizer-library/22816\n",
            "3419: Microsoft/codebert-base produces two sep tokens - https://discuss.huggingface.co/t/microsoft-codebert-base-produces-two-sep-tokens/21366\n",
            "3420: Padding with sliding window - https://discuss.huggingface.co/t/padding-with-sliding-window/18921\n",
            "3421: Find which tokens are unknown in new data - https://discuss.huggingface.co/t/find-which-tokens-are-unknown-in-new-data/22466\n",
            "3422: How to train target tokenizer - https://discuss.huggingface.co/t/how-to-train-target-tokenizer/22291\n",
            "3423: How to know if a subtoken is a word or part of a word? - https://discuss.huggingface.co/t/how-to-know-if-a-subtoken-is-a-word-or-part-of-a-word/923\n",
            "3424: BART Tokenizer tokenises same word differently? - https://discuss.huggingface.co/t/bart-tokenizer-tokenises-same-word-differently/21835\n",
            "3425: Fine-tuned BERT tokenizer taking too long to load - https://discuss.huggingface.co/t/fine-tuned-bert-tokenizer-taking-too-long-to-load/9747\n",
            "3426: Add BOS and EOS when encoding a sentence - https://discuss.huggingface.co/t/add-bos-and-eos-when-encoding-a-sentence/21833\n",
            "3427: Customization of Wav2Vec2CTCTokenizer with rules - https://discuss.huggingface.co/t/customization-of-wav2vec2ctctokenizer-with-rules/21912\n",
            "3428: Customized tokenization files in run_clm script - https://discuss.huggingface.co/t/customized-tokenization-files-in-run-clm-script/21460\n",
            "3429: Using customized algorithm - https://discuss.huggingface.co/t/using-customized-algorithm/21753\n",
            "3430: Issue with Flaubert Tokenizer as word_ids() method is not available for NER Task - https://discuss.huggingface.co/t/issue-with-flaubert-tokenizer-as-word-ids-method-is-not-available-for-ner-task/20374\n",
            "3431: Word_ids not working with deberta_v2 - https://discuss.huggingface.co/t/word-ids-not-working-with-deberta-v2/21523\n",
            "3432: How to tokenize large contexts without running out of memory - https://discuss.huggingface.co/t/how-to-tokenize-large-contexts-without-running-out-of-memory/5882\n",
            "3433: Does Deberta tokenizer use wordpiece? - https://discuss.huggingface.co/t/does-deberta-tokenizer-use-wordpiece/21307\n",
            "3434: Get vocabulary tokens in order to exclude them from generate function - https://discuss.huggingface.co/t/get-vocabulary-tokens-in-order-to-exclude-them-from-generate-function/5192\n",
            "3435: Avoid creating certain tokens when training a tokenizer - https://discuss.huggingface.co/t/avoid-creating-certain-tokens-when-training-a-tokenizer/20864\n",
            "3436: Error finetuning XLM-RoBERTa-Large when training - https://discuss.huggingface.co/t/error-finetuning-xlm-roberta-large-when-training/20412\n",
            "3437: HuggingFace BPE Trainer Error - Training Tokenizer - https://discuss.huggingface.co/t/huggingface-bpe-trainer-error-training-tokenizer/10629\n",
            "3438: Word_to_tokens() and word_ids() —- microsoft/deberta-v2/v3 - https://discuss.huggingface.co/t/word-to-tokens-and-word-ids-microsoft-deberta-v2-v3/19984\n",
            "3439: No PreTrainedTokenizerFast for Deberta-V3, no doc_stride - https://discuss.huggingface.co/t/no-pretrainedtokenizerfast-for-deberta-v3-no-doc-stride/20345\n",
            "3440: Tokenizer from own vocab - https://discuss.huggingface.co/t/tokenizer-from-own-vocab/20245\n",
            "3441: No labels column for tokenized data - https://discuss.huggingface.co/t/no-labels-column-for-tokenized-data/19540\n",
            "3442: Programmatic way to Tokenization on Custom Text Columns - https://discuss.huggingface.co/t/programmatic-way-to-tokenization-on-custom-text-columns/19679\n",
            "3443: Bug in Offset generation for Rupee symbol - https://discuss.huggingface.co/t/bug-in-offset-generation-for-rupee-symbol/19655\n",
            "3444: How to handle parenthesis, quotation marks, \\n etc when creating tokenizer from scratch - https://discuss.huggingface.co/t/how-to-handle-parenthesis-quotation-marks-n-etc-when-creating-tokenizer-from-scratch/19628\n",
            "3445: EM training on unigram tokenizer taking way longer than predicted - https://discuss.huggingface.co/t/em-training-on-unigram-tokenizer-taking-way-longer-than-predicted/19502\n",
            "3446: Training unigram on long sequences - https://discuss.huggingface.co/t/training-unigram-on-long-sequences/19188\n",
            "3447: Issue with post-processing - https://discuss.huggingface.co/t/issue-with-post-processing/2703\n",
            "3448: FutureWarning about BertTokenizer.from_pretrained() at latest version - https://discuss.huggingface.co/t/futurewarning-about-berttokenizer-from-pretrained-at-latest-version/18750\n",
            "3449: Enhaced word_ids() API for Chinese or CJK languages? - https://discuss.huggingface.co/t/enhaced-word-ids-api-for-chinese-or-cjk-languages/18662\n",
            "3450: Importing tokenizers version >0.10.3 fails due to openssl - https://discuss.huggingface.co/t/importing-tokenizers-version-0-10-3-fails-due-to-openssl/17820\n",
            "3451: Lower case with input ids - https://discuss.huggingface.co/t/lower-case-with-input-ids/18490\n",
            "3452: Dialogue classification - https://discuss.huggingface.co/t/dialogue-classification/18466\n",
            "3453: Multilang bert vs translating to english - https://discuss.huggingface.co/t/multilang-bert-vs-translating-to-english/18454\n",
            "3454: pyo3_runtime.PanicException: likelihood is NAN. Input sentence may be too long - https://discuss.huggingface.co/t/pyo3-runtime-panicexception-likelihood-is-nan-input-sentence-may-be-too-long/18398\n",
            "3455: Pytorch_model.bin not working because of lfs - https://discuss.huggingface.co/t/pytorch-model-bin-not-working-because-of-lfs/18290\n",
            "3456: Tokenizer ignores repeated whitespaces - https://discuss.huggingface.co/t/tokenizer-ignores-repeated-whitespaces/17864\n",
            "3457: How truncation works when applying BERT tokenizer on the batch of sentence pairs in HuggingFace? - https://discuss.huggingface.co/t/how-truncation-works-when-applying-bert-tokenizer-on-the-batch-of-sentence-pairs-in-huggingface/17977\n",
            "3458: How to save a tokenizer only consisting of added tokens - https://discuss.huggingface.co/t/how-to-save-a-tokenizer-only-consisting-of-added-tokens/17825\n",
            "3459: Passing list of inputs to tokenize - https://discuss.huggingface.co/t/passing-list-of-inputs-to-tokenize/17499\n",
            "3460: Issues with Data Collator and Tokenizing with NER Datasets - https://discuss.huggingface.co/t/issues-with-data-collator-and-tokenizing-with-ner-datasets/17489\n",
            "3461: How to perform tokenization on an ONNX model in JS? - https://discuss.huggingface.co/t/how-to-perform-tokenization-on-an-onnx-model-in-js/17581\n",
            "3462: Using the Tokenizers library in a Unity project - https://discuss.huggingface.co/t/using-the-tokenizers-library-in-a-unity-project/17533\n",
            "3463: Further pre-training the tokenizer? - https://discuss.huggingface.co/t/further-pre-training-the-tokenizer/17360\n",
            "3464: Error when doing tokenization - https://discuss.huggingface.co/t/error-when-doing-tokenization/17344\n",
            "3465: How to decode with spaces? - https://discuss.huggingface.co/t/how-to-decode-with-spaces/17289\n",
            "3466: Show Submodels of PegasusTokenizer - https://discuss.huggingface.co/t/show-submodels-of-pegasustokenizer/17286\n",
            "3467: Load SentencePieceBPETokenizer in TF - https://discuss.huggingface.co/t/load-sentencepiecebpetokenizer-in-tf/17257\n",
            "3468: Best way to mask a multi-token word when using `.*ForMaskedLM` models - https://discuss.huggingface.co/t/best-way-to-mask-a-multi-token-word-when-using-formaskedlm-models/6428\n",
            "3469: Does a tokenizer keep the mapping between my labels to their encoding? - https://discuss.huggingface.co/t/does-a-tokenizer-keep-the-mapping-between-my-labels-to-their-encoding/16296\n",
            "3470: What is Wav2Vec2FeatureExtractor doing? - https://discuss.huggingface.co/t/what-is-wav2vec2featureextractor-doing/16423\n",
            "3471: What does this warning mean? -overflowing tokens are not returned for the setting you have chosen - https://discuss.huggingface.co/t/what-does-this-warning-mean-overflowing-tokens-are-not-returned-for-the-setting-you-have-chosen/11594\n",
            "3472: How can I make sure Tokenizer pads to a fixed length? - https://discuss.huggingface.co/t/how-can-i-make-sure-tokenizer-pads-to-a-fixed-length/16116\n",
            "3473: Issue with Decoding in HuggingFace - https://discuss.huggingface.co/t/issue-with-decoding-in-huggingface/15699\n",
            "3474: ValueError: Unable to create tensor for 1 dataset but not the other of same type - https://discuss.huggingface.co/t/valueerror-unable-to-create-tensor-for-1-dataset-but-not-the-other-of-same-type/15995\n",
            "3475: Disabling addition of CLS from BERT tokenizer - https://discuss.huggingface.co/t/disabling-addition-of-cls-from-bert-tokenizer/15538\n",
            "3476: Tokenized sequence lengths - https://discuss.huggingface.co/t/tokenized-sequence-lengths/15117\n",
            "3477: Finetuning GPT-J6B for custom dataset - https://discuss.huggingface.co/t/finetuning-gpt-j6b-for-custom-dataset/9924\n",
            "3478: Training tokenizer takes too much RAM - https://discuss.huggingface.co/t/training-tokenizer-takes-too-much-ram/14256\n",
            "3479: How to “further pretrain” a tokenizer (do I need to do so?) - https://discuss.huggingface.co/t/how-to-further-pretrain-a-tokenizer-do-i-need-to-do-so/14719\n",
            "3480: Run_seq2seq_qa.py: Column 3 named labels expected length 1007 but got length 1000 - https://discuss.huggingface.co/t/run-seq2seq-qa-py-column-3-named-labels-expected-length-1007-but-got-length-1000/13237\n",
            "3481: Issues with offset_mapping values - https://discuss.huggingface.co/t/issues-with-offset-mapping-values/4237\n",
            "3482: How would you train a sentencepiece BPE tokenizer on this language with 400 “characters”? - https://discuss.huggingface.co/t/how-would-you-train-a-sentencepiece-bpe-tokenizer-on-this-language-with-400-characters/14669\n",
            "3483: All my sequences get tokenized the same - https://discuss.huggingface.co/t/all-my-sequences-get-tokenized-the-same/14657\n",
            "3484: Combine multiple sentences together during tokenization - https://discuss.huggingface.co/t/combine-multiple-sentences-together-during-tokenization/3430\n",
            "3485: NER tag , aggregation stratergy - https://discuss.huggingface.co/t/ner-tag-aggregation-stratergy/14199\n",
            "3486: How to ensure that tokenizers never truncate partial words? - https://discuss.huggingface.co/t/how-to-ensure-that-tokenizers-never-truncate-partial-words/14024\n",
            "3487: How to ensure the `overflow` with `stride` always starts with a full word? - https://discuss.huggingface.co/t/how-to-ensure-the-overflow-with-stride-always-starts-with-a-full-word/14030\n",
            "3488: Adding new tokens to a BERT tokenizer - Getting ValueError - https://discuss.huggingface.co/t/adding-new-tokens-to-a-bert-tokenizer-getting-valueerror/9253\n",
            "3489: Adding token to t5-base vocab does not respect space - https://discuss.huggingface.co/t/adding-token-to-t5-base-vocab-does-not-respect-space/13662\n",
            "3490: How can I change the token id of a special token? - https://discuss.huggingface.co/t/how-can-i-change-the-token-id-of-a-special-token/13445\n",
            "3491: Import distilbert-base-uncased tokenizer to an android app along with the tflite model - https://discuss.huggingface.co/t/import-distilbert-base-uncased-tokenizer-to-an-android-app-along-with-the-tflite-model/3234\n",
            "3492: What are the equivalent manner for using texts_to_sequences? - https://discuss.huggingface.co/t/what-are-the-equivalent-manner-for-using-texts-to-sequences/13220\n",
            "3493: ERROR?why encoding [MASK] before ‘.’ would gain a idx 13? - https://discuss.huggingface.co/t/error-why-encoding-mask-before-would-gain-a-idx-13/2897\n",
            "3494: LongFormer tokenizer has the same token_type_ids for sequence pairs - https://discuss.huggingface.co/t/longformer-tokenizer-has-the-same-token-type-ids-for-sequence-pairs/12992\n",
            "3495: Batch encode plus in Rust Tokenizers - https://discuss.huggingface.co/t/batch-encode-plus-in-rust-tokenizers/12722\n",
            "3496: Best solution for train tokenizer and MLM from scratch - https://discuss.huggingface.co/t/best-solution-for-train-tokenizer-and-mlm-from-scratch/12579\n",
            "3497: Implementing custom tokenizer components (normalizers, processors) - https://discuss.huggingface.co/t/implementing-custom-tokenizer-components-normalizers-processors/12371\n",
            "3498: Does T5Tokenizer support the Greek language? - https://discuss.huggingface.co/t/does-t5tokenizer-support-the-greek-language/12224\n",
            "3499: How padding in huggingface tokenizer works? - https://discuss.huggingface.co/t/how-padding-in-huggingface-tokenizer-works/12161\n",
            "3500: Why we need to add special tokens to tasks other than classification? - https://discuss.huggingface.co/t/why-we-need-to-add-special-tokens-to-tasks-other-than-classification/11975\n",
            "3501: How to configure TokenizerFast for AutoTokenizer - https://discuss.huggingface.co/t/how-to-configure-tokenizerfast-for-autotokenizer/11353\n",
            "3502: How to employ different vocabs for encoder and decoder respectively? - https://discuss.huggingface.co/t/how-to-employ-different-vocabs-for-encoder-and-decoder-respectively/11497\n",
            "3503: How to use tokenizer.tokenize in Chinese data properly? - https://discuss.huggingface.co/t/how-to-use-tokenizer-tokenize-in-chinese-data-properly/11491\n",
            "3504: Mask only specific words - https://discuss.huggingface.co/t/mask-only-specific-words/173\n",
            "3505: Load custom pretrained tokenizer - https://discuss.huggingface.co/t/load-custom-pretrained-tokenizer/11148\n",
            "3506: Using Custom Vocab.txt - https://discuss.huggingface.co/t/using-custom-vocab-txt/10847\n",
            "3507: Tokenizer.encode not returning encodings - https://discuss.huggingface.co/t/tokenizer-encode-not-returning-encodings/10616\n",
            "3508: There is no 0.11.0 tokenizers in pip - https://discuss.huggingface.co/t/there-is-no-0-11-0-tokenizers-in-pip/10381\n",
            "3509: Performance difference between ByteLevelBPE and Wordpiece tokenizers - https://discuss.huggingface.co/t/performance-difference-between-bytelevelbpe-and-wordpiece-tokenizers/10203\n",
            "3510: Should have a `model_type` key in its config.json - https://discuss.huggingface.co/t/should-have-a-model-type-key-in-its-config-json/10144\n",
            "3511: Using a fixed vocab.txt with AutoTokenizer? - https://discuss.huggingface.co/t/using-a-fixed-vocab-txt-with-autotokenizer/9919\n",
            "3512: Train wordpiece from scratch - https://discuss.huggingface.co/t/train-wordpiece-from-scratch/9843\n",
            "3513: I set up a different batch_size, but the time of data processing has not changed - https://discuss.huggingface.co/t/i-set-up-a-different-batch-size-but-the-time-of-data-processing-has-not-changed/9668\n",
            "3514: Cannot create an identical PretrainedTokenizerFast object from a Tokenizer created by tokenizers library - https://discuss.huggingface.co/t/cannot-create-an-identical-pretrainedtokenizerfast-object-from-a-tokenizer-created-by-tokenizers-library/9317\n",
            "3515: Index of wordpieces (subwords) after tokenization by transformers - https://discuss.huggingface.co/t/index-of-wordpieces-subwords-after-tokenization-by-transformers/9552\n",
            "3516: A problem about FutureWarning？ - https://discuss.huggingface.co/t/a-problem-about-futurewarning/9323\n",
            "3517: Extracting embedding values of NLP pertained models from tokenized strings - https://discuss.huggingface.co/t/extracting-embedding-values-of-nlp-pertained-models-from-tokenized-strings/9287\n",
            "3518: Tokenization in a NER context - https://discuss.huggingface.co/t/tokenization-in-a-ner-context/5635\n",
            "3519: Unable to convert output to interpretable format - https://discuss.huggingface.co/t/unable-to-convert-output-to-interpretable-format/8882\n",
            "3520: BpeTrainer implementation in Python - https://discuss.huggingface.co/t/bpetrainer-implementation-in-python/8625\n",
            "3521: MBart50Tokenizer vs XLMRobertaTokenizer - https://discuss.huggingface.co/t/mbart50tokenizer-vs-xlmrobertatokenizer/8498\n",
            "3522: Why multilingual BERT tokenizer doesn’t remove accent markers? - https://discuss.huggingface.co/t/why-multilingual-bert-tokenizer-doesnt-remove-accent-markers/8468\n",
            "3523: TypeError when loading tokenizer with from_pretrained method for bart-large-mnli model - https://discuss.huggingface.co/t/typeerror-when-loading-tokenizer-with-from-pretrained-method-for-bart-large-mnli-model/3378\n",
            "3524: Is it okay to split ids sequence when it is encoded using Byte-level BPE - https://discuss.huggingface.co/t/is-it-okay-to-split-ids-sequence-when-it-is-encoded-using-byte-level-bpe/8129\n",
            "3525: Using truncated fragments as input samples in training - https://discuss.huggingface.co/t/using-truncated-fragments-as-input-samples-in-training/6978\n",
            "3526: Using whitespace tokenizer for training models - https://discuss.huggingface.co/t/using-whitespace-tokenizer-for-training-models/6591\n",
            "3527: Save custom components - https://discuss.huggingface.co/t/save-custom-components/6458\n",
            "3528: How to see contents of a normalizer - https://discuss.huggingface.co/t/how-to-see-contents-of-a-normalizer/6045\n",
            "3529: Newbie: Main difference between tokenizers? - https://discuss.huggingface.co/t/newbie-main-difference-between-tokenizers/6035\n",
            "3530: Can’t load tokenizer for ‘sshleifer/student_blarge_12_3’ - https://discuss.huggingface.co/t/cant-load-tokenizer-for-sshleifer-student-blarge-12-3/6027\n",
            "3531: How to create a Huggingface tokenizer from a non-Huggingface tokenizer? - https://discuss.huggingface.co/t/how-to-create-a-huggingface-tokenizer-from-a-non-huggingface-tokenizer/5983\n",
            "3532: Add new tokens and learn the embeddings of the new tokens and keeping all the other parametes frozen - https://discuss.huggingface.co/t/add-new-tokens-and-learn-the-embeddings-of-the-new-tokens-and-keeping-all-the-other-parametes-frozen/5896\n",
            "3533: How do you use SentencePiece for BPE of sequences with no whitespace - https://discuss.huggingface.co/t/how-do-you-use-sentencepiece-for-bpe-of-sequences-with-no-whitespace/1895\n",
            "3534: BOS tokens for mBERT tokenizer - https://discuss.huggingface.co/t/bos-tokens-for-mbert-tokenizer/5467\n",
            "3535: BertTokenizerFast for stsb-xlm-r-multilingual model - https://discuss.huggingface.co/t/berttokenizerfast-for-stsb-xlm-r-multilingual-model/4742\n",
            "3536: Skip-gram tokens - https://discuss.huggingface.co/t/skip-gram-tokens/5294\n",
            "3537: Using a BertWordPieceTokenizer trained from scratch from transformers - https://discuss.huggingface.co/t/using-a-bertwordpiecetokenizer-trained-from-scratch-from-transformers/4391\n",
            "3538: Questions on model’s tokens - https://discuss.huggingface.co/t/questions-on-models-tokens/5023\n",
            "3539: Space token ’ ’ cannot be add when is_split_into_words = True - https://discuss.huggingface.co/t/space-token-cannot-be-add-when-is-split-into-words-true/4305\n",
            "3540: Are special_tokens the only tokens guaranteed to be atomic? - https://discuss.huggingface.co/t/are-special-tokens-the-only-tokens-guaranteed-to-be-atomic/4124\n",
            "3541: Does AutoTokenizer.from_pretrained add [cls] tokens? - https://discuss.huggingface.co/t/does-autotokenizer-from-pretrained-add-cls-tokens/4056\n",
            "3542: BertTokenizer’s encode_plus returns 2d tensor when printing ‘input_ids’/ ‘attention_mask’ - https://discuss.huggingface.co/t/berttokenizers-encode-plus-returns-2d-tensor-when-printing-input-ids-attention-mask/3530\n",
            "3543: Tunning tokenizer on my own dataset - https://discuss.huggingface.co/t/tunning-tokenizer-on-my-own-dataset/3367\n",
            "3544: Why Bert-chinese use do_lower_case=False? - https://discuss.huggingface.co/t/why-bert-chinese-use-do-lower-case-false/2952\n",
            "3545: Bug with tokernizer’s offset mapping for NER problems? - https://discuss.huggingface.co/t/bug-with-tokernizers-offset-mapping-for-ner-problems/2928\n",
            "3546: BERT WordPiece Tokenizer: some matras missing after tokenization for Hindi Language #572 - https://discuss.huggingface.co/t/bert-wordpiece-tokenizer-some-matras-missing-after-tokenization-for-hindi-language-572/2936\n",
            "3547: Error with <|endoftext|> in Tokenizer GPT2 - https://discuss.huggingface.co/t/error-with-endoftext-in-tokenizer-gpt2/2838\n",
            "3548: Build a RoBERTa tokenizer from scratch - https://discuss.huggingface.co/t/build-a-roberta-tokenizer-from-scratch/2758\n",
            "3549: Couldn’t instantiate the backend tokenizer - https://discuss.huggingface.co/t/couldnt-instantiate-the-backend-tokenizer/2662\n",
            "3550: Bypassing tokenizers - https://discuss.huggingface.co/t/bypassing-tokenizers/2162\n",
            "3551: Tokenizing Domain Specific Text - https://discuss.huggingface.co/t/tokenizing-domain-specific-text/1978\n",
            "3552: Issue with tokenizer.tokenize - https://discuss.huggingface.co/t/issue-with-tokenizer-tokenize/1891\n",
            "3553: Tokenizer taking extremely long time to train - https://discuss.huggingface.co/t/tokenizer-taking-extremely-long-time-to-train/1875\n",
            "3554: Where to find the “wiki-big.train.raw” data as mentioned in the snippet for tokenizers 0.9? - https://discuss.huggingface.co/t/where-to-find-the-wiki-big-train-raw-data-as-mentioned-in-the-snippet-for-tokenizers-0-9/1796\n",
            "3555: Change bpe-dropout value on the fly? - https://discuss.huggingface.co/t/change-bpe-dropout-value-on-the-fly/1721\n",
            "3556: Loading pretrained SentencePiece tokenizer from Fairseq - https://discuss.huggingface.co/t/loading-pretrained-sentencepiece-tokenizer-from-fairseq/1326\n",
            "3557: What does `tokenizers.normalizer.normalize` do? - https://discuss.huggingface.co/t/what-does-tokenizers-normalizer-normalize-do/1463\n",
            "3558: Automatic sentence segmentation and encoding - https://discuss.huggingface.co/t/automatic-sentence-segmentation-and-encoding/1479\n",
            "3559: How to truncate from the head in AutoTokenizer? - https://discuss.huggingface.co/t/how-to-truncate-from-the-head-in-autotokenizer/676\n",
            "3560: How much memory is needed for training ByteLevelBPETokenizer? - https://discuss.huggingface.co/t/how-much-memory-is-needed-for-training-bytelevelbpetokenizer/1165\n",
            "3561: How to make tokenizer convert subword token to an independent token? - https://discuss.huggingface.co/t/how-to-make-tokenizer-convert-subword-token-to-an-independent-token/1015\n",
            "3562: Using a pretrained tokenizer vs training a one from scratch - https://discuss.huggingface.co/t/using-a-pretrained-tokenizer-vs-training-a-one-from-scratch/783\n",
            "3563: Masking Probability - https://discuss.huggingface.co/t/masking-probability/746\n",
            "3564: Tokenizer not found - https://discuss.huggingface.co/t/tokenizer-not-found/757\n",
            "3565: Add new tokens for subwords - https://discuss.huggingface.co/t/add-new-tokens-for-subwords/489\n",
            "3566: Token alignment for word-level tasks - https://discuss.huggingface.co/t/token-alignment-for-word-level-tasks/577\n",
            "3567: ByteLevelBPETokenizer inconsistent behavior - https://discuss.huggingface.co/t/bytelevelbpetokenizer-inconsistent-behavior/442\n",
            "3568: Use a pretrained ByteLevelBPETokenizer on text - https://discuss.huggingface.co/t/use-a-pretrained-bytelevelbpetokenizer-on-text/348\n",
            "3569: Continuation token in pertained tokenizer bert-base-chinese - https://discuss.huggingface.co/t/continuation-token-in-pertained-tokenizer-bert-base-chinese/218\n",
            "3570: Tokenizers v0.8.0 is out! - https://discuss.huggingface.co/t/tokenizers-v0-8-0-is-out/51\n",
            "3571: About the Tokenizers category - https://discuss.huggingface.co/t/about-the-tokenizers-category/30\n"
          ]
        }
      ],
      "source": [
        "# Set up Chrome options for Colab\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "import time\n",
        "\n",
        "# Set up Chrome options to use headless mode (for Colab)\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "\n",
        "# Add extra options\n",
        "chrome_options.add_argument(\"--window-size=1920,1080\")  # Set the window size\n",
        "chrome_options.add_argument(\"--disable-infobars\")  # Disable the infobars\n",
        "chrome_options.add_argument(\"--disable-popup-blocking\")  # Disable pop-ups\n",
        "chrome_options.add_argument(\"--ignore-certificate-errors\")  # Ignore certificate errors\n",
        "chrome_options.add_argument(\"--incognito\")  # Use Chrome in incognito mode\n",
        "\n",
        "# Set the path to chromedriver explicitly (installed by apt)\n",
        "chrome_path = \"/usr/bin/chromedriver\"\n",
        "\n",
        "# Initialize the WebDriver with the updated path\n",
        "driver = gs.Chrome(options=chrome_options)\n",
        "\n",
        "# Open the specific forum page\n",
        "url = \"https://discuss.huggingface.co/c/tokenizers/11\"  # Replace with your desired URL\n",
        "driver.get(url)\n",
        "\n",
        "# Wait for the page to load\n",
        "time.sleep(5)\n",
        "\n",
        "# Base URL of the site (to handle relative links)\n",
        "# base_url = \"https://discuss.huggingface.co\"\n",
        "\n",
        "# Function to scroll and scrape titles and links\n",
        "def scrape_titles_and_links():\n",
        "    titles_and_links = []\n",
        "    while True:\n",
        "        # Find all titles and their corresponding links (adjust selector to match the structure you provided)\n",
        "        elements = driver.find_elements(By.CSS_SELECTOR, \"span.link-top-line a.title.raw-link.raw-topic-link\")  # Updated selector\n",
        "        for elem in elements:\n",
        "            title = elem.text.strip()  # Get the title text\n",
        "            relative_link = elem.get_attribute('href')  # Get the relative URL from the href attribute\n",
        "\n",
        "            # Construct the absolute URL by appending the relative path to the base URL\n",
        "            full_link = relative_link\n",
        "            titles_and_links.append((title, full_link))  # Store as tuple (title, link)\n",
        "\n",
        "        # Scroll down to load more content (if the forum uses infinite scroll)\n",
        "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
        "        time.sleep(3)  # Adjust based on loading speed\n",
        "\n",
        "        # Break the loop if no new titles are loaded\n",
        "        if len(elements) == len(driver.find_elements(By.CSS_SELECTOR, \"span.link-top-line a.title.raw-link.raw-topic-link\")):\n",
        "            break\n",
        "    return titles_and_links\n",
        "\n",
        "# Scrape and print the titles and their links\n",
        "titles_and_links = scrape_titles_and_links()\n",
        "print(\"Scraped Titles and Links:\")\n",
        "for i, (title, link) in enumerate(titles_and_links, 1):\n",
        "    print(f\"{i}: {title} - {link}\")\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}